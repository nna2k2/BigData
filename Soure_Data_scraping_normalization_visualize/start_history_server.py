#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script kh·ªüi ƒë·ªông Spark History Server b·∫±ng Python
Ch·∫°y: python3 start_history_server.py
"""

import os
import sys
import subprocess
import signal
from pathlib import Path

def find_spark_home():
    """T√¨m SPARK_HOME t·ª´ PySpark."""
    try:
        import pyspark
        spark_home = os.path.dirname(os.path.dirname(pyspark.__file__))
        return spark_home
    except ImportError:
        print("‚ùå Kh√¥ng t√¨m th·∫•y PySpark")
        print("   Vui l√≤ng c√†i ƒë·∫∑t: pip install pyspark")
        sys.exit(1)

def start_history_server():
    """Kh·ªüi ƒë·ªông Spark History Server."""
    script_dir = Path(__file__).parent.absolute()
    event_log_dir = script_dir / "spark-events"
    log_dir = script_dir / "logs"
    
    # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
    event_log_dir.mkdir(exist_ok=True)
    log_dir.mkdir(exist_ok=True)
    
    # T√¨m SPARK_HOME
    spark_home = find_spark_home()
    print(f"‚úÖ T√¨m th·∫•y Spark t·∫°i: {spark_home}")
    print(f"‚úÖ Event log directory: {event_log_dir}")
    print(f"‚úÖ History Server port: 18080")
    print("")
    
    # Ki·ªÉm tra xem History Server ƒë√£ ch·∫°y ch∆∞a
    try:
        result = subprocess.run(
            ["pgrep", "-f", "org.apache.spark.deploy.history.HistoryServer"],
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            pid = result.stdout.strip()
            print(f"‚ö†Ô∏è  Spark History Server ƒë√£ ƒëang ch·∫°y (PID: {pid})")
            print("   ƒê·ªÉ d·ª´ng, ch·∫°y: ./stop_history_server.sh")
            return
    except:
        pass
    
    # T√¨m spark-class ho·∫∑c sbin script
    spark_class = os.path.join(spark_home, "bin", "spark-class")
    start_script = os.path.join(spark_home, "sbin", "start-history-server.sh")
    
    log_file = log_dir / "history_server.log"
    
    print("üöÄ ƒêang kh·ªüi ƒë·ªông History Server...")
    print(f"   Log file: {log_file}")
    print("")
    
    # Set environment variables
    env = os.environ.copy()
    env["SPARK_HOME"] = spark_home
    env["SPARK_HISTORY_OPTS"] = f"-Dspark.history.fs.logDirectory=file://{event_log_dir.absolute()}"
    
    # Th·ª≠ d√πng start-history-server.sh tr∆∞·ªõc
    if os.path.exists(start_script):
        print("   S·ª≠ d·ª•ng: sbin/start-history-server.sh")
        os.chdir(spark_home)
        with open(log_file, "a") as f:
            process = subprocess.Popen(
                ["bash", start_script],
                stdout=f,
                stderr=subprocess.STDOUT,
                env=env,
                cwd=spark_home
            )
    elif os.path.exists(spark_class):
        print("   S·ª≠ d·ª•ng: bin/spark-class")
        os.chdir(spark_home)
        with open(log_file, "a") as f:
            process = subprocess.Popen(
                [spark_class, "org.apache.spark.deploy.history.HistoryServer"],
                stdout=f,
                stderr=subprocess.STDOUT,
                env=env,
                cwd=spark_home
            )
    else:
        # Fallback: d√πng pyspark ƒë·ªÉ t√¨m v√† ch·∫°y
        print("   S·ª≠ d·ª•ng: pyspark (fallback - t√¨m jars)")
        try:
            import pyspark
            # T√¨m jar file trong PySpark installation
            jars_dir = os.path.join(spark_home, "jars")
            
            if not os.path.exists(jars_dir):
                print(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c jars: {jars_dir}")
                print("   PySpark c√≥ th·ªÉ kh√¥ng c√≥ ƒë·∫ßy ƒë·ªß files")
                print("   Th·ª≠ c√†i ƒë·∫∑t Spark standalone ho·∫∑c d√πng c√°ch kh√°c")
                sys.exit(1)
            
            # S·ª≠ d·ª•ng java tr·ª±c ti·∫øp v·ªõi spark jars
            java_home = os.environ.get("JAVA_HOME", "")
            if java_home:
                java_bin = os.path.join(java_home, "bin", "java")
            else:
                java_bin = "java"
            
            # Ki·ªÉm tra java c√≥ t·ªìn t·∫°i kh√¥ng
            try:
                subprocess.run([java_bin, "-version"], capture_output=True, check=True)
            except:
                print(f"‚ùå Kh√¥ng t√¨m th·∫•y Java: {java_bin}")
                print("   Vui l√≤ng c√†i ƒë·∫∑t Java v√† set JAVA_HOME")
                sys.exit(1)
            
            # T√¨m t·∫•t c·∫£ jar files
            spark_jars = []
            if os.path.exists(jars_dir):
                for jar in os.listdir(jars_dir):
                    if jar.endswith(".jar"):
                        spark_jars.append(os.path.join(jars_dir, jar))
            
            if not spark_jars:
                print(f"‚ùå Kh√¥ng t√¨m th·∫•y Spark JAR files trong: {jars_dir}")
                print("   PySpark c√≥ th·ªÉ kh√¥ng c√≥ ƒë·∫ßy ƒë·ªß files")
                sys.exit(1)
            
            print(f"   T√¨m th·∫•y {len(spark_jars)} JAR files")
            
            # T·∫°o classpath
            classpath = ":".join(spark_jars)
            
            # Ch·∫°y History Server
            os.chdir(spark_home)
            cmd = [
                java_bin,
                f"-Dspark.history.fs.logDirectory=file://{event_log_dir.absolute()}",
                "-cp", classpath,
                "org.apache.spark.deploy.history.HistoryServer"
            ]
            
            print(f"   Command: {' '.join(cmd[:3])} ...")
            with open(log_file, "a") as f:
                f.write(f"\n{'='*60}\n")
                f.write(f"Starting History Server at {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Command: {' '.join(cmd)}\n")
                f.write(f"{'='*60}\n")
                process = subprocess.Popen(
                    cmd,
                    stdout=f,
                    stderr=subprocess.STDOUT,
                    env=env,
                    cwd=spark_home
                )
        except Exception as e:
            print(f"‚ùå L·ªói khi kh·ªüi ƒë·ªông History Server: {e}")
            print(f"   Xem log: tail -f {log_file}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
    
    # ƒê·ª£i m·ªôt ch√∫t
    import time
    time.sleep(3)
    
    # Ki·ªÉm tra xem ƒë√£ start th√†nh c√¥ng ch∆∞a
    try:
        result = subprocess.run(
            ["pgrep", "-f", "org.apache.spark.deploy.history.HistoryServer"],
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            pid = result.stdout.strip()
            print("‚úÖ Spark History Server ƒë√£ kh·ªüi ƒë·ªông th√†nh c√¥ng!")
            print("")
            print("üìä Th√¥ng tin:")
            print(f"   PID: {pid}")
            print("   Port: 18080")
            print(f"   Event Log Dir: {event_log_dir}")
            print("")
            
            # L·∫•y IP server
            import socket
            try:
                s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
                s.connect(("8.8.8.8", 80))
                server_ip = s.getsockname()[0]
                s.close()
            except:
                server_ip = "localhost"
            
            print("üåê Truy c·∫≠p Spark History Server:")
            print(f"   http://{server_ip}:18080")
            print("   ho·∫∑c")
            print("   http://localhost:18080")
            print("")
            print(f"üìù Log file: {log_file}")
            print("")
            print("üí° ƒê·ªÉ d·ª´ng History Server:")
            print("   ./stop_history_server.sh")
        else:
            print("‚ùå Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông History Server")
            print(f"   Xem log: tail -f {log_file}")
            sys.exit(1)
    except Exception as e:
        print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ ki·ªÉm tra tr·∫°ng th√°i: {e}")
        print(f"   Xem log: tail -f {log_file}")

if __name__ == "__main__":
    start_history_server()

