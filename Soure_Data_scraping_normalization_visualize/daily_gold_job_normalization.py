# -*- coding: utf-8 -*-
"""
Daily Gold ETL Job (Oracle 23ai schema)
- B1: LOCATION_DIMENSION: ph√°t hi·ªán th√†nh ph·ªë tr√πng ng·ªØ nghƒ©a -> g·ªôp v·ªÅ 1 ID v√† c·∫≠p nh·∫≠t GOLD_PRICE_FACT.LOCATION_ID
- B2: GOLD_TYPE_DIMENSION: d√πng t∆∞∆°ng ƒë·ªìng ƒë·ªÉ ƒëi·ªÅn PURITY/CATEGORY c√≤n thi·∫øu; (tu·ª≥ ch·ªçn) g·ªôp TYPE gi·ªëng nhau -> c·∫≠p nh·∫≠t FACT.TYPE_ID
- B3: GOLD_PRICE_FACT: v·ªõi (SOURCE_ID, TYPE_ID, LOCATION_ID, TIME_ID) tr√πng nhau -> gi·ªØ RECORDED_AT m·ªõi nh·∫•t, c√≤n l·∫°i IS_DELETED=1
- Incremental b·∫±ng checkpoint trong DB (b·∫£ng ETL_CHECKPOINT)
- Ch·ª•p snapshot tr∆∞·ªõc/sau ra CSV ƒë·ªÉ b√°o c√°o
- Tu·ª≥ ch·ªçn insert d·ªØ li·ªáu fake (ƒëa ngu·ªìn, synonym ƒë·ªãa danh & gold type) ƒë√∫ng v·ªõi schema b·∫°n g·ª≠i
"""

import argparse
import datetime as dt
import os
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import oracledb

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from fuzzywuzzy import fuzz

import re

# ====================== CONFIG ======================
DB_USER = "ADMIN"
DB_PASS = "Abcd12345678!"
DB_DSN  = "34.126.123.190:1521/MYATP_low.adb.oraclecloud.com"  # gi·ªØ nguy√™n DSN b·∫°n ƒë∆∞a

SNAPSHOT_DIR = "./snapshots"  # n∆°i l∆∞u CSV tr∆∞·ªõc/sau ƒë·ªÉ ch·ª•p m√†n h√¨nh b√°o c√°o
JOB_NAME = "DAILY_GOLD_JOB"
SIM_THRESHOLD_LOC = 0.80      # ng∆∞·ª°ng cosine TF-IDF cho City
SIM_THRESHOLD_TYPE = 0.75     # ng∆∞·ª°ng cosine TF-IDF cho TypeName
FUZZY_FALLBACK = 90           # ng∆∞·ª°ng fuzzy token_set_ratio fallback

# ====================================================

def conn():
    return oracledb.connect(user=DB_USER, password=DB_PASS, dsn=DB_DSN)

def ensure_infra(c):
    """ƒê·∫£m b·∫£o c√°c b·∫£ng/c·ªôt ph·ª• tr·ª£ t·ªìn t·∫°i (ETL_CHECKPOINT, IS_DELETE n·∫øu ch∆∞a c√≥)."""
    with c.cursor() as cur:
        # 1) B·∫£ng checkpoint
        cur.execute("""
            BEGIN
                EXECUTE IMMEDIATE '
                    CREATE TABLE ETL_CHECKPOINT (
                        JOB_NAME VARCHAR2(100) PRIMARY KEY,
                        LAST_RUN TIMESTAMP
                    )
                ';
            EXCEPTION
                WHEN OTHERS THEN
                    IF SQLCODE != -955 THEN RAISE; END IF; -- -955: table already exists
            END;""")

        # 2) C·ªôt IS_DELETE trong GOLD_PRICE_FACT (schema ƒë√£ c√≥ IS_DELETED; ta v·∫´n th√™m IS_DELETE n·∫øu ch∆∞a c√≥ ƒë·ªÉ t∆∞∆°ng th√≠ch job c≈©)
        cur.execute("""
            DECLARE
                v_dummy NUMBER;
            BEGIN
                SELECT 1 INTO v_dummy FROM USER_TAB_COLS 
                WHERE TABLE_NAME = 'GOLD_PRICE_FACT' AND COLUMN_NAME = 'IS_DELETE';
            EXCEPTION
                WHEN NO_DATA_FOUND THEN
                    EXECUTE IMMEDIATE 'ALTER TABLE GOLD_PRICE_FACT ADD (IS_DELETE NUMBER(1) DEFAULT 0)';
            END;""")

    c.commit()

def get_last_checkpoint(c) -> dt.datetime:
    with c.cursor() as cur:
        cur.execute("SELECT LAST_RUN FROM ETL_CHECKPOINT WHERE JOB_NAME = :j", {"j": JOB_NAME})
        row = cur.fetchone()
        if not row or not row[0]:
            return dt.datetime(2000,1,1)  # l√πi s√¢u l·∫ßn ƒë·∫ßu
        return row[0]

def set_checkpoint(c, ts: dt.datetime):
    with c.cursor() as cur:
        cur.execute("""
            MERGE INTO ETL_CHECKPOINT t
            USING (SELECT :j JOB_NAME, :lr LAST_RUN FROM dual) s
            ON (t.JOB_NAME = s.JOB_NAME)
            WHEN MATCHED THEN UPDATE SET t.LAST_RUN = s.LAST_RUN
            WHEN NOT MATCHED THEN INSERT (JOB_NAME, LAST_RUN) VALUES (s.JOB_NAME, s.LAST_RUN)
        """, {"j": JOB_NAME, "lr": ts})
    c.commit()

def snapshot_table(c, table: str, tag: str):
    os.makedirs(SNAPSHOT_DIR, exist_ok=True)
    df = pd.read_sql(f'SELECT * FROM "{DB_USER}"."{table}"', c)  # gi·ªØ schema ADMIN t∆∞·ªùng minh
    path = os.path.join(SNAPSHOT_DIR, f"{table}_{tag}_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
    df.to_csv(path, index=False, encoding="utf-8-sig")
    print(f"üì∏ Snapshot {table} -> {path}")

# ----------------------- Fake data -----------------------

def get_or_create_time_id(c, date_obj: dt.date) -> int:
    """Tr·∫£ v·ªÅ TIME_DIMENSION.ID t∆∞∆°ng ·ª©ng v·ªõi DATE_TIME = date_obj (00:00:00). N·∫øu ch∆∞a c√≥ th√¨ t·∫°o m·ªõi."""
    with c.cursor() as cur:
        cur.execute('SELECT ID FROM TIME_DIMENSION WHERE TRUNC(DATE_TIME)=:d', {"d": date_obj})
        r = cur.fetchone()
        if r:
            return int(r[0])
        # ch√®n m·ªõi
        cur.execute(
            'INSERT INTO TIME_DIMENSION (DATE_TIME, YEAR, MONTH, DAY, HOUR) VALUES (:dt, :y, :m, :d, :h) RETURNING ID INTO :rid',
            {"dt": dt.datetime.combine(date_obj, dt.time(0,0,0)), "y": date_obj.year, "m": date_obj.month, "d": date_obj.day, "h": 0, "rid": cur.var(oracledb.NUMBER)}
        )
        rid = int(cur.getimplicitresults()[0][0]) if hasattr(cur, "getimplicitresults") else None
        if rid is None:
            # fallback: l·∫•y l·∫°i
            cur.execute('SELECT ID FROM TIME_DIMENSION WHERE TRUNC(DATE_TIME)=:d', {"d": date_obj})
            rid = int(cur.fetchone()[0])
        c.commit()
        return rid

def insert_fake_data(c):
    """T·∫°o d·ªØ li·ªáu demo theo ƒë√∫ng schema (ID, TYPE_NAME, CITY, ...)."""
    with c.cursor() as cur:
        # LOCATION_DIMENSION demo (ch√®n t√™n ƒë·ªìng nghƒ©a)
        for (idv, city, region) in [
            (101, 'H·ªì Ch√≠ Minh', 'Mi·ªÅn Nam'),
            (102, 'S√†i G√≤n', 'Mi·ªÅn Nam'),
            (201, 'H√† N·ªôi', 'Mi·ªÅn B·∫Øc'),
            (202, 'Th·ªß ƒë√¥', 'Mi·ªÅn B·∫Øc'),
            (301, 'ƒê√† N·∫µng', 'Mi·ªÅn Trung'),
            (302, 'Danang', 'Mi·ªÅn Trung'),
        ]:
            try:
                cur.execute('INSERT INTO LOCATION_DIMENSION (ID, CITY, REGION) VALUES (:i, :c, :r)', {"i": idv, "c": city, "r": region})
            except oracledb.Error:
                pass  # ƒë√£ t·ªìn t·∫°i

        # GOLD_TYPE_DIMENSION demo
        for (idv, tname, purity, cat, brand) in [
            (11, 'V√†ng SJC 1L', '99.99%', 'Gold bar', 'SJC'),
            (12, 'V√†ng SJC 5 ch·ªâ', None, 'other', 'SJC'),
            (13, 'V√†ng SJC 10L', '99.99%', 'Gold bar', 'SJC'),
            (14, 'Ph√∫ Qu√Ω 1 l∆∞·ª£ng 99.9', '99.9%', 'other', 'Ph√∫ Qu√Ω'),
            (15, 'Ph√∫ Qu√Ω 5 ch·ªâ', None, None, 'Ph√∫ Qu√Ω'),
        ]:
            try:
                cur.execute('INSERT INTO GOLD_TYPE_DIMENSION (ID, TYPE_NAME, PURITY, CATEGORY, BRAND) VALUES (:i,:t,:p,:c,:b)',
                            {"i": idv, "t": tname, "p": purity, "c": cat, "b": brand})
            except oracledb.Error:
                pass

        # SOURCE_DIMENSION demo
        for (idv, name, url, descp) in [
            (1, 'PNJ',  'https://www.giavang.pnj.com.vn/', 'PNJ'),
            (2, 'DOJI', 'https://www.giadoji.vn/', 'DOJI'),
        ]:
            try:
                cur.execute('INSERT INTO SOURCE_DIMENSION (ID, SOURCE_NAME, SOURCE_URL, DESCRIPTION) VALUES (:i,:n,:u,:d)',
                            {"i": idv, "n": name, "u": url, "d": descp})
            except oracledb.Error:
                pass

        # TIME_DIMENSION demo (10 ng√†y g·∫ßn ƒë√¢y)
        for d in pd.date_range(end=pd.Timestamp.today().normalize(), periods=10):
            _ = get_or_create_time_id(c, d.date())

        # GOLD_PRICE_FACT demo: c·ªë t√¨nh tr√πng l·∫∑p & mix location/type synonym, nhi·ªÅu ngu·ªìn
        base = [
            (1, 11, 101), (1, 12, 102), (1, 13, 201),
            (2, 14, 202), (2, 15, 301), (1, 11, 302)
        ]
        now = pd.Timestamp.now()
        pid = 50000
        for (src, typ, loc) in base:
            for d in pd.date_range(end=pd.Timestamp.today().normalize(), periods=5):
                time_id = get_or_create_time_id(c, d.date())
                buy = 70000000 + np.random.randint(-300000, 300000)
                sell = buy + np.random.randint(50000, 200000)
                # ch√®n 2 b·∫£n ghi c√πng key nh∆∞ng RECORDED_AT kh√°c nhau
                for dup in range(2):
                    try:
                        cur.execute("""
                            INSERT INTO GOLD_PRICE_FACT
                                (ID, SOURCE_ID, TYPE_ID, LOCATION_ID, TIME_ID,
                                 BUY_PRICE, SELL_PRICE, UNIT, RECORDED_AT, IS_DELETED, RECORDED_BY, IS_DELETE)
                            VALUES
                                (:id, :sid, :tid, :lid, :tt,
                                 :bp, :sp, 'VND/L∆∞·ª£ng', :rec, 0, 'demo', 0)
                        """, {
                            "id": pid, "sid": src, "tid": typ, "lid": loc, "tt": time_id,
                            "bp": buy, "sp": sell,
                            "rec": (now - pd.Timedelta(days=5 - (pd.Timestamp.today().normalize() - d).days, minutes=10 - dup)).to_pydatetime()
                        })
                        pid += 1
                    except oracledb.Error:
                        pass

    c.commit()
    print("‚úÖ ƒê√£ ch√®n d·ªØ li·ªáu demo.")

# -------------------- LOCATION normalize --------------------

import unicodedata

# ======= B·ªô t·ª´ ƒëi·ªÉn ƒë·ªìng nghƒ©a & ch·ªëng g·ªôp =======

def norm_txt(s: str) -> str:
    """Chu·∫©n ho√° ti·∫øng Vi·ªát (b·ªè d·∫•u, lowercase, trim) ƒë·ªÉ so kh·ªõp ·ªïn ƒë·ªãnh."""
    s = (s or "").strip().lower()
    s = unicodedata.normalize("NFD", s)
    s = "".join(ch for ch in s if unicodedata.category(ch) != "Mn")
    return s

# C·∫∑p √©p g·ªôp (√©p bu·ªôc nh√≥m c√πng)
POSITIVE_SYNONYMS = {
    ("ho chi minh", "tphcm"),
    ("ho chi minh", "tp hcm"),
    ("da nang", "danang"),
    ("ha noi", "thu do"),
    ("ha noi", "hn"),
}

# C·∫∑p c·∫•m g·ªôp (k·ªÉ c·∫£ similarity cao v·∫´n kh√¥ng g·ªôp)
NEGATIVE_SYNONYMS = {
    ("sai gon", "ho chi minh"),
    ("sai gon", "tphcm"),
    ("sai gon", "tp hcm"),
}

def pair_blocked(a: str, b: str) -> bool:
    A, B = norm_txt(a), norm_txt(b)
    return (A, B) in NEGATIVE_SYNONYMS or (B, A) in NEGATIVE_SYNONYMS

def pair_forced(a: str, b: str) -> bool:
    A, B = norm_txt(a), norm_txt(b)
    return (A, B) in POSITIVE_SYNONYMS or (B, A) in POSITIVE_SYNONYMS

# T√≠nh TF-IDF (char n-gram) cho t·ª´ng t√™n.
# D√πng cosine_similarity ƒë·ªÉ ƒëo ƒë·ªô gi·ªëng nhau gi·ªØa c√°c t√™n.
# N·∫øu cosine >= SIM_THRESHOLD_LOC (0.75) ho·∫∑c fuzz.token_set_ratio >= 90, coi l√† t∆∞∆°ng ƒë·ªìng.
# C√≥ b·ªô lu·∫≠t √©p g·ªôp v√† b·ªô lu·∫≠t c·∫•m g·ªôp:
# V√≠ d·ª•: √©p g·ªôp ‚Äúƒê√† N·∫µng‚Äù ~ ‚ÄúDanang‚Äù.
# C·∫•m g·ªôp ‚ÄúS√†i G√≤n‚Äù ~ ‚ÄúH·ªì Ch√≠ Minh‚Äù.
def build_similarity_groups(values: List[str], threshold: float) -> List[List[int]]:
    """Nh√≥m c√°c index c√≥ ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine TF-IDF >= threshold + fallback fuzzy, c√≥ √©p g·ªôp & c·∫•m g·ªôp."""
    if not values:
        return []

    vec = TfidfVectorizer(ngram_range=(1,2), analyzer='char_wb').fit(values)
    tf = vec.transform(values)
    sim = cosine_similarity(tf)

    n = len(values)
    visited = [False]*n
    groups = []

    for i in range(n):
        if visited[i]:
            continue
        group = [i]
        visited[i] = True

        for j in range(i+1, n):
            if visited[j]:
                continue

            a, b = values[i], values[j]

            # 1Ô∏è‚É£ N·∫øu b·ªã c·∫•m g·ªôp ‚Üí b·ªè qua
            if pair_blocked(a, b):
                print(f"üö´ BLOCKED: '{a}' vs '{b}'")
                continue

            # 2Ô∏è‚É£ N·∫øu l√† c·∫∑p √©p g·ªôp ‚Üí g·ªôp lu√¥n
            if pair_forced(a, b):
                print(f"‚úÖ FORCED GROUP: '{a}' ~ '{b}'")
                group.append(j)
                visited[j] = True
                continue

            # 3Ô∏è‚É£ N·∫øu similarity ƒë·ªß cao ‚Üí g·ªôp
            if sim[i, j] >= threshold or fuzz.token_set_ratio(a, b) >= FUZZY_FALLBACK:
                print(f"‚âà SIMILAR: '{a}' ~ '{b}' (cos={sim[i,j]:.2f}, fuzzy={fuzz.token_set_ratio(a,b)})")
                group.append(j)
                visited[j] = True

        groups.append(group)
    return groups

# Trong b·∫£ng LOCATION_DIMENSION, c√≥ th·ªÉ c√≥ nhi·ªÅu b·∫£n ghi c√πng nghƒ©a nh∆∞ng kh√°c c√°ch vi·∫øt (v√≠ d·ª•: ‚ÄúH·ªì Ch√≠ Minh‚Äù, ‚ÄúTP HCM‚Äù, ‚ÄúS√†i G√≤n‚Äù, ‚ÄúHo Chi Minh City‚Äù‚Ä¶).
# H√†m n√†y:
# Ph√°t hi·ªán c√°c th√†nh ph·ªë t∆∞∆°ng t·ª± nhau (ng·ªØ nghƒ©a g·∫ßn nhau).
# G·ªôp v·ªÅ c√πng m·ªôt LOCATION_ID chu·∫©n (canonical ID).
# C·∫≠p nh·∫≠t b·∫£ng GOLD_PRICE_FACT ƒë·ªÉ t·∫•t c·∫£ b·∫£n ghi c√πng nghƒ©a d√πng LOCATION_ID th·ªëng nh·∫•t.
def normalize_locations(c) -> Dict[int, int]:
    """Ph√°t hi·ªán & g·ªôp c√°c LOCATION t∆∞∆°ng ƒë·ªìng; c√≥ snapshot tr∆∞·ªõc/sau ƒë·ªÉ b√°o c√°o."""
    df_loc = pd.read_sql('SELECT ID AS LOCATION_ID, CITY FROM LOCATION_DIMENSION', c)
    if df_loc.empty:
        print("‚ö†Ô∏è LOCATION_DIMENSION tr·ªëng.")
        return {}

    # Snapshot tr∆∞·ªõc khi x·ª≠ l√Ω
    snapshot_table(c, "LOCATION_DIMENSION", "before_loc_norm")

    names = df_loc["CITY"].astype(str).fillna("").str.lower().tolist()
    groups = build_similarity_groups(names, SIM_THRESHOLD_LOC)

    mapping = {}
    for grp in groups:
        ids = df_loc.iloc[grp]["LOCATION_ID"].tolist()
        canon = min(ids)
        for idx in grp:
            lid = int(df_loc.iloc[idx]["LOCATION_ID"])
            if lid != canon:
                mapping[lid] = canon

    print(f"üîé Mapping location (old->new): {mapping}")

    # C·∫≠p nh·∫≠t v√†o FACT
    with c.cursor() as cur:
        for old_id, new_id in mapping.items():
            cur.execute(
                "UPDATE GOLD_PRICE_FACT SET LOCATION_ID = :new WHERE LOCATION_ID = :old",
                {"new": new_id, "old": old_id}
            )
    c.commit()

    snapshot_table(c, "LOCATION_DIMENSION", "after_loc_norm")
    return mapping


# -------------------- GOLD TYPE enrichment --------------------
# D√πng ƒë·ªÉ t·ª± ƒë·ªông ƒëi·ªÅn th√¥ng tin c√≤n thi·∫øu (PURITY, CATEGORY) trong b·∫£ng 
#GOLD_TYPE_DIMENSION d·ª±a tr√™n s·ª± t∆∞∆°ng ƒë·ªìng c·ªßa t√™n lo·∫°i v√†ng (TYPE_NAME).
# D√πng TF-IDF + Cosine Similarity (scikit-learn) v√† FuzzyWuzzy ƒë·ªÉ t√¨m c√°c lo·∫°i v√†ng g·∫ßn gi·ªëng nhau.
# Trong m·ªói nh√≥m t∆∞∆°ng ƒë·ªìng, l·∫•y gi√° tr·ªã ph·ªï bi·∫øn nh·∫•t (mode) c·ªßa PURITY v√† CATEGORY ƒë·ªÉ ƒëi·ªÅn cho b·∫£n ghi b·ªã thi·∫øu.
def enrich_gold_types(c) -> Tuple[int, int]:
    """
    L√†m gi√†u GOLD_TYPE_DIMENSION: 
    - ƒêi·ªÅn PURITY/CATEGORY c√≤n thi·∫øu d·ª±a v√†o nh√≥m t∆∞∆°ng t·ª± TYPE_NAME.
    - B·ªï sung fallback n·∫øu nh√≥m to√†n None.
    - Ghi snapshot before/after v√† in log chi ti·∫øt thay ƒë·ªïi.
    """
    df = pd.read_sql('SELECT ID AS TYPE_ID, TYPE_NAME, PURITY, CATEGORY FROM GOLD_TYPE_DIMENSION', c)
    if df.empty:
        print("‚ö†Ô∏è GOLD_TYPE_DIMENSION tr·ªëng.")
        return (0, 0)

    snapshot_table(c, "GOLD_TYPE_DIMENSION", "before_type_enrich")

    values = df["TYPE_NAME"].astype(str).str.lower().fillna("").tolist()
    groups = build_similarity_groups(values, SIM_THRESHOLD_TYPE)

    purity_fill = 0
    category_fill = 0
    with c.cursor() as cur:
        for grp in groups:
            sub = df.iloc[grp]

            # L·∫•y mode c·ªßa c√°c gi√° tr·ªã kh√°c "unknown"/None
            known_purity = sub["PURITY"].dropna()
            known_purity = known_purity[~known_purity.astype(str).str.lower().isin(["unknown", "nan", "none", ""])]
            known_cat = sub["CATEGORY"].dropna()
            known_cat = known_cat[~known_cat.astype(str).str.lower().isin(["unknown", "nan", "none", "other", ""])]

            purity_mode = known_purity.mode().iloc[0] if not known_purity.empty else "99.99%"
            cat_mode = known_cat.mode().iloc[0] if not known_cat.empty else "Gold bar"

            for _, row in sub.iterrows():
                tid = int(row["TYPE_ID"])

                # ---- fill PURITY ----
                purity = str(row["PURITY"]).strip().lower() if row["PURITY"] is not None else ""
                if purity in ["", "unknown", "nan", "none"]:
                    cur.execute("UPDATE GOLD_TYPE_DIMENSION SET PURITY = :p WHERE ID = :id",
                                {"p": purity_mode, "id": tid})
                    purity_fill += 1

                # ---- fill CATEGORY ----
                cat = str(row["CATEGORY"]).strip().lower() if row["CATEGORY"] is not None else ""
                if cat in ["", "unknown", "nan", "none", "other"]:
                    cur.execute("UPDATE GOLD_TYPE_DIMENSION SET CATEGORY = :c WHERE ID = :id",
                                {"c": cat_mode, "id": tid})
                    category_fill += 1

    c.commit()
    snapshot_table(c, "GOLD_TYPE_DIMENSION", "after_type_enrich")

    print(f"‚ú® ƒê√£ fill PURITY: {purity_fill}, CATEGORY: {category_fill}")

    # ==== So s√°nh before/after ƒë·ªÉ xem thay ƒë·ªïi ====
    df_before = pd.read_csv(
        os.path.join(SNAPSHOT_DIR, sorted([f for f in os.listdir(SNAPSHOT_DIR) if "GOLD_TYPE_DIMENSION_before_type_enrich" in f])[-1]),
        encoding="utf-8-sig"
    )
    df_after = pd.read_csv(
        os.path.join(SNAPSHOT_DIR, sorted([f for f in os.listdir(SNAPSHOT_DIR) if "GOLD_TYPE_DIMENSION_after_type_enrich" in f])[-1]),
        encoding="utf-8-sig"
    )

    compare = df_before.merge(df_after, on="ID", suffixes=("_BEFORE", "_AFTER"))
    changed = compare[
        (compare["PURITY_BEFORE"] != compare["PURITY_AFTER"]) |
        (compare["CATEGORY_BEFORE"] != compare["CATEGORY_AFTER"])
    ]

    if not changed.empty:
        path_diff = os.path.join(SNAPSHOT_DIR, f"GOLD_TYPE_DIMENSION_diff_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx")
        changed.to_excel(path_diff, index=False)
        print(f"üìä ƒê√£ t·∫°o file so s√°nh Before/After: {path_diff}")
        for _, row in changed.iterrows():
            print(f" - {row['TYPE_NAME_BEFORE']}: PURITY {row['PURITY_BEFORE']} ‚Üí {row['PURITY_AFTER']}, CATEGORY {row['CATEGORY_BEFORE']} ‚Üí {row['CATEGORY_AFTER']}")
    else:
        print("‚ÑπÔ∏è Kh√¥ng c√≥ thay ƒë·ªïi n√†o trong PURITY ho·∫∑c CATEGORY.")

    return (purity_fill, category_fill)


# Lo·∫°i b·ªè c√°c ki·ªÉu nh·∫≠p li·ªáu kh√¥ng ƒë·ªìng nh·∫•t nh∆∞:
# "99,99", "99.9%", "99.99 % ", "99,9", "99.9"‚Ä¶
# ƒê∆∞a t·∫•t c·∫£ v·ªÅ d·∫°ng chu·∫©n ‚Äú99.99%‚Äù ƒë·ªÉ d·ªÖ so s√°nh v√† x·ª≠ l√Ω sau n√†y.
def normalize_purity_format(c):
    """
    Chu·∫©n ho√° c·ªôt PURITY trong GOLD_TYPE_DIMENSION v·ªÅ d·∫°ng 'xx.xx%'.
    V√≠ d·ª•: '99,99', '99.99', '99.99 %', '99.9', '99,9%' -> '99.99%'
    """
    df = pd.read_sql('SELECT ID, PURITY FROM GOLD_TYPE_DIMENSION', c)
    if df.empty:
        print("‚ö†Ô∏è GOLD_TYPE_DIMENSION tr·ªëng.")
        return 0

    changed = []
    with c.cursor() as cur:
        for _, row in df.iterrows():
            old_val = str(row["PURITY"]).strip() if row["PURITY"] is not None else ""
            new_val = old_val

            # B·ªè k√Ω t·ª± %, kho·∫£ng tr·∫Øng, ƒë·ªïi d·∫•u ph·∫©y sang ch·∫•m
            new_val = new_val.replace("%", "").replace(" ", "").replace(",", ".").lower()

            # Lo·∫°i b·ªè chu·ªói kh√¥ng h·ª£p l·ªá
            if new_val in ["", "none", "nan", "unknown", "unk"]:
                continue

            # L·∫•y ph·∫ßn s·ªë
            nums = re.findall(r"[\d\.]+", new_val)
            if not nums:
                continue

            try:
                val = float(nums[0])
                if val <= 0 or val > 100:
                    continue
                new_val = f"{val:.2f}%"
            except ValueError:
                continue

            if new_val != old_val:
                cur.execute("UPDATE GOLD_TYPE_DIMENSION SET PURITY = :p WHERE ID = :i", {"p": new_val, "i": int(row["ID"])})
                changed.append((row["ID"], old_val, new_val))

    c.commit()

    print(f"üîß ƒê√£ chu·∫©n ho√° PURITY cho {len(changed)} b·∫£n ghi:")
    for cid, old, new in changed[:10]:
        print(f" - ID {cid}: '{old}' ‚Üí '{new}'")
    if len(changed) > 10:
        print(f"   ... v√† {len(changed)-10} b·∫£n ghi kh√°c.")
    return len(changed)


def merge_duplicate_types_and_update_fact(c):
    """
    G·ªôp c√°c b·∫£n ghi GOLD_TYPE_DIMENSION tr√πng 4 c·ªôt (TYPE_NAME, PURITY, CATEGORY, BRAND):
    - Gi·ªØ ID nh·ªè nh·∫•t l√†m chu·∫©n.
    - C·∫≠p nh·∫≠t GOLD_PRICE_FACT.TYPE_ID v·ªÅ ID chu·∫©n.
    - G·∫Øn IS_DELETED=1 cho TYPE_ID c≈© ƒë√£ ƒë∆∞·ª£c g·ªôp.
    """
    df = pd.read_sql(
        'SELECT ID AS TYPE_ID, TYPE_NAME, PURITY, CATEGORY, BRAND FROM GOLD_TYPE_DIMENSION', c
    )
    if df.empty:
        print("‚ö†Ô∏è GOLD_TYPE_DIMENSION tr·ªëng.")
        return

    # Chu·∫©n ho√° d·ªØ li·ªáu ƒë·ªÉ tr√°nh l·ªách
    for col in ["TYPE_NAME", "PURITY", "CATEGORY", "BRAND"]:
        df[col] = df[col].astype(str).fillna("").str.strip().str.lower()

    grouped = df.groupby(["TYPE_NAME", "PURITY", "CATEGORY", "BRAND"])
    mapping = {}

    for (tname, purity, cat, brand), subdf in grouped:
        ids = subdf["TYPE_ID"].tolist()
        if len(ids) <= 1:
            continue
        canon = min(ids)
        dups = [tid for tid in ids if tid != canon]
        for tid in dups:
            mapping[tid] = canon
        print(f"‚úÖ G·ªôp '{tname}' | '{purity}' | '{cat}' | '{brand}' ‚Üí gi·ªØ ID {canon}, g·ªôp {dups}")

    if not mapping:
        print("‚ÑπÔ∏è Kh√¥ng c√≥ TYPE tr√πng c·∫ßn g·ªôp.")
        return

    # --- 1Ô∏è‚É£ C·∫≠p nh·∫≠t FACT.TYPE_ID ---
    with c.cursor() as cur:
        for old_id, new_id in mapping.items():
            cur.execute("""
                UPDATE GOLD_PRICE_FACT
                SET TYPE_ID = :new
                WHERE TYPE_ID = :old
            """, {"new": new_id, "old": old_id})
    c.commit()
    print(f"üîÅ ƒê√£ c·∫≠p nh·∫≠t GOLD_PRICE_FACT.TYPE_ID cho {len(mapping)} b·∫£n ghi tr√πng.")

    # --- 2Ô∏è‚É£ ƒê·∫£m b·∫£o c·ªôt IS_DELETED t·ªìn t·∫°i ---
    with c.cursor() as cur:
        cur.execute("""
            DECLARE v NUMBER;
            BEGIN
              SELECT 1 INTO v FROM USER_TAB_COLS 
              WHERE TABLE_NAME='GOLD_TYPE_DIMENSION' AND COLUMN_NAME='IS_DELETED';
            EXCEPTION WHEN NO_DATA_FOUND THEN
              EXECUTE IMMEDIATE 'ALTER TABLE GOLD_TYPE_DIMENSION ADD (IS_DELETED NUMBER(1) DEFAULT 0)';
            END;
        """)

        # --- 3Ô∏è‚É£ G·∫Øn IS_DELETED=1 cho TYPE_ID c≈© ƒë√£ b·ªã g·ªôp ---
        for old_id in mapping.keys():
            cur.execute("""
                UPDATE GOLD_TYPE_DIMENSION
                SET IS_DELETED = 1
                WHERE ID = :old_id
            """, {"old_id": old_id})
    c.commit()
    print(f"üßπ ƒê√£ g·∫Øn IS_DELETED=1 cho {len(mapping)} TYPE_ID c≈© ƒë√£ b·ªã g·ªôp.")




def normalize_text(s: str) -> str:
    """Chu·∫©n ho√° text d·∫°ng lowercase, b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, s·∫Øp x·∫øp t·ª´ a-z ƒë·ªÉ g·ªôp 'bar gold' = 'gold bar'"""
    s = s or ""
    s = re.sub(r'[^A-Za-z0-9]+', ' ', s)
    s = re.sub(r'\s+', ' ', s).strip().lower()
    # S·∫Øp x·∫øp t·ª´ ƒë·ªÉ g·ªôp c√°c ƒë·∫£o th·ª© t·ª± (vd: gold bar, bar gold)
    tokens = sorted(s.split())
    return " ".join(tokens)

def title_case(s: str) -> str:
    """Vi·∫øt hoa ch·ªØ ƒë·∫ßu m·ªói t·ª´"""
    return s.title()

def normalize_category_smart(c):
    """
    Chu·∫©n ho√° CATEGORY:
    - G·ªôp bi·∫øn th·ªÉ (gold bar, gold_bar, GOLD-BAR, bar gold, gold   bar,...)
    - Ch·ªçn d·∫°ng chu·∫©n vi·∫øt hoa ƒë·∫ßu t·ª´ (Gold Bar)
    - C·∫≠p nh·∫≠t tr·ª±c ti·∫øp v√†o DB
    """
    df = pd.read_sql('SELECT ID, CATEGORY FROM GOLD_TYPE_DIMENSION', c)
    if df.empty:
        print("‚ö†Ô∏è GOLD_TYPE_DIMENSION tr·ªëng.")
        return 0

    # Chu·∫©n ho√° & nh√≥m t∆∞∆°ng ƒë·ªìng
    df["CLEAN"] = df["CATEGORY"].astype(str).apply(normalize_text)
    unique_vals = df["CLEAN"].unique().tolist()

    # Gom nh√≥m theo ƒë·ªô t∆∞∆°ng ƒë·ªìng fuzzy
    groups = []
    visited = set()
    for i, base in enumerate(unique_vals):
        if base in visited:
            continue
        group = [base]
        visited.add(base)
        for j, other in enumerate(unique_vals):
            if other in visited:
                continue
            if fuzz.token_set_ratio(base, other) >= 90:
                group.append(other)
                visited.add(other)
        groups.append(group)

    # X√°c ƒë·ªãnh gi√° tr·ªã chu·∫©n c·ªßa m·ªói nh√≥m
    mapping = {}
    for grp in groups:
        canon = sorted(grp, key=len)[0]  # l·∫•y chu·ªói ng·∫Øn nh·∫•t l√†m chu·∫©n
        for val in grp:
            mapping[val] = canon

    # C·∫≠p nh·∫≠t DB
    changed = []
    with c.cursor() as cur:
        for _, row in df.iterrows():
            old_raw = row["CATEGORY"]
            clean = row["CLEAN"]
            canon_clean = mapping.get(clean, clean)
            new_val = title_case(canon_clean)
            if new_val != old_raw:
                cur.execute("UPDATE GOLD_TYPE_DIMENSION SET CATEGORY = :c WHERE ID = :i",
                            {"c": new_val, "i": int(row["ID"])})
                changed.append((row["ID"], old_raw, new_val))
        c.commit()

    print(f"‚úÖ ƒê√£ chu·∫©n ho√° CATEGORY cho {len(changed)} b·∫£n ghi:")
    for cid, old, new in changed[:10]:
        print(f" - ID {cid}: '{old}' ‚Üí '{new}'")
    if len(changed) > 10:
        print(f"   ... v√† {len(changed) - 10} b·∫£n ghi kh√°c.")
    return len(changed)




# -------------------- FACT dedup incremental --------------------

def dedup_fact_incremental(c, last_run: dt.datetime):
    # ch·ª´a bi√™n 1 ng√†y ƒë·ªÉ an to√†n
    floor_ts = last_run - dt.timedelta(days=1)
    q = """
        SELECT ID, SOURCE_ID, TYPE_ID, LOCATION_ID, TIME_ID, RECORDED_AT, IS_DELETED
        FROM GOLD_PRICE_FACT
        WHERE RECORDED_AT >= :ts
    """
    df = pd.read_sql(q, c, params={"ts": floor_ts})
    if df.empty:
        print("‚ÑπÔ∏è Kh√¥ng c√≥ FACT m·ªõi ƒë·ªÉ dedup.")
        return 0

    # combo key 4 tr∆∞·ªùng
    df["COMBO"] = (
        df["SOURCE_ID"].astype(str) + "_" +
        df["TYPE_ID"].astype(str) + "_" +
        df["LOCATION_ID"].astype(str) + "_" +
        df["TIME_ID"].astype(str)
    )
    # gi·ªØ b·∫£n m·ªõi nh·∫•t theo RECORDED_AT
    keep_idx = df.sort_values(["COMBO", "RECORDED_AT"]).groupby("COMBO").tail(1).index
    to_mark = df.index.difference(keep_idx)
    n_dup = len(to_mark)

    if n_dup > 0:
        with c.cursor() as cur:
            ids = df.loc[to_mark, "ID"].astype(int).tolist()
            for pid in ids:
                cur.execute("UPDATE GOLD_PRICE_FACT SET IS_DELETED = 1, IS_DELETE = 1 WHERE ID = :p", {"p": pid})
        c.commit()

    snapshot_table(c, "GOLD_PRICE_FACT", "after_fact_dedup")
    print(f"üßπ ƒê√£ g·∫Øn IS_DELETED/IS_DELETE=1 cho {n_dup} b·∫£n ghi tr√πng.")
    return n_dup

    
def handle_missing_values_fact(c, last_run: dt.datetime):
    """
    X·ª≠ l√Ω missing values (incremental):
    - Ch·ªâ ki·ªÉm tra d·ªØ li·ªáu m·ªõi ho·∫∑c c·∫≠p nh·∫≠t sau last_run.
    - Drop b·∫£n ghi thi·∫øu BUY_PRICE, SELL_PRICE, TIME_ID.
    - Impute UNIT b·∫±ng mode.
    - Flag IS_DELETED=1 cho b·∫£n ghi kh√¥ng h·ª£p l·ªá.
    """
    floor_ts = last_run - dt.timedelta(days=1)
    q = """
        SELECT ID, BUY_PRICE, SELL_PRICE, TIME_ID, UNIT, RECORDED_AT
        FROM GOLD_PRICE_FACT
        WHERE RECORDED_AT >= :ts
    """
    df = pd.read_sql(q, c, params={"ts": floor_ts})
    if df.empty:
        print("‚ÑπÔ∏è Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi ƒë·ªÉ x·ª≠ l√Ω missing values.")
        return 0

    before = len(df)
    df_clean = df.dropna(subset=["BUY_PRICE", "SELL_PRICE", "TIME_ID"])

    if "UNIT" in df_clean.columns:
        mode_val = df_clean["UNIT"].mode().iloc[0] if not df_clean["UNIT"].isna().all() else "VND/L∆∞·ª£ng"
        df_clean["UNIT"] = df_clean["UNIT"].fillna(mode_val)

    dropped = before - len(df_clean)
    dropped_ids = df.loc[df.index.difference(df_clean.index), "ID"].astype(int).tolist()

    if dropped_ids:
        with c.cursor() as cur:
            for pid in dropped_ids:
                cur.execute("UPDATE GOLD_PRICE_FACT SET IS_DELETED = 1, IS_DELETE = 1 WHERE ID = :p", {"p": pid})
        c.commit()

    snapshot_table(c, "GOLD_PRICE_FACT", "after_handle_missing")
    print(f"üß© ƒê√£ flag {dropped} b·∫£n ghi thi·∫øu gi√° ho·∫∑c th·ªùi gian (IS_DELETED=1).")
    return dropped




def flag_price_outliers(c, last_run: dt.datetime):
    """
    Ph√°t hi·ªán outlier gi√° mua/b√°n (incremental):
    - D·ª±a tr√™n IQR method.
    - Ch·ªâ x·ª≠ l√Ω d·ªØ li·ªáu m·ªõi ho·∫∑c c·∫≠p nh·∫≠t sau last_run.
    - Flag IS_DELETED=1 cho c√°c b·∫£n ghi v∆∞·ª£t ng∆∞·ª°ng.
    """
    floor_ts = last_run - dt.timedelta(days=1)
    q = """
        SELECT ID, BUY_PRICE, SELL_PRICE, RECORDED_AT
        FROM GOLD_PRICE_FACT
        WHERE RECORDED_AT >= :ts
    """
    df = pd.read_sql(q, c, params={"ts": floor_ts})
    if df.empty:
        print("‚ÑπÔ∏è Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi ƒë·ªÉ flag outlier.")
        return 0

    for col in ["BUY_PRICE", "SELL_PRICE"]:
        q1, q3 = df[col].quantile([0.25, 0.75])
        iqr = q3 - q1
        lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr
        df[f"{col}_OUTLIER"] = ((df[col] < lower) | (df[col] > upper)).astype(int)

    df["IS_DELETED"] = ((df["BUY_PRICE_OUTLIER"] == 1) | (df["SELL_PRICE_OUTLIER"] == 1)).astype(int)
    flagged = df["IS_DELETED"].sum()

    with c.cursor() as cur:
        cur.execute("""
            DECLARE
                v_dummy NUMBER;
            BEGIN
                SELECT 1 INTO v_dummy FROM USER_TAB_COLS 
                WHERE TABLE_NAME='GOLD_PRICE_FACT' AND COLUMN_NAME='IS_DELETED';
            EXCEPTION
                WHEN NO_DATA_FOUND THEN
                    EXECUTE IMMEDIATE 'ALTER TABLE GOLD_PRICE_FACT ADD (IS_DELETED NUMBER(1) DEFAULT 0)';
            END;
        """)
        for _, r in df[df["IS_DELETED"] == 1].iterrows():
            cur.execute("UPDATE GOLD_PRICE_FACT SET IS_DELETED = 1 WHERE ID = :i", {"i": int(r["ID"])})

    c.commit()
    snapshot_table(c, "GOLD_PRICE_FACT", "after_outlier_flag")
    print(f"‚ö†Ô∏è ƒê√£ flag {flagged} b·∫£n ghi outlier m·ªõi (IS_DELETED=1).")
    return flagged




def normalize_gold_type_and_unit(c):
    """
    Chu·∫©n h√≥a th∆∞∆°ng hi·ªáu v√† ƒë∆°n v·ªã ƒëo trong d·ªØ li·ªáu v√†ng.
    - Chu·∫©n h√≥a BRAND: b·ªè k√Ω t·ª± th·ª´a, vi·∫øt hoa.
    - Chu·∫©n h√≥a UNIT: quy v·ªÅ 'VND/L∆∞·ª£ng'.
    """
    with c.cursor() as cur:
        # BRAND normalization
        cur.execute("""
            UPDATE GOLD_TYPE_DIMENSION 
            SET BRAND = UPPER(TRIM(REPLACE(REPLACE(BRAND, '.', ''), 'V√ÄNG ', '')))
            WHERE BRAND IS NOT NULL
        """)

        # UNIT normalization
        cur.execute("""
            UPDATE GOLD_PRICE_FACT 
            SET UNIT = 'VND/L∆∞·ª£ng'
            WHERE UNIT IN ('VND/chi', 'VND/ch·ªâ', 'tri·ªáu/l∆∞·ª£ng', 'VND/gram', 'VND/Gr', 'VND/luong')
        """)

    c.commit()
    snapshot_table(c, "GOLD_PRICE_FACT", "after_unit_norm")
    print("üìè ƒê√£ chu·∫©n h√≥a BRAND v√† UNIT v·ªÅ d·∫°ng chu·∫©n.")



# --------------------------- MAIN ---------------------------

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--merge-types", action="store_true", help="Th·ª≠ g·ªôp TYPE t∆∞∆°ng ƒë·ªìng v·ªÅ 1 ID v√† c·∫≠p nh·∫≠t FACT") 
    args = parser.parse_args()

    c = conn()
    ensure_infra(c)

    # Snapshot tr∆∞·ªõc khi x·ª≠ l√Ω ƒë·ªÉ ch·ª•p "before"
    snapshot_table(c, "LOCATION_DIMENSION", "before")
    snapshot_table(c, "GOLD_TYPE_DIMENSION", "before")
    snapshot_table(c, "GOLD_PRICE_FACT", "before")

    last_run = get_last_checkpoint(c)
    print(f"‚è±Ô∏è Last checkpoint: {last_run}")

    # B1: LOCATION normalize
    normalize_locations(c)

    # B2: GOLD TYPE enrich
    enrich_gold_types(c)

    # B2.1: Chu·∫©n ho√° PURITY v·ªÅ d·∫°ng chu·∫©n xx.xx%
    normalize_purity_format(c)
    normalize_category_smart(c)

    # (Tu·ª≥ ch·ªçn) g·ªôp TYPE t∆∞∆°ng ƒë·ªìng v√† c·∫≠p nh·∫≠t FACT
    if args.merge_types:
        merge_duplicate_types_and_update_fact(c)

    handle_missing_values_fact(c,last_run)
    flag_price_outliers(c,last_run)
    normalize_gold_type_and_unit(c)
    # B3: FACT dedup incremental
    dedup_fact_incremental(c, last_run)

    # c·∫≠p nh·∫≠t checkpoint
    now = dt.datetime.now()
    set_checkpoint(c, now)
    print(f"‚úÖ Job ho√†n t·∫•t. Checkpoint m·ªõi: {now}")

    # Snapshot cu·ªëi ƒë·ªÉ ch·ª•p "after"
    snapshot_table(c, "LOCATION_DIMENSION", "final")
    snapshot_table(c, "GOLD_TYPE_DIMENSION", "final")
    snapshot_table(c, "GOLD_PRICE_FACT", "final")

    c.close()

if __name__ == "__main__":
    import time
    import datetime as dt

    while True:
        now = dt.datetime.now()
        run_time = dt.time(7, 0, 0)  # ch·∫°y m·ªói s√°ng l√∫c 7:00

        # --- Ch·∫°y ngay l·∫ßn ƒë·∫ßu ---
        print(f"üöÄ L·∫ßn ƒë·∫ßu ch·∫°y job l√∫c {now}")
        try:
            main()
        except Exception as e:
            print(f"‚ùå L·ªói khi ch·∫°y job l·∫ßn ƒë·∫ßu: {e}")

        # --- Sau ƒë√≥ ch·ªù ƒë·∫øn s√°ng h√¥m sau ---
        while True:
            now = dt.datetime.now()
            if now.time().hour == run_time.hour and now.time().minute == run_time.minute:
                print(f"‚è∞ {now} - B·∫Øt ƒë·∫ßu ch·∫°y job bu·ªïi s√°ng...")
                try:
                    main()
                except Exception as e:
                    print(f"‚ùå L·ªói khi ch·∫°y job bu·ªïi s√°ng: {e}")
                # Sau khi ch·∫°y, ch·ªù 24 ti·∫øng (ƒë·ªÉ kh√¥ng ch·∫°y l·∫°i c√πng ng√†y)
                time.sleep(24 * 3600)
            else:
                # ki·ªÉm tra l·∫°i sau m·ªói 5 ph√∫t
                time.sleep(300)