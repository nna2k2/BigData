# -*- coding: utf-8 -*-
"""
Daily Gold ETL Job - Spark Structured Streaming v·ªõi Oracle Polling
T·ª± ƒë·ªông ph√°t hi·ªán v√† x·ª≠ l√Ω khi Oracle database thay ƒë·ªïi

Gi·∫£i ph√°p: Spark Structured Streaming + foreachBatch
- D√πng memory source l√†m trigger
- foreachBatch ƒë·ªÉ polling Oracle m·ªói interval
- C√≥ checkpoint t·ª± ƒë·ªông, recovery, monitoring
"""

import argparse
import datetime as dt
import os
import sys
from typing import Dict, List, Tuple, Optional

import pandas as pd
import numpy as np
from pyspark.sql import SparkSession
from pyspark.sql.streaming import StreamingQuery
from pyspark.sql.functions import (
    col, when, lit, trim, upper, lower, regexp_replace, 
    concat_ws, first, last, max as spark_max, min as spark_min,
    count, isnan, isnull, coalesce, to_timestamp, date_format,
    row_number, window, monotonically_increasing_id, current_timestamp
)
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, FloatType, 
    TimestampType, DoubleType, LongType
)
from pyspark.sql.window import Window

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from fuzzywuzzy import fuzz

import re
import unicodedata

# Import c√°c h√†m clean t·ª´ batch job
# Th√™m ƒë∆∞·ªùng d·∫´n ƒë·ªÉ import
sys.path.insert(0, os.path.dirname(__file__))
try:
    from daily_gold_job_normalization_spark import (
        normalize_locations,
        enrich_gold_types,
        normalize_purity_format,
        normalize_category_smart,
        normalize_gold_type_and_unit,
        merge_duplicate_types_and_update_fact,
        build_similarity_groups,
        norm_txt,
        snapshot_table
    )
    BATCH_FUNCTIONS_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ import batch functions: {e}")
    print("   S·∫Ω ch·ªâ x·ª≠ l√Ω FACT, kh√¥ng clean LOCATION v√† TYPE")
    BATCH_FUNCTIONS_AVAILABLE = False

# ====================== CONFIG ======================
# ƒê·ªçc t·ª´ environment variables (Docker) ho·∫∑c d√πng gi√° tr·ªã m·∫∑c ƒë·ªãnh
DB_USER = os.environ.get("DB_USER", "SYSTEM")
DB_PASS = os.environ.get("DB_PASS", "Welcome_1234")
DB_HOST = os.environ.get("DB_HOST", "136.110.60.196")
DB_PORT = os.environ.get("DB_PORT", "1521")
DB_SERVICE = os.environ.get("DB_SERVICE", "XEPDB1")

DB_DSN = f"{DB_HOST}:{DB_PORT}/{DB_SERVICE}"
DB_URL = f"jdbc:oracle:thin:@{DB_DSN}"

SNAPSHOT_DIR = "./snapshots"
JOB_NAME = "DAILY_GOLD_JOB_STREAMING_ORACLE"
SIM_THRESHOLD_LOC = 0.80
SIM_THRESHOLD_TYPE = 0.75
FUZZY_FALLBACK = 90

# C√°c constants c·∫ßn thi·∫øt cho batch functions (n·∫øu import ƒë∆∞·ª£c)
try:
    from daily_gold_job_normalization_spark import (
        SIM_THRESHOLD_LOC as BATCH_SIM_THRESHOLD_LOC,
        SIM_THRESHOLD_TYPE as BATCH_SIM_THRESHOLD_TYPE,
        FUZZY_FALLBACK as BATCH_FUZZY_FALLBACK
    )
except:
    pass

# Streaming config
STREAMING_CHECKPOINT_DIR = "./checkpoints/streaming_oracle"
STREAMING_TRIGGER_INTERVAL = "60 seconds"  # Polling m·ªói 60 gi√¢y
TIMESTAMP_COLUMN = "RECORDED_AT"  # C·ªôt timestamp ƒë·ªÉ ph√°t hi·ªán thay ƒë·ªïi

# Spark config
SPARK_APP_NAME = "DailyGoldETLJobStreamingOracle"
SPARK_MASTER = "local[*]"

# ====================================================

def create_spark_session(ojdbc_path: str = None):
    """T·∫°o SparkSession v·ªõi c·∫•u h√¨nh streaming."""
    builder = SparkSession.builder \
        .appName(SPARK_APP_NAME) \
        .master(SPARK_MASTER) \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.driver.memory", "2g") \
        .config("spark.executor.memory", "2g") \
        .config("spark.sql.streaming.checkpointLocation", STREAMING_CHECKPOINT_DIR)
    
    # Th√™m JDBC driver n·∫øu c√≥
    if ojdbc_path:
        if os.path.exists(ojdbc_path):
            builder = builder.config("spark.jars", ojdbc_path)
            print(f"‚úÖ ƒê√£ load JDBC driver t·ª´: {ojdbc_path}")
        else:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y JDBC driver t·∫°i: {ojdbc_path}")
    else:
        possible_paths = [
            "ojdbc8.jar",
            "./ojdbc8.jar",
            "../ojdbc8.jar",
            os.path.join(os.path.dirname(__file__), "ojdbc8.jar")
        ]
        for path in possible_paths:
            if os.path.exists(path):
                builder = builder.config("spark.jars", os.path.abspath(path))
                print(f"‚úÖ ƒê√£ t·ª± ƒë·ªông t√¨m th·∫•y JDBC driver: {os.path.abspath(path)}")
                break
    
    spark = builder.getOrCreate()
    return spark

def read_table_from_oracle(spark: SparkSession, table_name: str, schema: str = None) -> 'DataFrame':
    """ƒê·ªçc b·∫£ng t·ª´ Oracle DB (batch)."""
    schema_prefix = f'"{schema}"."' if schema else '"'
    full_table = f'{schema_prefix}{table_name}"'
    
    df = spark.read \
        .format("jdbc") \
        .option("url", f"jdbc:oracle:thin:{DB_USER}/{DB_PASS}@{DB_DSN}") \
        .option("dbtable", full_table) \
        .option("driver", "oracle.jdbc.driver.OracleDriver") \
        .load()
    return df

def read_new_data_from_oracle(spark: SparkSession, table_name: str, 
                              last_timestamp: dt.datetime,
                              timestamp_column: str = TIMESTAMP_COLUMN) -> 'DataFrame':
    """
    ƒê·ªçc ch·ªâ d·ªØ li·ªáu M·ªöI t·ª´ Oracle d·ª±a tr√™n timestamp.
    
    Logic:
    - Query: WHERE RECORDED_AT > last_timestamp
    - Ch·ªâ l·∫•y d·ªØ li·ªáu sau timestamp cu·ªëi c√πng ƒë√£ x·ª≠ l√Ω
    - ORDER BY timestamp ƒë·ªÉ ƒë·∫£m b·∫£o th·ª© t·ª±
    
    Args:
        spark: SparkSession
        table_name: T√™n b·∫£ng Oracle
        last_timestamp: Timestamp cu·ªëi c√πng ƒë√£ x·ª≠ l√Ω
        timestamp_column: T√™n c·ªôt timestamp (m·∫∑c ƒë·ªãnh: RECORDED_AT)
    
    Returns:
        DataFrame: D·ªØ li·ªáu m·ªõi sau last_timestamp
    """
    schema_prefix = f'"{DB_USER}"."'
    full_table = f'{schema_prefix}{table_name}"'
    
    # T·∫°o query ƒë·ªÉ ch·ªâ l·∫•y d·ªØ li·ªáu m·ªõi
    # D√πng > (l·ªõn h∆°n) ƒë·ªÉ tr√°nh l·∫•y l·∫°i record ƒë√£ x·ª≠ l√Ω
    ts_str = last_timestamp.strftime('%Y-%m-%d %H:%M:%S')
    query = f"""
        (SELECT * FROM {full_table}
         WHERE {timestamp_column} > TO_TIMESTAMP('{ts_str}', 'YYYY-MM-DD HH24:MI:SS')
         ORDER BY {timestamp_column})
    """
    
    print(f"   üîç Query: WHERE {timestamp_column} > '{ts_str}'")
    
    try:
        df = spark.read \
            .format("jdbc") \
            .option("url", f"jdbc:oracle:thin:{DB_USER}/{DB_PASS}@{DB_DSN}") \
            .option("dbtable", query) \
            .option("driver", "oracle.jdbc.driver.OracleDriver") \
            .load()
        
        count = df.count()
        if count > 0:
            # L·∫•y min v√† max timestamp ƒë·ªÉ log
            min_ts = df.agg(spark_min(col(timestamp_column))).first()[0]
            max_ts = df.agg(spark_max(col(timestamp_column))).first()[0]
            print(f"   ‚úÖ T√¨m th·∫•y {count} records m·ªõi (t·ª´ {min_ts} ƒë·∫øn {max_ts})")
        else:
            print(f"   ‚ÑπÔ∏è Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi sau {ts_str}")
        
        return df
    except Exception as e:
        print(f"   ‚ö†Ô∏è L·ªói khi ƒë·ªçc d·ªØ li·ªáu m·ªõi: {e}")
        print(f"   üìù Tr·∫£ v·ªÅ DataFrame r·ªóng")
        return spark.createDataFrame([], get_fact_schema())

def get_last_timestamp_from_checkpoint(spark: SparkSession) -> dt.datetime:
    """
    L·∫•y timestamp cu·ªëi c√πng t·ª´ checkpoint.
    
    Logic:
    1. ƒê·ªçc t·ª´ b·∫£ng ETL_CHECKPOINT v·ªõi JOB_NAME
    2. N·∫øu kh√¥ng c√≥, l·∫•y max timestamp t·ª´ GOLD_PRICE_FACT
    3. N·∫øu v·∫´n kh√¥ng c√≥, d√πng 2000-01-01 l√†m m·∫∑c ƒë·ªãnh
    
    Returns:
        dt.datetime: Timestamp cu·ªëi c√πng ƒë√£ x·ª≠ l√Ω
    """
    try:
        df = read_table_from_oracle(spark, "ETL_CHECKPOINT", DB_USER)
        df_checkpoint = df.filter(col("JOB_NAME") == JOB_NAME)
        
        if df_checkpoint.count() > 0:
            last_run = df_checkpoint.select("LAST_RUN").first()
            if last_run and last_run[0]:
                last_ts = last_run[0]
                print(f"üìå Checkpoint t√¨m th·∫•y: {last_ts}")
                return last_ts
            else:
                print("‚ö†Ô∏è Checkpoint c√≥ record nh∆∞ng LAST_RUN l√† NULL")
        else:
            print("‚ÑπÔ∏è Ch∆∞a c√≥ checkpoint trong ETL_CHECKPOINT, ƒëang t√¨m trong FACT...")
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c checkpoint: {e}")
        print("   ƒêang fallback sang FACT table...")
    
    # N·∫øu ch∆∞a c√≥ checkpoint, l·∫•y timestamp t·ª´ b·∫£ng FACT
    try:
        df_fact = read_table_from_oracle(spark, "GOLD_PRICE_FACT", DB_USER)
        if df_fact.count() > 0:
            max_ts = df_fact.agg(spark_max(col(TIMESTAMP_COLUMN))).first()[0]
            if max_ts:
                print(f"üìå L·∫•y max timestamp t·ª´ FACT: {max_ts}")
                return max_ts
            else:
                print("‚ö†Ô∏è FACT c√≥ d·ªØ li·ªáu nh∆∞ng kh√¥ng c√≥ timestamp h·ª£p l·ªá")
        else:
            print("‚ÑπÔ∏è FACT table tr·ªëng")
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c timestamp t·ª´ FACT: {e}")
    
    default_ts = dt.datetime(2000, 1, 1)
    print(f"üìå S·ª≠ d·ª•ng timestamp m·∫∑c ƒë·ªãnh: {default_ts}")
    return default_ts

def update_checkpoint(spark: SparkSession, ts: dt.datetime):
    """
    C·∫≠p nh·∫≠t checkpoint v·ªõi timestamp m·ªõi nh·∫•t.
    
    Logic:
    1. ƒê·ªçc t·∫•t c·∫£ records t·ª´ ETL_CHECKPOINT
    2. Filter ra record c·ªßa job kh√°c (gi·ªØ l·∫°i)
    3. Union v·ªõi record m·ªõi c·ªßa job n√†y
    4. Overwrite to√†n b·ªô b·∫£ng (c√≥ th·ªÉ c·∫£i thi·ªán b·∫±ng MERGE/UPDATE)
    
    Args:
        spark: SparkSession
        ts: Timestamp m·ªõi nh·∫•t ƒë√£ x·ª≠ l√Ω
    """
    print(f"üíæ ƒêang c·∫≠p nh·∫≠t checkpoint v·ªõi timestamp: {ts}")
    
    checkpoint_df = spark.createDataFrame(
        [(JOB_NAME, ts)],
        ["JOB_NAME", "LAST_RUN"]
    )
    
    try:
        # ƒê·ªçc t·∫•t c·∫£ records hi·ªán c√≥
        existing = read_table_from_oracle(spark, "ETL_CHECKPOINT", DB_USER)
        existing_count = existing.count()
        print(f"   üìä Records hi·ªán c√≥ trong checkpoint: {existing_count}")
        
        # Gi·ªØ l·∫°i records c·ªßa job kh√°c, th√™m/update record c·ªßa job n√†y
        other_jobs = existing.filter(col("JOB_NAME") != JOB_NAME)
        other_count = other_jobs.count()
        print(f"   üìä Records c·ªßa job kh√°c: {other_count}")
        
        combined = other_jobs.union(checkpoint_df)
        combined_count = combined.count()
        print(f"   üìä T·ªïng records sau merge: {combined_count}")
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c checkpoint hi·ªán c√≥: {e}")
        print(f"   üìù S·∫Ω t·∫°o checkpoint m·ªõi")
        combined = checkpoint_df
    
    # Ghi l·∫°i to√†n b·ªô b·∫£ng (c√≥ th·ªÉ c·∫£i thi·ªán b·∫±ng MERGE/UPDATE trong t∆∞∆°ng lai)
    try:
        combined.write \
            .format("jdbc") \
            .option("url", f"jdbc:oracle:thin:{DB_USER}/{DB_PASS}@{DB_DSN}") \
            .option("dbtable", f"{DB_USER}.ETL_CHECKPOINT") \
            .option("driver", "oracle.jdbc.driver.OracleDriver") \
            .mode("overwrite") \
            .save()
        print(f"   ‚úÖ ƒê√£ c·∫≠p nh·∫≠t checkpoint th√†nh c√¥ng")
    except Exception as e:
        print(f"   ‚ùå L·ªói khi c·∫≠p nh·∫≠t checkpoint: {e}")
        raise

def write_table_to_oracle(df: 'DataFrame', table_name: str, mode: str = "append"):
    """Ghi DataFrame v√†o Oracle DB."""
    if df.count() == 0:
        return
    
    df.write \
        .format("jdbc") \
        .option("url", f"jdbc:oracle:thin:{DB_USER}/{DB_PASS}@{DB_DSN}") \
        .option("dbtable", table_name) \
        .option("driver", "oracle.jdbc.driver.OracleDriver") \
        .mode(mode) \
        .save()

def get_fact_schema():
    """Schema cho GOLD_PRICE_FACT."""
    return StructType([
        StructField("SOURCE_ID", IntegerType(), True),
        StructField("TYPE_ID", IntegerType(), True),
        StructField("LOCATION_ID", IntegerType(), True),
        StructField("TIME_ID", IntegerType(), True),
        StructField("BUY_PRICE", DoubleType(), True),
        StructField("SELL_PRICE", DoubleType(), True),
        StructField("RECORDED_AT", TimestampType(), True),
        StructField("UNIT", StringType(), True),
    ])

# ==================== PROCESSING FUNCTIONS ====================

def process_new_fact_data(spark: SparkSession, df_new: 'DataFrame',
                          location_mapping: Dict, type_mapping: Dict) -> 'DataFrame':
    """
    X·ª≠ l√Ω d·ªØ li·ªáu FACT m·ªõi:
    1. Apply location/type mappings
    2. Deduplicate
    3. Handle missing values
    4. Flag outliers
    """
    if df_new.count() == 0:
        return df_new
    
    # Apply location mapping
    if location_mapping:
        mapping_df = spark.createDataFrame(
            [(k, v) for k, v in location_mapping.items()],
            ["OLD_LOC_ID", "NEW_LOC_ID"]
        )
        df_new = df_new.join(
            mapping_df,
            df_new["LOCATION_ID"] == mapping_df["OLD_LOC_ID"],
            "left"
        ).withColumn(
            "LOCATION_ID",
            when(col("NEW_LOC_ID").isNotNull(), col("NEW_LOC_ID"))
            .otherwise(col("LOCATION_ID"))
        ).drop("OLD_LOC_ID", "NEW_LOC_ID")
    
    # Apply type mapping
    if type_mapping:
        mapping_df = spark.createDataFrame(
            [(k, v) for k, v in type_mapping.items()],
            ["OLD_TYPE_ID", "NEW_TYPE_ID"]
        )
        df_new = df_new.join(
            mapping_df,
            df_new["TYPE_ID"] == mapping_df["OLD_TYPE_ID"],
            "left"
        ).withColumn(
            "TYPE_ID",
            when(col("NEW_TYPE_ID").isNotNull(), col("NEW_TYPE_ID"))
            .otherwise(col("TYPE_ID"))
        ).drop("OLD_TYPE_ID", "NEW_TYPE_ID")
    
    # Deduplicate (gi·ªØ record m·ªõi nh·∫•t)
    df_new = df_new.withColumn(
        "COMBO",
        concat_ws("|",
            col("SOURCE_ID").cast("string"),
            col("TYPE_ID").cast("string"),
            col("LOCATION_ID").cast("string"),
            col("TIME_ID").cast("string")
        )
    )
    
    window_spec = Window.partitionBy("COMBO").orderBy(col(TIMESTAMP_COLUMN).desc())
    df_new = df_new.withColumn("rn", row_number().over(window_spec)) \
        .filter(col("rn") == 1) \
        .drop("rn", "COMBO")
    
    # Handle missing values
    df_new = df_new.filter(
        col("BUY_PRICE").isNotNull() &
        col("SELL_PRICE").isNotNull() &
        col(TIMESTAMP_COLUMN).isNotNull()
    )
    
    # Flag outliers
    df_new = df_new.withColumn("IS_DELETED", lit(0))
    df_new = df_new.withColumn("IS_DELETE", lit(0))
    
    return df_new

def load_dimension_mappings(spark: SparkSession) -> Tuple[Dict, Dict]:
    """Load location v√† type mappings t·ª´ CLEAN tables."""
    location_mapping = {}
    type_mapping = {}
    
    # TODO: Implement logic ƒë·ªÉ load mappings
    # C√≥ th·ªÉ load t·ª´ LOCATION_DIMENSION_CLEAN v√† GOLD_TYPE_DIMENSION_CLEAN
    
    return location_mapping, type_mapping

# ==================== STREAMING WITH FOREACHBATCH ====================

def merge_duplicate_types_and_update_fact_streaming(spark: SparkSession) -> Dict:
    """
    G·ªôp c√°c b·∫£n ghi tr√πng trong GOLD_TYPE_DIMENSION_CLEAN v√† c·∫≠p nh·∫≠t l·∫°i b·∫£ng CLEAN.
    
    ‚ö†Ô∏è QUAN TR·ªåNG - TUY·ªÜT ƒê·ªêI KH√îNG ƒê·ªòNG ƒê·∫æN B·∫¢NG G·ªêC: 
    - ‚úÖ CH·ªà ƒë·ªçc t·ª´: GOLD_TYPE_DIMENSION_CLEAN
    - ‚úÖ Merge c√°c records tr√πng trong CLEAN (gi·ªØ 1 record cho m·ªói group)
    - ‚úÖ C·∫≠p nh·∫≠t l·∫°i b·∫£ng CLEAN v·ªõi d·ªØ li·ªáu ƒë√£ merge
    - ‚úÖ T·∫°o mapping ƒë·ªÉ d√πng cho FACT
    - ‚ùå KH√îNG ƒë·ªçc t·ª´: GOLD_TYPE_DIMENSION (b·∫£ng g·ªëc)
    - ‚ùå KH√îNG ghi v√†o: GOLD_TYPE_DIMENSION (b·∫£ng g·ªëc)
    """
    from decimal import Decimal
    
    # ‚ö†Ô∏è QUAN TR·ªåNG: CH·ªà ƒë·ªçc t·ª´ b·∫£ng CLEAN, KH√îNG ƒë·ªçc t·ª´ b·∫£ng g·ªëc GOLD_TYPE_DIMENSION
    df = read_table_from_oracle(spark, "GOLD_TYPE_DIMENSION_CLEAN", DB_USER)
    
    if df.count() == 0:
        print("‚ö†Ô∏è GOLD_TYPE_DIMENSION_CLEAN tr·ªëng.")
        return {}

    original_count = df.count()
    print(f"üìä B·∫£ng CLEAN hi·ªán c√≥: {original_count} records")
    
    # ‚ö†Ô∏è QUAN TR·ªåNG: L∆∞u backup d·ªØ li·ªáu CLEAN c≈© ƒë·ªÉ restore n·∫øu merge th·∫•t b·∫°i
    df_backup = df
    print(f"   üíæ ƒê√£ backup {original_count} records ƒë·ªÉ ph·ª•c h·ªìi n·∫øu c·∫ßn")

    # Ki·ªÉm tra c√°c c·ªôt c√≥ t·ªìn t·∫°i kh√¥ng
    columns = df.columns
    df_normalized = df.withColumn(
        "TYPE_NAME_NORM", lower(trim(col("TYPE_NAME")))
    ).withColumn(
        "PURITY_NORM", lower(trim(col("PURITY")))
    ).withColumn(
        "CATEGORY_NORM", lower(trim(col("CATEGORY")))
    )
    
    # Ch·ªâ th√™m BRAND_NORM n·∫øu c·ªôt BRAND t·ªìn t·∫°i
    if "BRAND" in columns:
        df_normalized = df_normalized.withColumn(
            "BRAND_NORM", lower(trim(col("BRAND")))
        )
        partition_cols = ["TYPE_NAME_NORM", "PURITY_NORM", "CATEGORY_NORM", "BRAND_NORM"]
    else:
        # T·∫°o c·ªôt BRAND_NORM r·ªóng n·∫øu kh√¥ng c√≥ BRAND
        df_normalized = df_normalized.withColumn("BRAND_NORM", lit(""))
        partition_cols = ["TYPE_NAME_NORM", "PURITY_NORM", "CATEGORY_NORM", "BRAND_NORM"]
        print("‚ö†Ô∏è C·ªôt BRAND kh√¥ng t·ªìn t·∫°i, s·ª≠ d·ª•ng gi√° tr·ªã r·ªóng cho BRAND_NORM")

    # Group by normalized values and find canonical ID (ID nh·ªè nh·∫•t trong m·ªói group)
    window_spec = Window.partitionBy(*partition_cols).orderBy("ID")
    
    df_with_canon = df_normalized.withColumn(
        "CANON_ID",
        first("ID").over(window_spec)
    )

    # Create mapping: old_id -> new_id (canonical_id)
    mapping_df = df_with_canon.filter(col("ID") != col("CANON_ID")) \
        .select(col("ID").alias("OLD_ID"), col("CANON_ID").alias("NEW_ID")) \
        .distinct()
    
    mapping = {}
    if mapping_df.count() > 0:
        for row in mapping_df.collect():
            # X·ª≠ l√Ω OLD_ID v√† NEW_ID - c√≥ th·ªÉ l√† Decimal, float, int, ho·∫∑c NaN
            old_id_val = row["OLD_ID"]
            new_id_val = row["NEW_ID"]
            
            # Skip n·∫øu c√≥ gi√° tr·ªã None ho·∫∑c NaN
            if old_id_val is None or new_id_val is None:
                continue
            try:
                # Convert OLD_ID
                if isinstance(old_id_val, int):
                    old_id = old_id_val
                elif isinstance(old_id_val, (float, Decimal)):
                    if pd.isna(old_id_val):
                        continue
                    old_id = int(old_id_val)
                else:
                    old_id = int(float(str(old_id_val)))
                
                # Convert NEW_ID
                if isinstance(new_id_val, int):
                    new_id = new_id_val
                elif isinstance(new_id_val, (float, Decimal)):
                    if pd.isna(new_id_val):
                        continue
                    new_id = int(new_id_val)
                else:
                    new_id = int(float(str(new_id_val)))
                
                mapping[old_id] = new_id
            except (ValueError, TypeError, OverflowError):
                continue  # Skip n·∫øu kh√¥ng convert ƒë∆∞·ª£c
    
    # T·∫°o b·∫£ng CLEAN m·ªõi: merge c√°c records tr√πng (gi·ªØ 1 record cho m·ªói CANON_ID)
    # ‚ö†Ô∏è QUAN TR·ªåNG: CH·ªà x·ª≠ l√Ω b·∫£ng CLEAN, KH√îNG ƒë·ªông ƒë·∫øn b·∫£ng g·ªëc
    select_cols = ["TYPE_NAME", "PURITY", "CATEGORY"]
    if "BRAND" in columns:
        select_cols.append("BRAND")
    
    print(f"   üìù C√°c c·ªôt s·∫Ω gi·ªØ l·∫°i: {select_cols}")
    
    # L·∫•y gi√° tr·ªã t·ª´ canonical record (ID nh·ªè nh·∫•t) trong m·ªói group
    window_spec_clean = Window.partitionBy("CANON_ID").orderBy("ID")
    
    # Ki·ªÉm tra df_with_canon tr∆∞·ªõc khi merge
    canon_count = df_with_canon.count()
    print(f"   üìä S·ªë records sau khi t√¨m CANON_ID: {canon_count}")
    
    df_clean_merged = df_with_canon.withColumn(
        "ROW_NUM", row_number().over(window_spec_clean)
    ).filter(col("ROW_NUM") == 1)
    
    # Ki·ªÉm tra sau filter
    after_filter_count = df_clean_merged.count()
    print(f"   üìä S·ªë records sau filter ROW_NUM=1: {after_filter_count}")
    
    # Select c√°c c·ªôt c·∫ßn thi·∫øt
    try:
        df_clean_merged = df_clean_merged.select(
            col("CANON_ID").alias("ID"),
            *[col(c) for c in select_cols]
        )
    except Exception as e:
        print(f"   ‚ùå L·ªói khi select columns: {e}")
        print(f"   üìù C√°c c·ªôt c√≥ s·∫µn: {df_with_canon.columns}")
        print(f"   üìù C√°c c·ªôt c·∫ßn select: {select_cols}")
        print(f"   üìä Gi·ªØ nguy√™n d·ªØ li·ªáu CLEAN c≈©: {original_count} records")
        return mapping
    
    clean_count = df_clean_merged.count()
    print(f"   üìä S·ªë records sau select: {clean_count}")
    
    # ‚ö†Ô∏è QUAN TR·ªåNG: Ki·ªÉm tra an to√†n tr∆∞·ªõc khi ghi
    # N·∫øu df_clean_merged r·ªóng ho·∫∑c m·∫•t qu√° nhi·ªÅu d·ªØ li·ªáu, restore d·ªØ li·ªáu c≈©
    if clean_count == 0:
        print(f"‚ùå L·ªñI: Sau merge b·∫£ng CLEAN r·ªóng! Kh√¥i ph·ª•c d·ªØ li·ªáu c≈©...")
        try:
            write_table_to_oracle(df_backup, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
            print(f"   ‚úÖ ƒê√£ kh√¥i ph·ª•c {original_count} records")
        except Exception as restore_error:
            print(f"   ‚ùå L·ªói khi kh√¥i ph·ª•c: {restore_error}")
        return mapping
    
    if clean_count < original_count * 0.5:  # N·∫øu m·∫•t > 50% th√¨ c√≥ v·∫•n ƒë·ªÅ
        print(f"‚ö†Ô∏è C·∫¢NH B√ÅO: Sau merge m·∫•t qu√° nhi·ªÅu d·ªØ li·ªáu ({original_count} ‚Üí {clean_count})!")
        print(f"   üìä Kh√¥i ph·ª•c d·ªØ li·ªáu CLEAN c≈© ƒë·ªÉ tr√°nh m·∫•t d·ªØ li·ªáu...")
        try:
            write_table_to_oracle(df_backup, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
            print(f"   ‚úÖ ƒê√£ kh√¥i ph·ª•c {original_count} records")
        except Exception as restore_error:
            print(f"   ‚ùå L·ªói khi kh√¥i ph·ª•c: {restore_error}")
        return mapping
    
    if mapping:
        print(f"‚úÖ ƒê√£ t·∫°o mapping cho {len(mapping)} TYPE tr√πng:")
        for old_id, new_id in list(mapping.items())[:5]:  # In 5 mapping ƒë·∫ßu
            print(f"   ID {old_id} ‚Üí ID {new_id}")
        if len(mapping) > 5:
            print(f"   ... v√† {len(mapping) - 5} mapping kh√°c")
        
        # C·∫≠p nh·∫≠t b·∫£ng CLEAN v·ªõi d·ªØ li·ªáu ƒë√£ merge
        # ‚ö†Ô∏è QUAN TR·ªåNG: Ch·ªâ c·∫≠p nh·∫≠t b·∫£ng CLEAN, KH√îNG ƒë·ªông v√†o b·∫£ng g·ªëc
        try:
            write_table_to_oracle(df_clean_merged, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
            
            # ‚ö†Ô∏è QUAN TR·ªåNG: Verify sau khi ghi - ƒë·ªçc l·∫°i ƒë·ªÉ ki·ªÉm tra
            spark.catalog.clearCache()
            df_verify = read_table_from_oracle(spark, "GOLD_TYPE_DIMENSION_CLEAN", DB_USER)
            verify_count = df_verify.count()
            
            if verify_count == 0:
                print(f"‚ùå L·ªñI: Sau khi ghi, b·∫£ng CLEAN b·ªã r·ªóng! Kh√¥i ph·ª•c d·ªØ li·ªáu c≈©...")
                write_table_to_oracle(df_backup, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
                print(f"   ‚úÖ ƒê√£ kh√¥i ph·ª•c {original_count} records")
                return mapping
            
            if verify_count != clean_count:
                print(f"‚ö†Ô∏è C·∫£nh b√°o: S·ªë records sau khi ghi ({verify_count}) kh√°c v·ªõi expected ({clean_count})")
            
            print(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t GOLD_TYPE_DIMENSION_CLEAN: {verify_count} records (t·ª´ {original_count} records)")
            print(f"   üìù ƒê√£ merge {original_count - verify_count} records tr√πng")
        except Exception as e:
            print(f"‚ùå L·ªñI khi ghi v√†o b·∫£ng CLEAN: {e}")
            print(f"   üìä Kh√¥i ph·ª•c d·ªØ li·ªáu CLEAN c≈©: {original_count} records...")
            try:
                write_table_to_oracle(df_backup, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
                print(f"   ‚úÖ ƒê√£ kh√¥i ph·ª•c {original_count} records")
            except Exception as restore_error:
                print(f"   ‚ùå L·ªói khi kh√¥i ph·ª•c: {restore_error}")
            return mapping
        
        # ‚ö†Ô∏è QUAN TR·ªåNG: C·∫≠p nh·∫≠t GOLD_PRICE_FACT_CLEAN v·ªõi mapping
        # T·∫•t c·∫£ records c√≥ TYPE_ID = old_id ph·∫£i ƒë·ªïi th√†nh TYPE_ID = new_id
        print(f"\nüîÑ ƒêang c·∫≠p nh·∫≠t GOLD_PRICE_FACT_CLEAN v·ªõi type mapping...")
        try:
            df_fact_clean = read_table_from_oracle(spark, "GOLD_PRICE_FACT_CLEAN", DB_USER)
            fact_before_count = df_fact_clean.count()
            
            # ‚ö†Ô∏è QUAN TR·ªåNG: Backup d·ªØ li·ªáu FACT_CLEAN tr∆∞·ªõc khi c·∫≠p nh·∫≠t
            df_fact_backup = df_fact_clean
            print(f"   üíæ ƒê√£ backup {fact_before_count} records FACT_CLEAN")
            
            if fact_before_count > 0:
                # T·∫°o mapping DataFrame
                mapping_df = spark.createDataFrame(
                    [(k, v) for k, v in mapping.items()],
                    ["OLD_TYPE_ID", "NEW_TYPE_ID"]
                )
                
                # Join v√† c·∫≠p nh·∫≠t TYPE_ID (LEFT JOIN ƒë·ªÉ gi·ªØ T·∫§T C·∫¢ records)
                df_fact_updated = df_fact_clean.join(
                    mapping_df,
                    df_fact_clean["TYPE_ID"] == mapping_df["OLD_TYPE_ID"],
                    "left"  # LEFT JOIN ƒë·ªÉ gi·ªØ t·∫•t c·∫£ records, k·ªÉ c·∫£ kh√¥ng c√≥ mapping
                ).withColumn(
                    "TYPE_ID",
                    when(col("NEW_TYPE_ID").isNotNull(), col("NEW_TYPE_ID"))
                    .otherwise(col("TYPE_ID"))  # Gi·ªØ nguy√™n n·∫øu kh√¥ng c√≥ mapping
                ).drop("OLD_TYPE_ID", "NEW_TYPE_ID")
                
                fact_after_count = df_fact_updated.count()
                
                # ‚ö†Ô∏è QUAN TR·ªåNG: Ki·ªÉm tra an to√†n - s·ªë records ph·∫£i gi·ªØ nguy√™n
                if fact_after_count == 0:
                    print(f"   ‚ùå L·ªñI: Sau c·∫≠p nh·∫≠t FACT_CLEAN b·ªã r·ªóng! Kh√¥i ph·ª•c...")
                    write_table_to_oracle(df_fact_backup, f"{DB_USER}.GOLD_PRICE_FACT_CLEAN", "overwrite")
                    print(f"   ‚úÖ ƒê√£ kh√¥i ph·ª•c {fact_before_count} records FACT_CLEAN")
                elif fact_after_count != fact_before_count:
                    print(f"   ‚ö†Ô∏è C·∫¢NH B√ÅO: S·ªë records thay ƒë·ªïi ({fact_before_count} ‚Üí {fact_after_count})!")
                    print(f"   üìä Kh√¥i ph·ª•c d·ªØ li·ªáu FACT_CLEAN c≈©...")
                    write_table_to_oracle(df_fact_backup, f"{DB_USER}.GOLD_PRICE_FACT_CLEAN", "overwrite")
                    print(f"   ‚úÖ ƒê√£ kh√¥i ph·ª•c {fact_before_count} records FACT_CLEAN")
                else:
                    # ƒê·∫øm s·ªë records ƒë∆∞·ª£c c·∫≠p nh·∫≠t
                    updated_count = df_fact_clean.join(
                        mapping_df,
                        df_fact_clean["TYPE_ID"] == mapping_df["OLD_TYPE_ID"],
                        "inner"
                    ).count()
                    
                    # Ghi l·∫°i b·∫£ng FACT_CLEAN ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t
                    write_table_to_oracle(df_fact_updated, f"{DB_USER}.GOLD_PRICE_FACT_CLEAN", "overwrite")
                    
                    # Verify sau khi ghi
                    spark.catalog.clearCache()
                    df_fact_verify = read_table_from_oracle(spark, "GOLD_PRICE_FACT_CLEAN", DB_USER)
                    verify_count = df_fact_verify.count()
                    
                    if verify_count == 0:
                        print(f"   ‚ùå L·ªñI: Sau khi ghi, FACT_CLEAN b·ªã r·ªóng! Kh√¥i ph·ª•c...")
                        write_table_to_oracle(df_fact_backup, f"{DB_USER}.GOLD_PRICE_FACT_CLEAN", "overwrite")
                        print(f"   ‚úÖ ƒê√£ kh√¥i ph·ª•c {fact_before_count} records FACT_CLEAN")
                    else:
                        print(f"   ‚úÖ ƒê√£ c·∫≠p nh·∫≠t {updated_count} records trong GOLD_PRICE_FACT_CLEAN")
                        print(f"   üìä GOLD_PRICE_FACT_CLEAN: {fact_before_count} ‚Üí {verify_count} records")
                        
                        # In chi ti·∫øt c√°c mapping ƒë√£ √°p d·ª•ng
                        for old_id, new_id in mapping.items():
                            count = df_fact_clean.filter(col("TYPE_ID") == old_id).count()
                            if count > 0:
                                print(f"      TYPE_ID {old_id} ‚Üí {new_id}: {count} records")
            else:
                print(f"   ‚ÑπÔ∏è GOLD_PRICE_FACT_CLEAN tr·ªëng, kh√¥ng c·∫ßn c·∫≠p nh·∫≠t")
        except Exception as e:
            print(f"   ‚ùå L·ªñI khi c·∫≠p nh·∫≠t GOLD_PRICE_FACT_CLEAN: {e}")
            print(f"   üìä Kh√¥i ph·ª•c d·ªØ li·ªáu FACT_CLEAN c≈©...")
            try:
                if 'df_fact_backup' in locals():
                    write_table_to_oracle(df_fact_backup, f"{DB_USER}.GOLD_PRICE_FACT_CLEAN", "overwrite")
                    print(f"   ‚úÖ ƒê√£ kh√¥i ph·ª•c {fact_before_count} records FACT_CLEAN")
            except Exception as restore_error:
                print(f"   ‚ùå L·ªói khi kh√¥i ph·ª•c FACT_CLEAN: {restore_error}")
            print(f"   üìù Mapping v·∫´n ƒë∆∞·ª£c tr·∫£ v·ªÅ ƒë·ªÉ d√πng cho FACT m·ªõi")
        
        print(f"   üìù Mapping s·∫Ω ƒë∆∞·ª£c d√πng ƒë·ªÉ c·∫≠p nh·∫≠t FACT.TYPE_ID cho d·ªØ li·ªáu m·ªõi")
    else:
        print("‚ÑπÔ∏è Kh√¥ng c√≥ TYPE tr√πng c·∫ßn g·ªôp.")
        print(f"   üìä B·∫£ng CLEAN gi·ªØ nguy√™n: {original_count} records")
    
    return mapping

def clean_all_dimensions_incremental(spark: SparkSession, merge_types: bool = False) -> Tuple[Dict, Dict]:
    """
    Clean t·∫•t c·∫£ dimension tables (LOCATION v√† TYPE) - INCREMENTAL.
    Gi·ªØ nguy√™n d·ªØ li·ªáu CLEAN c≈©, ch·ªâ c·∫≠p nh·∫≠t/th√™m m·ªõi.
    Tr·∫£ v·ªÅ mappings ƒë·ªÉ d√πng cho FACT.
    """
    if not BATCH_FUNCTIONS_AVAILABLE:
        print("‚ö†Ô∏è Kh√¥ng th·ªÉ clean dimensions, ch·ªâ d√πng mappings hi·ªán c√≥")
        return {}, {}
    
    print("\n" + "="*60)
    print("üßπ ƒêang clean t·∫•t c·∫£ dimension tables (INCREMENTAL)...")
    print("="*60)
    
    # B1: LOCATION normalize -> LOCATION_DIMENSION_CLEAN
    print("\nüìç B∆∞·ªõc 1: Normalize LOCATION_DIMENSION...")
    
    # ƒê·ªçc d·ªØ li·ªáu CLEAN hi·ªán c√≥ TR∆Ø·ªöC (ƒë·ªÉ gi·ªØ l·∫°i)
    try:
        df_loc_clean_existing = read_table_from_oracle(spark, "LOCATION_DIMENSION_CLEAN", DB_USER)
        existing_loc_count = df_loc_clean_existing.count()
        existing_loc_ids = set([row["ID"] for row in df_loc_clean_existing.select("ID").collect()])
        print(f"üìä LOCATION_CLEAN hi·ªán c√≥: {existing_loc_count} records")
    except:
        df_loc_clean_existing = None
        existing_loc_ids = set()
        existing_loc_count = 0
        print("üìä LOCATION_CLEAN ch∆∞a c√≥, s·∫Ω t·∫°o m·ªõi")
    
    # Clear cache ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªçc d·ªØ li·ªáu m·ªõi nh·∫•t
    spark.catalog.clearCache()
    
    # G·ªçi normalize_locations (s·∫Ω overwrite, nh∆∞ng ta s·∫Ω merge l·∫°i sau)
    try:
        location_mapping = normalize_locations(spark)
    except Exception as e:
        print(f"‚ùå L·ªói khi normalize_locations: {e}")
        print(f"   Traceback: {type(e).__name__}: {str(e)}")
        # Fallback: Kh√¥ng c√≥ mapping, ch·ªâ d√πng d·ªØ li·ªáu hi·ªán c√≥
        location_mapping = {}
        print("‚ö†Ô∏è S·ª≠ d·ª•ng location_mapping r·ªóng, gi·ªØ nguy√™n d·ªØ li·ªáu CLEAN hi·ªán c√≥")
    
    # Clear cache l·∫°i sau khi normalize
    spark.catalog.clearCache()
    
    # ƒê·ªçc CLEAN m·ªõi sau khi normalize
    try:
        df_loc_clean_new = read_table_from_oracle(spark, "LOCATION_DIMENSION_CLEAN", DB_USER)
        new_loc_count = df_loc_clean_new.count()
        
        # Ki·ªÉm tra n·∫øu b·∫£ng CLEAN m·ªõi r·ªóng nh∆∞ng c√≥ d·ªØ li·ªáu c≈©
        if new_loc_count == 0 and existing_loc_count > 0:
            print("‚ö†Ô∏è B·∫£ng CLEAN m·ªõi r·ªóng nh∆∞ng c√≥ d·ªØ li·ªáu c≈©. Gi·ªØ nguy√™n d·ªØ li·ªáu c≈©...")
            write_table_to_oracle(df_loc_clean_existing, f"{DB_USER}.LOCATION_DIMENSION_CLEAN", "overwrite")
            print(f"‚úÖ ƒê√£ gi·ªØ nguy√™n LOCATION_DIMENSION_CLEAN: {existing_loc_count} records")
        
        # Merge: Gi·ªØ nguy√™n CLEAN c≈© + CLEAN m·ªõi (union v√† distinct)
        elif df_loc_clean_existing is not None and existing_loc_count > 0:
            df_loc_clean_combined = df_loc_clean_existing.unionByName(df_loc_clean_new, allowMissingColumns=True)
            df_loc_clean_final = df_loc_clean_combined.distinct()
            final_count = df_loc_clean_final.count()
            
            # ƒê·∫£m b·∫£o c√≥ d·ªØ li·ªáu tr∆∞·ªõc khi ghi
            if final_count > 0:
                write_table_to_oracle(df_loc_clean_final, f"{DB_USER}.LOCATION_DIMENSION_CLEAN", "overwrite")
                print(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t LOCATION_DIMENSION_CLEAN: {final_count} records (gi·ªØ {existing_loc_count} c≈©)")
            else:
                print("‚ö†Ô∏è Sau merge kh√¥ng c√≤n d·ªØ li·ªáu! Gi·ªØ nguy√™n d·ªØ li·ªáu c≈©...")
                write_table_to_oracle(df_loc_clean_existing, f"{DB_USER}.LOCATION_DIMENSION_CLEAN", "overwrite")
                print(f"‚úÖ ƒê√£ gi·ªØ nguy√™n LOCATION_DIMENSION_CLEAN: {existing_loc_count} records")
        else:
            # Ki·ªÉm tra n·∫øu b·∫£ng CLEAN m·ªõi c√≥ d·ªØ li·ªáu
            if new_loc_count > 0:
                print(f"‚úÖ ƒê√£ t·∫°o LOCATION_DIMENSION_CLEAN: {new_loc_count} records")
            else:
                print("‚ö†Ô∏è B·∫£ng CLEAN m·ªõi r·ªóng! Ki·ªÉm tra l·∫°i b·∫£ng g·ªëc...")
                # Fallback: ƒë·ªçc t·ª´ b·∫£ng g·ªëc
                try:
                    df_original = read_table_from_oracle(spark, "LOCATION_DIMENSION", DB_USER)
                    original_count = df_original.count()
                    if original_count > 0:
                        print(f"‚ö†Ô∏è Copy {original_count} records t·ª´ b·∫£ng g·ªëc...")
                        write_table_to_oracle(df_original, f"{DB_USER}.LOCATION_DIMENSION_CLEAN", "overwrite")
                        print(f"‚úÖ ƒê√£ copy t·ª´ b·∫£ng g·ªëc: {original_count} records")
                    else:
                        print("‚ùå B·∫£ng g·ªëc c≈©ng tr·ªëng!")
                except Exception as e2:
                    print(f"‚ùå Kh√¥ng th·ªÉ copy t·ª´ b·∫£ng g·ªëc: {e2}")
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi merge LOCATION_CLEAN: {e}")
        # Fallback: gi·ªØ nguy√™n d·ªØ li·ªáu c≈© n·∫øu c√≥
        if df_loc_clean_existing is not None and existing_loc_count > 0:
            try:
                write_table_to_oracle(df_loc_clean_existing, f"{DB_USER}.LOCATION_DIMENSION_CLEAN", "overwrite")
                print(f"‚úÖ ƒê√£ gi·ªØ nguy√™n d·ªØ li·ªáu c≈©: {existing_loc_count} records")
            except:
                pass
    
    print(f"‚úÖ Location mapping: {len(location_mapping)} mappings")
    
    # B2: GOLD TYPE enrich -> GOLD_TYPE_DIMENSION_CLEAN
    print("\nüíé B∆∞·ªõc 2: Enrich GOLD_TYPE_DIMENSION...")
    
    # ƒê·ªçc d·ªØ li·ªáu CLEAN hi·ªán c√≥ TR∆Ø·ªöC (ƒë·ªÉ gi·ªØ l·∫°i)
    try:
        df_type_clean_existing = read_table_from_oracle(spark, "GOLD_TYPE_DIMENSION_CLEAN", DB_USER)
        existing_type_count = df_type_clean_existing.count()
        print(f"üìä TYPE_CLEAN hi·ªán c√≥: {existing_type_count} records")
    except:
        df_type_clean_existing = None
        existing_type_count = 0
        print("üìä TYPE_CLEAN ch∆∞a c√≥, s·∫Ω t·∫°o m·ªõi")
    
    # Clear cache ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªçc d·ªØ li·ªáu m·ªõi nh·∫•t
    spark.catalog.clearCache()
    
    # G·ªçi c√°c h√†m enrich (s·∫Ω overwrite, nh∆∞ng ta s·∫Ω merge l·∫°i sau)
    try:
        enrich_gold_types(spark)
        normalize_purity_format(spark)
        normalize_category_smart(spark)
    except Exception as e:
        print(f"‚ùå L·ªói khi enrich/normalize TYPE: {e}")
        print(f"   Traceback: {type(e).__name__}: {str(e)}")
        print("‚ö†Ô∏è Gi·ªØ nguy√™n d·ªØ li·ªáu TYPE_CLEAN hi·ªán c√≥")
    
    # Clear cache l·∫°i sau khi g·ªçi c√°c h√†m
    spark.catalog.clearCache()
    
    # ƒê·ªçc CLEAN m·ªõi sau khi enrich
    try:
        df_type_clean_new = read_table_from_oracle(spark, "GOLD_TYPE_DIMENSION_CLEAN", DB_USER)
        new_type_count = df_type_clean_new.count()
        
        # Ki·ªÉm tra n·∫øu b·∫£ng CLEAN m·ªõi r·ªóng nh∆∞ng c√≥ d·ªØ li·ªáu c≈©
        if new_type_count == 0 and existing_type_count > 0:
            print("‚ö†Ô∏è B·∫£ng CLEAN m·ªõi r·ªóng nh∆∞ng c√≥ d·ªØ li·ªáu c≈©. Gi·ªØ nguy√™n d·ªØ li·ªáu c≈©...")
            write_table_to_oracle(df_type_clean_existing, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
            print(f"‚úÖ ƒê√£ gi·ªØ nguy√™n GOLD_TYPE_DIMENSION_CLEAN: {existing_type_count} records")
            return location_mapping, {}
        
        # Merge: Gi·ªØ nguy√™n CLEAN c≈© + CLEAN m·ªõi (union v√† distinct)
        if df_type_clean_existing is not None and existing_type_count > 0:
            df_type_clean_combined = df_type_clean_existing.unionByName(df_type_clean_new, allowMissingColumns=True)
            # Deduplicate theo ID (gi·ªØ record m·ªõi nh·∫•t n·∫øu c√≥ tr√πng)
            window_spec = Window.partitionBy("ID").orderBy(col("ID"))
            df_type_clean_final = df_type_clean_combined.withColumn("rn", row_number().over(window_spec)) \
                .filter(col("rn") == 1) \
                .drop("rn") \
                .distinct()
            final_count = df_type_clean_final.count()
            
            # ƒê·∫£m b·∫£o c√≥ d·ªØ li·ªáu tr∆∞·ªõc khi ghi
            if final_count > 0:
                write_table_to_oracle(df_type_clean_final, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
                print(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t GOLD_TYPE_DIMENSION_CLEAN: {final_count} records (gi·ªØ {existing_type_count} c≈©)")
            else:
                print("‚ö†Ô∏è Sau merge kh√¥ng c√≤n d·ªØ li·ªáu! Gi·ªØ nguy√™n d·ªØ li·ªáu c≈©...")
                write_table_to_oracle(df_type_clean_existing, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
                print(f"‚úÖ ƒê√£ gi·ªØ nguy√™n GOLD_TYPE_DIMENSION_CLEAN: {existing_type_count} records")
        else:
            # Ki·ªÉm tra n·∫øu b·∫£ng CLEAN m·ªõi c√≥ d·ªØ li·ªáu
            if new_type_count > 0:
                print(f"‚úÖ ƒê√£ t·∫°o GOLD_TYPE_DIMENSION_CLEAN: {new_type_count} records")
            else:
                print("‚ö†Ô∏è B·∫£ng CLEAN m·ªõi r·ªóng! Ki·ªÉm tra l·∫°i b·∫£ng g·ªëc...")
                # Fallback: CH·ªà ƒë·ªçc t·ª´ b·∫£ng g·ªëc ƒë·ªÉ copy v√†o CLEAN (KH√îNG s·ª≠a b·∫£ng g·ªëc)
                # ƒê√¢y l√† tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát khi CLEAN b·ªã r·ªóng, c·∫ßn copy t·ª´ g·ªëc ƒë·ªÉ kh√¥i ph·ª•c
                try:
                    df_original = read_table_from_oracle(spark, "GOLD_TYPE_DIMENSION", DB_USER)
                    original_count = df_original.count()
                    if original_count > 0:
                        print(f"‚ö†Ô∏è Copy {original_count} records t·ª´ b·∫£ng g·ªëc v√†o CLEAN...")
                        # QUAN TR·ªåNG: Ch·ªâ ghi v√†o CLEAN, KH√îNG ƒë·ªông v√†o b·∫£ng g·ªëc
                        write_table_to_oracle(df_original, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
                        print(f"‚úÖ ƒê√£ copy t·ª´ b·∫£ng g·ªëc v√†o CLEAN: {original_count} records")
                    else:
                        print("‚ùå B·∫£ng g·ªëc c≈©ng tr·ªëng!")
                except Exception as e2:
                    print(f"‚ùå Kh√¥ng th·ªÉ copy t·ª´ b·∫£ng g·ªëc: {e2}")
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi merge TYPE_CLEAN: {e}")
        # Fallback: gi·ªØ nguy√™n d·ªØ li·ªáu c≈© n·∫øu c√≥
        if df_type_clean_existing is not None and existing_type_count > 0:
            try:
                write_table_to_oracle(df_type_clean_existing, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
                print(f"‚úÖ ƒê√£ gi·ªØ nguy√™n d·ªØ li·ªáu c≈©: {existing_type_count} records")
            except:
                pass
    
    # (Tu·ª≥ ch·ªçn) g·ªôp TYPE t∆∞∆°ng ƒë·ªìng
    # QUAN TR·ªåNG: D√πng h√†m ri√™ng trong file streaming (KH√îNG import t·ª´ batch)
    # - ƒê·ªçc t·ª´: GOLD_TYPE_DIMENSION_CLEAN
    # - CH·ªà t·∫°o mapping, KH√îNG ghi ƒë√® b·∫£ng CLEAN
    # - KH√îNG ƒë·ªông v√†o b·∫£ng g·ªëc GOLD_TYPE_DIMENSION
    type_mapping = {}
    if merge_types:
        print("\nüîó B∆∞·ªõc 3: Merge duplicate types...")
        print("   üìù Ch·ªâ x·ª≠ l√Ω b·∫£ng CLEAN, kh√¥ng ƒë·ªông v√†o b·∫£ng g·ªëc")
        print("   üìù CH·ªà t·∫°o mapping, KH√îNG ghi ƒë√® b·∫£ng CLEAN (gi·ªëng logic c≈©)")
        try:
            # D√πng h√†m ri√™ng trong file streaming (kh√¥ng import t·ª´ batch)
            type_mapping = merge_duplicate_types_and_update_fact_streaming(spark)
            print(f"‚úÖ Type mapping: {len(type_mapping)} mappings")
        except Exception as e:
            print(f"‚ùå L·ªói khi merge duplicate types: {e}")
            print(f"   Traceback: {type(e).__name__}: {str(e)}")
            type_mapping = {}
            print("‚ö†Ô∏è S·ª≠ d·ª•ng type_mapping r·ªóng")
    else:
        print("\n‚è≠Ô∏è  B∆∞·ªõc 3: B·ªè qua merge types (d√πng --merge-types ƒë·ªÉ b·∫≠t)")
    
    try:
        normalize_gold_type_and_unit(spark)
    except Exception as e:
        print(f"‚ùå L·ªói khi normalize_gold_type_and_unit: {e}")
        print(f"   Traceback: {type(e).__name__}: {str(e)}")
        print("‚ö†Ô∏è B·ªè qua b∆∞·ªõc normalize_gold_type_and_unit")
    
    print("\n‚úÖ ƒê√£ clean t·∫•t c·∫£ dimension tables (gi·ªØ nguy√™n d·ªØ li·ªáu c≈©)!")
    print("="*60 + "\n")
    
    return location_mapping, type_mapping

def process_batch(batch_id: int, batch_df: 'DataFrame', 
                 spark: SparkSession, table_name: str,
                 clean_all: bool = False, merge_types: bool = False):
    """
    X·ª≠ l√Ω m·ªói batch trong streaming.
    ƒê∆∞·ª£c g·ªçi t·ª± ƒë·ªông b·ªüi foreachBatch.
    
    N·∫øu clean_all=True, s·∫Ω clean t·∫•t c·∫£ b·∫£ng (LOCATION, TYPE, FACT) m·ªói khi FACT thay ƒë·ªïi.
    """
    print(f"\n{'='*60}")
    print(f"üì¶ Batch {batch_id} - {dt.datetime.now()}")
    print(f"{'='*60}")
    
    # B·ªè qua batch_df (kh√¥ng d√πng, ch·ªâ l√† trigger)
    # ƒê·ªçc d·ªØ li·ªáu m·ªõi t·ª´ Oracle d·ª±a tr√™n checkpoint
    print(f"\nüîç B∆∞·ªõc 1: L·∫•y checkpoint ƒë·ªÉ ph√°t hi·ªán d·ªØ li·ªáu m·ªõi...")
    last_ts = get_last_timestamp_from_checkpoint(spark)
    print(f"   üìå Timestamp checkpoint: {last_ts}")
    
    print(f"\nüîç B∆∞·ªõc 2: ƒê·ªçc d·ªØ li·ªáu m·ªõi sau checkpoint...")
    df_new = read_new_data_from_oracle(spark, table_name, last_ts)
    
    new_count = df_new.count()
    if new_count == 0:
        print(f"\n‚ÑπÔ∏è Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi trong batch n√†y (sau {last_ts})")
        print(f"   ‚è≠Ô∏è  B·ªè qua batch {batch_id}")
        return
    
    print(f"\n‚úÖ Ph√°t hi·ªán {new_count} records m·ªõi c·∫ßn x·ª≠ l√Ω")
    
    print(f"üìä S·ªë l∆∞·ª£ng records FACT m·ªõi: {df_new.count()}")
    
    # N·∫øu clean_all=True, clean t·∫•t c·∫£ dimension tables tr∆∞·ªõc
    location_mapping = {}
    type_mapping = {}
    
    if clean_all:
        print("\nüîÑ Ph√°t hi·ªán FACT thay ƒë·ªïi, ƒëang clean T·∫§T C·∫¢ c√°c b·∫£ng...")
        print("   (Gi·ªØ nguy√™n d·ªØ li·ªáu CLEAN c≈©, ch·ªâ c·∫≠p nh·∫≠t/th√™m m·ªõi)")
        location_mapping, type_mapping = clean_all_dimensions_incremental(spark, merge_types)
    else:
        # Ch·ªâ load mappings hi·ªán c√≥
        location_mapping, type_mapping = load_dimension_mappings(spark)
        print(f"üìä S·ª≠ d·ª•ng mappings hi·ªán c√≥: Location={len(location_mapping)}, Type={len(type_mapping)}")
    
    # X·ª≠ l√Ω d·ªØ li·ªáu FACT m·ªõi v·ªõi mappings
    df_processed = process_new_fact_data(spark, df_new, location_mapping, type_mapping)
    
    if df_processed.count() == 0:
        print("‚ö†Ô∏è Sau x·ª≠ l√Ω kh√¥ng c√≤n d·ªØ li·ªáu")
        return
    
    # Merge v·ªõi d·ªØ li·ªáu CLEAN hi·ªán c√≥ (CH·ªà TH√äM, KH√îNG X√ìA D·ªÆ LI·ªÜU C≈®) - Logic gi·ªëng batch file
    try:
        # ƒê·ªçc b·∫£ng CLEAN hi·ªán c√≥
        df_existing = read_table_from_oracle(spark, "GOLD_PRICE_FACT_CLEAN", DB_USER)
        existing_count = df_existing.count()
        print(f"üìä GOLD_PRICE_FACT_CLEAN hi·ªán c√≥: {existing_count} records")
        
        # Union d·ªØ li·ªáu m·ªõi v·ªõi d·ªØ li·ªáu c≈©
        df_combined = df_existing.unionByName(df_processed, allowMissingColumns=True)
        combined_count = df_combined.count()
        processed_count = df_processed.count()
        print(f"üìä Sau merge: {combined_count} records (c≈©: {existing_count}, m·ªõi: {processed_count})")
        
        # Apply cleaning tr√™n d·ªØ li·ªáu ƒë√£ merge (dedup, handle missing, flag outliers)
        # Logic gi·ªëng h·ªát batch file ƒë·ªÉ ƒë·∫£m b·∫£o consistency
        print("üßπ ƒêang x·ª≠ l√Ω cleaning tr√™n d·ªØ li·ªáu ƒë√£ merge...")
        
        # 1. Dedup tr√™n to√†n b·ªô d·ªØ li·ªáu ƒë√£ merge
        df_combined = df_combined.cache()
        before_dedup = df_combined.count()
        
        # T·∫°o composite key ƒë·ªÉ dedup (v·ªõi RECORDED_AT_SAFE ƒë·ªÉ handle null)
        df_combined = df_combined.withColumn(
            "COMBO",
            concat_ws("|", 
                col("SOURCE_ID").cast("string"),
                col("TYPE_ID").cast("string"),
                col("LOCATION_ID").cast("string"),
                col("TIME_ID").cast("string")
            )
        ).withColumn(
            "RECORDED_AT_SAFE",
            coalesce(col(TIMESTAMP_COLUMN), to_timestamp(lit("2000-01-01 00:00:00")))
        )
        
        window_spec = Window.partitionBy("COMBO").orderBy(col("RECORDED_AT_SAFE").desc())
        df_combined = df_combined.withColumn("rn", row_number().over(window_spec)) \
            .filter(col("rn") == 1) \
            .drop("rn", "COMBO", "RECORDED_AT_SAFE")
        
        after_dedup = df_combined.count()
        n_dup = before_dedup - after_dedup
        print(f"   ‚úÖ ƒê√£ lo·∫°i b·ªè {n_dup} b·∫£n ghi tr√πng")
        
        # 2. Handle missing values (ch·ªâ lo·∫°i b·ªè record thi·∫øu critical fields)
        before_missing = df_combined.count()
        df_combined = df_combined.filter(
            col("BUY_PRICE").isNotNull() & 
            col("SELL_PRICE").isNotNull() & 
            col(TIMESTAMP_COLUMN).isNotNull()
        )
        after_missing = df_combined.count()
        n_missing = before_missing - after_missing
        print(f"   ‚úÖ ƒê√£ lo·∫°i b·ªè {n_missing} b·∫£n ghi thi·∫øu gi√° ho·∫∑c th·ªùi gian")
        
        # 3. Flag outliers (kh√¥ng x√≥a, ch·ªâ flag) - Logic gi·ªëng batch file
        from pyspark.sql.functions import percentile_approx
        from decimal import Decimal
        
        def to_float(val):
            if val is None:
                return None
            if isinstance(val, Decimal):
                return float(val)
            return float(val)
        
        try:
            buy_q1_val = df_combined.select(percentile_approx("BUY_PRICE", 0.25).alias("q1")).first()[0]
            buy_q3_val = df_combined.select(percentile_approx("BUY_PRICE", 0.75).alias("q3")).first()[0]
            buy_q1 = to_float(buy_q1_val)
            buy_q3 = to_float(buy_q3_val)
            buy_iqr = buy_q3 - buy_q1
            buy_lower = buy_q1 - 1.5 * buy_iqr
            buy_upper = buy_q3 + 1.5 * buy_iqr
            
            sell_q1_val = df_combined.select(percentile_approx("SELL_PRICE", 0.25).alias("q1")).first()[0]
            sell_q3_val = df_combined.select(percentile_approx("SELL_PRICE", 0.75).alias("q3")).first()[0]
            sell_q1 = to_float(sell_q1_val)
            sell_q3 = to_float(sell_q3_val)
            sell_iqr = sell_q3 - sell_q1
            sell_lower = sell_q1 - 1.5 * sell_iqr
            sell_upper = sell_q3 + 1.5 * sell_iqr
            
            df_combined = df_combined.withColumn(
                "IS_DELETED",
                when(
                    (col("BUY_PRICE") < lit(buy_lower)) | (col("BUY_PRICE") > lit(buy_upper)) |
                    (col("SELL_PRICE") < lit(sell_lower)) | (col("SELL_PRICE") > lit(sell_upper)),
                    lit(1)
                ).otherwise(lit(0))
            )
            
            n_outliers = df_combined.filter(col("IS_DELETED") == 1).count()
            print(f"   ‚úÖ ƒê√£ flag {n_outliers} b·∫£n ghi outlier (IS_DELETED=1)")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Kh√¥ng th·ªÉ flag outliers: {e}. Gi·ªØ nguy√™n d·ªØ li·ªáu.")
            if "IS_DELETED" not in df_combined.columns:
                df_combined = df_combined.withColumn("IS_DELETED", lit(0))
        
        # ƒê·∫£m b·∫£o c√≥ c·ªôt IS_DELETE (n·∫øu c·∫ßn)
        if "IS_DELETE" not in df_combined.columns:
            df_combined = df_combined.withColumn("IS_DELETE", col("IS_DELETED"))
        
        # Ghi l·∫°i b·∫£ng CLEAN v·ªõi d·ªØ li·ªáu ƒë√£ merge v√† ƒë√£ clean
        final_count = df_combined.count()
        write_table_to_oracle(df_combined, f"{DB_USER}.GOLD_PRICE_FACT_CLEAN", "overwrite")
        print(f"‚úÖ ƒê√£ merge v√† clean: {final_count} records (th√™m {processed_count} m·ªõi, gi·ªØ {existing_count} c≈©)")
        
        # C·∫≠p nh·∫≠t checkpoint v·ªõi timestamp m·ªõi nh·∫•t t·ª´ d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
        print(f"\nüíæ B∆∞·ªõc cu·ªëi: C·∫≠p nh·∫≠t checkpoint...")
        max_ts = df_processed.agg(spark_max(col(TIMESTAMP_COLUMN))).first()[0]
        if max_ts:
            update_checkpoint(spark, max_ts)
            print(f"‚úÖ Checkpoint ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t: {max_ts}")
            print(f"   üìå Batch ti·∫øp theo s·∫Ω x·ª≠ l√Ω d·ªØ li·ªáu sau {max_ts}")
        else:
            print(f"‚ö†Ô∏è Kh√¥ng c√≥ timestamp h·ª£p l·ªá ƒë·ªÉ c·∫≠p nh·∫≠t checkpoint")
    
    except Exception as e:
        # N·∫øu b·∫£ng CLEAN ch∆∞a c√≥, ghi d·ªØ li·ªáu m·ªõi (ch·ªâ l·∫ßn ƒë·∫ßu)
        print(f"‚ö†Ô∏è B·∫£ng CLEAN ch∆∞a c√≥ ho·∫∑c l·ªói: {e}. Ghi d·ªØ li·ªáu m·ªõi...")
        # Apply basic cleaning tr∆∞·ªõc khi ghi
        df_processed = df_processed.filter(
            col("BUY_PRICE").isNotNull() & 
            col("SELL_PRICE").isNotNull() & 
            col(TIMESTAMP_COLUMN).isNotNull()
        )
        if "IS_DELETED" not in df_processed.columns:
            df_processed = df_processed.withColumn("IS_DELETED", lit(0))
        if "IS_DELETE" not in df_processed.columns:
            df_processed = df_processed.withColumn("IS_DELETE", lit(0))
        write_table_to_oracle(df_processed, f"{DB_USER}.GOLD_PRICE_FACT_CLEAN", "overwrite")
        print(f"‚úÖ ƒê√£ ghi {df_processed.count()} records v√†o GOLD_PRICE_FACT_CLEAN (l·∫ßn ƒë·∫ßu)")

def create_oracle_polling_stream(spark: SparkSession, table_name: str,
                                trigger_interval: str = STREAMING_TRIGGER_INTERVAL,
                                clean_all: bool = False,
                                merge_types: bool = False):
    """
    T·∫°o Spark Structured Streaming query ƒë·ªÉ polling Oracle.
    
    C√°ch ho·∫°t ƒë·ªông:
    1. D√πng rate source ƒë·ªÉ t·∫°o trigger (emit 1 row m·ªói interval)
    2. D√πng foreachBatch ƒë·ªÉ polling Oracle m·ªói interval
    3. N·∫øu clean_all=True, s·∫Ω clean t·∫•t c·∫£ b·∫£ng m·ªói khi FACT thay ƒë·ªïi
    4. Spark t·ª± ƒë·ªông qu·∫£n l√Ω checkpoint v√† recovery
    """
    
    # T·∫°o rate source - emit 1 row m·ªói interval ƒë·ªÉ trigger foreachBatch
    # Rate source l√† built-in streaming source c·ªßa Spark
    trigger_df = spark.readStream \
        .format("rate") \
        .option("rowsPerSecond", 1) \
        .option("numPartitions", 1) \
        .load()
    
    # Ch·ªâ l·∫•y timestamp column ƒë·ªÉ l√†m trigger
    trigger_df = trigger_df.select(col("timestamp").alias("trigger_time"))
    
    # T·∫°o streaming query v·ªõi foreachBatch
    def foreach_batch_wrapper(batch_id, batch_df):
        # B·ªè qua batch_df (ch·ªâ l√† trigger)
        # G·ªçi process_batch ƒë·ªÉ polling Oracle v√† clean n·∫øu c·∫ßn
        process_batch(batch_id, batch_df, spark, table_name, clean_all, merge_types)
    
    # T·∫°o streaming query
    query = trigger_df.writeStream \
        .foreachBatch(foreach_batch_wrapper) \
        .outputMode("update") \
        .trigger(processingTime=trigger_interval) \
        .option("checkpointLocation", f"{STREAMING_CHECKPOINT_DIR}/oracle_polling") \
        .start()
    
    return query

# ==================== MAIN ====================

def main():
    parser = argparse.ArgumentParser(description="Spark Structured Streaming v·ªõi Oracle polling")
    parser.add_argument("--interval", type=str, default=STREAMING_TRIGGER_INTERVAL,
                       help="Trigger interval (v√≠ d·ª•: '30 seconds', '1 minute')")
    parser.add_argument("--table", type=str, default="GOLD_PRICE_FACT",
                       help="T√™n b·∫£ng Oracle ƒë·ªÉ monitor (GOLD_PRICE_FACT, LOCATION_DIMENSION, GOLD_TYPE_DIMENSION)")
    parser.add_argument("--clean-all", action="store_true",
                       help="Khi FACT thay ƒë·ªïi, t·ª± ƒë·ªông clean T·∫§T C·∫¢ c√°c b·∫£ng (LOCATION, TYPE, FACT)")
    parser.add_argument("--merge-types", action="store_true",
                       help="G·ªôp TYPE t∆∞∆°ng ƒë·ªìng khi clean (ch·ªâ d√πng v·ªõi --clean-all)")
    
    args = parser.parse_args()
    
    # T·∫°o checkpoint directory
    os.makedirs(STREAMING_CHECKPOINT_DIR, exist_ok=True)
    
    spark = create_spark_session()
    
    print("\n" + "="*60)
    print("üöÄ SPARK STRUCTURED STREAMING - ORACLE POLLING")
    print("="*60)
    print(f"üìä Table: {args.table}")
    print(f"‚è±Ô∏è  Trigger Interval: {args.interval}")
    print(f"üìÅ Checkpoint: {STREAMING_CHECKPOINT_DIR}")
    if args.clean_all:
        print(f"üîÑ Mode: Clean ALL tables khi FACT thay ƒë·ªïi")
        print(f"   ‚úÖ LOCATION_DIMENSION ‚Üí LOCATION_DIMENSION_CLEAN")
        print(f"   ‚úÖ GOLD_TYPE_DIMENSION ‚Üí GOLD_TYPE_DIMENSION_CLEAN")
        print(f"   ‚úÖ GOLD_PRICE_FACT ‚Üí GOLD_PRICE_FACT_CLEAN")
        if args.merge_types:
            print(f"   ‚úÖ Merge duplicate types: ON")
    else:
        print(f"üîÑ Mode: Streaming FACT only (ch·ªâ x·ª≠ l√Ω FACT)")
    print("="*60 + "\n")
    
    # Ki·ªÉm tra batch functions c√≥ s·∫µn kh√¥ng
    if args.clean_all and not BATCH_FUNCTIONS_AVAILABLE:
        print("‚ùå L·ªói: Kh√¥ng th·ªÉ import batch functions ƒë·ªÉ clean dimensions!")
        print("   Vui l√≤ng ƒë·∫£m b·∫£o c√°c dependencies ƒë√£ ƒë∆∞·ª£c c√†i:")
        print("   pip install pandas numpy scikit-learn fuzzywuzzy python-Levenshtein")
        print("\n   Ho·∫∑c ch·∫°y kh√¥ng c√≥ --clean-all ƒë·ªÉ ch·ªâ x·ª≠ l√Ω FACT")
        sys.exit(1)
    
    # Ch·ªâ streaming FACT table
    if args.table != "GOLD_PRICE_FACT":
        print(f"‚ö†Ô∏è L∆∞u √Ω: Streaming ch·ªâ h·ªó tr·ª£ GOLD_PRICE_FACT")
        print(f"   ƒêang chuy·ªÉn sang GOLD_PRICE_FACT...\n")
        args.table = "GOLD_PRICE_FACT"
    
    # Kh·ªüi ƒë·ªông streaming query
    query = create_oracle_polling_stream(
        spark, 
        args.table, 
        args.interval,
        args.clean_all,
        args.merge_types
    )
    
    print(f"\n‚úÖ Streaming query ƒë√£ kh·ªüi ƒë·ªông!")
    print(f"üìä Query ID: {query.id}")
    print(f"üìä Status: {query.status}")
    print(f"üìä Spark UI: http://localhost:4040")
    print(f"\nüîÑ ƒêang ch·∫°y... Nh·∫•n Ctrl+C ƒë·ªÉ d·ª´ng\n")
    
    try:
        # Ch·ªù streaming query ch·∫°y
        query.awaitTermination()
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è ƒêang d·ª´ng streaming query...")
        query.stop()
        print("‚úÖ ƒê√£ d·ª´ng")
    
    spark.stop()

if __name__ == "__main__":
    main()

