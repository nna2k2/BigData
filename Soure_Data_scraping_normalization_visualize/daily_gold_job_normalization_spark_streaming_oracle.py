# -*- coding: utf-8 -*-
"""
Daily Gold ETL Job - Spark Structured Streaming v·ªõi Oracle Polling
T·ª± ƒë·ªông ph√°t hi·ªán v√† x·ª≠ l√Ω khi Oracle database thay ƒë·ªïi

Gi·∫£i ph√°p: Spark Structured Streaming + foreachBatch
- D√πng memory source l√†m trigger
- foreachBatch ƒë·ªÉ polling Oracle m·ªói interval
- C√≥ checkpoint t·ª± ƒë·ªông, recovery, monitoring
"""

import argparse
import datetime as dt
import os
from typing import Dict, List, Tuple, Optional

from pyspark.sql import SparkSession
from pyspark.sql.streaming import StreamingQuery
from pyspark.sql.functions import (
    col, when, lit, trim, upper, lower, regexp_replace, 
    concat_ws, first, last, max as spark_max, min as spark_min,
    count, isnan, isnull, coalesce, to_timestamp, date_format,
    row_number, window, monotonically_increasing_id, current_timestamp
)
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, FloatType, 
    TimestampType, DoubleType, LongType
)
from pyspark.sql.window import Window

# ====================== CONFIG ======================
DB_USER = "CLOUD"
DB_PASS = "cloud123"
DB_HOST = "136.110.60.196"
DB_PORT = "1521"
DB_SERVICE = "XEPDB1"

DB_DSN = f"{DB_HOST}:{DB_PORT}/{DB_SERVICE}"
DB_URL = f"jdbc:oracle:thin:@{DB_DSN}"

SNAPSHOT_DIR = "./snapshots"
JOB_NAME = "DAILY_GOLD_JOB_STREAMING_ORACLE"
SIM_THRESHOLD_LOC = 0.80
SIM_THRESHOLD_TYPE = 0.75
FUZZY_FALLBACK = 90

# Streaming config
STREAMING_CHECKPOINT_DIR = "./checkpoints/streaming_oracle"
STREAMING_TRIGGER_INTERVAL = "60 seconds"  # Polling m·ªói 60 gi√¢y
TIMESTAMP_COLUMN = "RECORDED_AT"  # C·ªôt timestamp ƒë·ªÉ ph√°t hi·ªán thay ƒë·ªïi

# Spark config
SPARK_APP_NAME = "DailyGoldETLJobStreamingOracle"
SPARK_MASTER = "local[*]"

# ====================================================

def create_spark_session(ojdbc_path: str = None):
    """T·∫°o SparkSession v·ªõi c·∫•u h√¨nh streaming."""
    builder = SparkSession.builder \
        .appName(SPARK_APP_NAME) \
        .master(SPARK_MASTER) \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.driver.memory", "2g") \
        .config("spark.executor.memory", "2g") \
        .config("spark.sql.streaming.checkpointLocation", STREAMING_CHECKPOINT_DIR)
    
    # Th√™m JDBC driver n·∫øu c√≥
    if ojdbc_path:
        if os.path.exists(ojdbc_path):
            builder = builder.config("spark.jars", ojdbc_path)
            print(f"‚úÖ ƒê√£ load JDBC driver t·ª´: {ojdbc_path}")
        else:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y JDBC driver t·∫°i: {ojdbc_path}")
    else:
        possible_paths = [
            "ojdbc8.jar",
            "./ojdbc8.jar",
            "../ojdbc8.jar",
            os.path.join(os.path.dirname(__file__), "ojdbc8.jar")
        ]
        for path in possible_paths:
            if os.path.exists(path):
                builder = builder.config("spark.jars", os.path.abspath(path))
                print(f"‚úÖ ƒê√£ t·ª± ƒë·ªông t√¨m th·∫•y JDBC driver: {os.path.abspath(path)}")
                break
    
    spark = builder.getOrCreate()
    return spark

def read_table_from_oracle(spark: SparkSession, table_name: str, schema: str = None) -> 'DataFrame':
    """ƒê·ªçc b·∫£ng t·ª´ Oracle DB (batch)."""
    schema_prefix = f'"{schema}"."' if schema else '"'
    full_table = f'{schema_prefix}{table_name}"'
    
    df = spark.read \
        .format("jdbc") \
        .option("url", f"jdbc:oracle:thin:{DB_USER}/{DB_PASS}@{DB_DSN}") \
        .option("dbtable", full_table) \
        .option("driver", "oracle.jdbc.driver.OracleDriver") \
        .load()
    return df

def read_new_data_from_oracle(spark: SparkSession, table_name: str, 
                              last_timestamp: dt.datetime,
                              timestamp_column: str = TIMESTAMP_COLUMN) -> 'DataFrame':
    """
    ƒê·ªçc ch·ªâ d·ªØ li·ªáu M·ªöI t·ª´ Oracle d·ª±a tr√™n timestamp.
    """
    schema_prefix = f'"{DB_USER}"."'
    full_table = f'{schema_prefix}{table_name}"'
    
    # T·∫°o query ƒë·ªÉ ch·ªâ l·∫•y d·ªØ li·ªáu m·ªõi
    ts_str = last_timestamp.strftime('%Y-%m-%d %H:%M:%S')
    query = f"""
        (SELECT * FROM {full_table}
         WHERE {timestamp_column} > TO_TIMESTAMP('{ts_str}', 'YYYY-MM-DD HH24:MI:SS')
         ORDER BY {timestamp_column})
    """
    
    try:
        df = spark.read \
            .format("jdbc") \
            .option("url", f"jdbc:oracle:thin:{DB_USER}/{DB_PASS}@{DB_DSN}") \
            .option("dbtable", query) \
            .option("driver", "oracle.jdbc.driver.OracleDriver") \
            .load()
        return df
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi ƒë·ªçc d·ªØ li·ªáu m·ªõi: {e}")
        return spark.createDataFrame([], get_fact_schema())

def get_last_timestamp_from_checkpoint(spark: SparkSession) -> dt.datetime:
    """L·∫•y timestamp cu·ªëi c√πng t·ª´ checkpoint."""
    try:
        df = read_table_from_oracle(spark, "ETL_CHECKPOINT", DB_USER)
        df_checkpoint = df.filter(col("JOB_NAME") == JOB_NAME)
        
        if df_checkpoint.count() > 0:
            last_run = df_checkpoint.select("LAST_RUN").first()
            if last_run and last_run[0]:
                return last_run[0]
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c checkpoint: {e}")
    
    # N·∫øu ch∆∞a c√≥ checkpoint, l·∫•y timestamp t·ª´ b·∫£ng FACT
    try:
        df_fact = read_table_from_oracle(spark, "GOLD_PRICE_FACT", DB_USER)
        if df_fact.count() > 0:
            max_ts = df_fact.agg(spark_max(col(TIMESTAMP_COLUMN))).first()[0]
            if max_ts:
                return max_ts
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c timestamp t·ª´ FACT: {e}")
    
    return dt.datetime(2000, 1, 1)

def update_checkpoint(spark: SparkSession, ts: dt.datetime):
    """C·∫≠p nh·∫≠t checkpoint."""
    checkpoint_df = spark.createDataFrame(
        [(JOB_NAME, ts)],
        ["JOB_NAME", "LAST_RUN"]
    )
    
    try:
        existing = read_table_from_oracle(spark, "ETL_CHECKPOINT", DB_USER)
        combined = existing.filter(col("JOB_NAME") != JOB_NAME).union(checkpoint_df)
    except:
        combined = checkpoint_df
    
    combined.write \
        .format("jdbc") \
        .option("url", f"jdbc:oracle:thin:{DB_USER}/{DB_PASS}@{DB_DSN}") \
        .option("dbtable", f"{DB_USER}.ETL_CHECKPOINT") \
        .option("driver", "oracle.jdbc.driver.OracleDriver") \
        .mode("overwrite") \
        .save()

def write_table_to_oracle(df: 'DataFrame', table_name: str, mode: str = "append"):
    """Ghi DataFrame v√†o Oracle DB."""
    if df.count() == 0:
        return
    
    df.write \
        .format("jdbc") \
        .option("url", f"jdbc:oracle:thin:{DB_USER}/{DB_PASS}@{DB_DSN}") \
        .option("dbtable", table_name) \
        .option("driver", "oracle.jdbc.driver.OracleDriver") \
        .mode(mode) \
        .save()

def get_fact_schema():
    """Schema cho GOLD_PRICE_FACT."""
    return StructType([
        StructField("SOURCE_ID", IntegerType(), True),
        StructField("TYPE_ID", IntegerType(), True),
        StructField("LOCATION_ID", IntegerType(), True),
        StructField("TIME_ID", IntegerType(), True),
        StructField("BUY_PRICE", DoubleType(), True),
        StructField("SELL_PRICE", DoubleType(), True),
        StructField("RECORDED_AT", TimestampType(), True),
        StructField("UNIT", StringType(), True),
    ])

# ==================== PROCESSING FUNCTIONS ====================

def process_new_fact_data(spark: SparkSession, df_new: 'DataFrame',
                          location_mapping: Dict, type_mapping: Dict) -> 'DataFrame':
    """
    X·ª≠ l√Ω d·ªØ li·ªáu FACT m·ªõi:
    1. Apply location/type mappings
    2. Deduplicate
    3. Handle missing values
    4. Flag outliers
    """
    if df_new.count() == 0:
        return df_new
    
    # Apply location mapping
    if location_mapping:
        mapping_df = spark.createDataFrame(
            [(k, v) for k, v in location_mapping.items()],
            ["OLD_LOC_ID", "NEW_LOC_ID"]
        )
        df_new = df_new.join(
            mapping_df,
            df_new["LOCATION_ID"] == mapping_df["OLD_LOC_ID"],
            "left"
        ).withColumn(
            "LOCATION_ID",
            when(col("NEW_LOC_ID").isNotNull(), col("NEW_LOC_ID"))
            .otherwise(col("LOCATION_ID"))
        ).drop("OLD_LOC_ID", "NEW_LOC_ID")
    
    # Apply type mapping
    if type_mapping:
        mapping_df = spark.createDataFrame(
            [(k, v) for k, v in type_mapping.items()],
            ["OLD_TYPE_ID", "NEW_TYPE_ID"]
        )
        df_new = df_new.join(
            mapping_df,
            df_new["TYPE_ID"] == mapping_df["OLD_TYPE_ID"],
            "left"
        ).withColumn(
            "TYPE_ID",
            when(col("NEW_TYPE_ID").isNotNull(), col("NEW_TYPE_ID"))
            .otherwise(col("TYPE_ID"))
        ).drop("OLD_TYPE_ID", "NEW_TYPE_ID")
    
    # Deduplicate (gi·ªØ record m·ªõi nh·∫•t)
    df_new = df_new.withColumn(
        "COMBO",
        concat_ws("|",
            col("SOURCE_ID").cast("string"),
            col("TYPE_ID").cast("string"),
            col("LOCATION_ID").cast("string"),
            col("TIME_ID").cast("string")
        )
    )
    
    window_spec = Window.partitionBy("COMBO").orderBy(col(TIMESTAMP_COLUMN).desc())
    df_new = df_new.withColumn("rn", row_number().over(window_spec)) \
        .filter(col("rn") == 1) \
        .drop("rn", "COMBO")
    
    # Handle missing values
    df_new = df_new.filter(
        col("BUY_PRICE").isNotNull() &
        col("SELL_PRICE").isNotNull() &
        col(TIMESTAMP_COLUMN).isNotNull()
    )
    
    # Flag outliers
    df_new = df_new.withColumn("IS_DELETED", lit(0))
    df_new = df_new.withColumn("IS_DELETE", lit(0))
    
    return df_new

def load_dimension_mappings(spark: SparkSession) -> Tuple[Dict, Dict]:
    """Load location v√† type mappings t·ª´ CLEAN tables."""
    location_mapping = {}
    type_mapping = {}
    
    # TODO: Implement logic ƒë·ªÉ load mappings
    # C√≥ th·ªÉ load t·ª´ LOCATION_DIMENSION_CLEAN v√† GOLD_TYPE_DIMENSION_CLEAN
    
    return location_mapping, type_mapping

# ==================== STREAMING WITH FOREACHBATCH ====================

def process_batch(batch_id: int, batch_df: 'DataFrame', 
                 spark: SparkSession, table_name: str,
                 location_mapping: Dict, type_mapping: Dict):
    """
    X·ª≠ l√Ω m·ªói batch trong streaming.
    ƒê∆∞·ª£c g·ªçi t·ª± ƒë·ªông b·ªüi foreachBatch.
    """
    print(f"\n{'='*60}")
    print(f"üì¶ Batch {batch_id} - {dt.datetime.now()}")
    print(f"{'='*60}")
    
    # B·ªè qua batch_df (kh√¥ng d√πng, ch·ªâ l√† trigger)
    # ƒê·ªçc d·ªØ li·ªáu m·ªõi t·ª´ Oracle
    last_ts = get_last_timestamp_from_checkpoint(spark)
    print(f"üîç ƒêang ki·ªÉm tra d·ªØ li·ªáu m·ªõi sau {last_ts}...")
    
    df_new = read_new_data_from_oracle(spark, table_name, last_ts)
    
    if df_new.count() == 0:
        print("‚ÑπÔ∏è Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi trong batch n√†y")
        return
    
    print(f"üìä S·ªë l∆∞·ª£ng records m·ªõi: {df_new.count()}")
    
    # X·ª≠ l√Ω d·ªØ li·ªáu m·ªõi
    df_processed = process_new_fact_data(spark, df_new, location_mapping, type_mapping)
    
    if df_processed.count() == 0:
        print("‚ö†Ô∏è Sau x·ª≠ l√Ω kh√¥ng c√≤n d·ªØ li·ªáu")
        return
    
    # Merge v·ªõi d·ªØ li·ªáu CLEAN hi·ªán c√≥ (ƒë·ªÉ dedup to√†n b·ªô)
    try:
        df_existing = read_table_from_oracle(spark, "GOLD_PRICE_FACT_CLEAN", DB_USER)
        df_combined = df_existing.unionByName(df_processed, allowMissingColumns=True)
        
        # Deduplicate to√†n b·ªô
        df_combined = df_combined.withColumn(
            "COMBO",
            concat_ws("|",
                col("SOURCE_ID").cast("string"),
                col("TYPE_ID").cast("string"),
                col("LOCATION_ID").cast("string"),
                col("TIME_ID").cast("string")
            )
        )
        
        window_spec = Window.partitionBy("COMBO").orderBy(col(TIMESTAMP_COLUMN).desc())
        df_final = df_combined.withColumn("rn", row_number().over(window_spec)) \
            .filter(col("rn") == 1) \
            .drop("rn", "COMBO")
        
        # Ghi l·∫°i b·∫£ng CLEAN
        write_table_to_oracle(df_final, f"{DB_USER}.GOLD_PRICE_FACT_CLEAN", "overwrite")
        print(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t GOLD_PRICE_FACT_CLEAN: {df_final.count()} records")
        
        # C·∫≠p nh·∫≠t checkpoint v·ªõi timestamp m·ªõi nh·∫•t
        max_ts = df_processed.agg(spark_max(col(TIMESTAMP_COLUMN))).first()[0]
        if max_ts:
            update_checkpoint(spark, max_ts)
            print(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t checkpoint: {max_ts}")
    
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi merge v·ªõi CLEAN: {e}")
        # N·∫øu l·ªói, ch·ªâ append d·ªØ li·ªáu m·ªõi
        write_table_to_oracle(df_processed, f"{DB_USER}.GOLD_PRICE_FACT_CLEAN", "append")
        print(f"‚úÖ ƒê√£ append {df_processed.count()} records m·ªõi")

def create_oracle_polling_stream(spark: SparkSession, table_name: str,
                                location_mapping: Dict, type_mapping: Dict,
                                trigger_interval: str = STREAMING_TRIGGER_INTERVAL):
    """
    T·∫°o Spark Structured Streaming query ƒë·ªÉ polling Oracle.
    
    C√°ch ho·∫°t ƒë·ªông:
    1. D√πng rate source ƒë·ªÉ t·∫°o trigger (emit 1 row m·ªói interval)
    2. D√πng foreachBatch ƒë·ªÉ polling Oracle m·ªói interval
    3. Spark t·ª± ƒë·ªông qu·∫£n l√Ω checkpoint v√† recovery
    """
    
    # T·∫°o rate source - emit 1 row m·ªói interval ƒë·ªÉ trigger foreachBatch
    # Rate source l√† built-in streaming source c·ªßa Spark
    trigger_df = spark.readStream \
        .format("rate") \
        .option("rowsPerSecond", 1) \
        .option("numPartitions", 1) \
        .load()
    
    # Ch·ªâ l·∫•y timestamp column ƒë·ªÉ l√†m trigger
    trigger_df = trigger_df.select(col("timestamp").alias("trigger_time"))
    
    # T·∫°o streaming query v·ªõi foreachBatch
    def foreach_batch_wrapper(batch_id, batch_df):
        # B·ªè qua batch_df (ch·ªâ l√† trigger)
        # G·ªçi process_batch ƒë·ªÉ polling Oracle
        process_batch(batch_id, batch_df, spark, table_name, location_mapping, type_mapping)
    
    # T·∫°o streaming query
    query = trigger_df.writeStream \
        .foreachBatch(foreach_batch_wrapper) \
        .outputMode("update") \
        .trigger(processingTime=trigger_interval) \
        .option("checkpointLocation", f"{STREAMING_CHECKPOINT_DIR}/oracle_polling") \
        .start()
    
    return query

# ==================== MAIN ====================

def main():
    parser = argparse.ArgumentParser(description="Spark Structured Streaming v·ªõi Oracle polling")
    parser.add_argument("--interval", type=str, default=STREAMING_TRIGGER_INTERVAL,
                       help="Trigger interval (v√≠ d·ª•: '30 seconds', '1 minute')")
    parser.add_argument("--table", type=str, default="GOLD_PRICE_FACT",
                       help="T√™n b·∫£ng Oracle ƒë·ªÉ monitor")
    
    args = parser.parse_args()
    
    # T·∫°o checkpoint directory
    os.makedirs(STREAMING_CHECKPOINT_DIR, exist_ok=True)
    
    spark = create_spark_session()
    
    print("\n" + "="*60)
    print("üöÄ SPARK STRUCTURED STREAMING - ORACLE POLLING")
    print("="*60)
    print(f"üìä Table: {args.table}")
    print(f"‚è±Ô∏è  Trigger Interval: {args.interval}")
    print(f"üìÅ Checkpoint: {STREAMING_CHECKPOINT_DIR}")
    print("="*60 + "\n")
    
    # Load dimension mappings (ch·∫°y m·ªôt l·∫ßn, c√≥ th·ªÉ refresh ƒë·ªãnh k·ª≥)
    print("üìä ƒêang load dimension mappings...")
    location_mapping, type_mapping = load_dimension_mappings(spark)
    print(f"‚úÖ Location mappings: {len(location_mapping)}")
    print(f"‚úÖ Type mappings: {len(type_mapping)}")
    
    # Kh·ªüi ƒë·ªông streaming query
    query = create_oracle_polling_stream(
        spark, 
        args.table, 
        location_mapping, 
        type_mapping,
        args.interval
    )
    
    print(f"\n‚úÖ Streaming query ƒë√£ kh·ªüi ƒë·ªông!")
    print(f"üìä Query ID: {query.id}")
    print(f"üìä Status: {query.status}")
    print(f"üìä Spark UI: http://localhost:4040")
    print(f"\nüîÑ ƒêang ch·∫°y... Nh·∫•n Ctrl+C ƒë·ªÉ d·ª´ng\n")
    
    try:
        # Ch·ªù streaming query ch·∫°y
        query.awaitTermination()
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è ƒêang d·ª´ng streaming query...")
        query.stop()
        print("‚úÖ ƒê√£ d·ª´ng")
    
    spark.stop()

if __name__ == "__main__":
    main()

