# -*- coding: utf-8 -*-
"""
Daily Gold ETL Job - Spark Structured Streaming vá»›i Oracle Polling
Tá»± Ä‘á»™ng phÃ¡t hiá»‡n vÃ  xá»­ lÃ½ khi Oracle database thay Ä‘á»•i

Giáº£i phÃ¡p: Spark Structured Streaming + foreachBatch
- DÃ¹ng memory source lÃ m trigger
- foreachBatch Ä‘á»ƒ polling Oracle má»—i interval
- CÃ³ checkpoint tá»± Ä‘á»™ng, recovery, monitoring
"""

import argparse
import datetime as dt
import os
import sys
from typing import Dict, List, Tuple, Optional

import pandas as pd
import numpy as np
from pyspark.sql import SparkSession
from pyspark.sql.streaming import StreamingQuery
from pyspark.sql.functions import (
    col, when, lit, trim, upper, lower, regexp_replace, 
    concat_ws, first, last, max as spark_max, min as spark_min,
    count, isnan, isnull, coalesce, to_timestamp, date_format,
    row_number, window, monotonically_increasing_id, current_timestamp
)
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, FloatType, 
    TimestampType, DoubleType, LongType
)
from pyspark.sql.window import Window

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from fuzzywuzzy import fuzz

import re
import unicodedata

# Import cÃ¡c hÃ m clean tá»« batch job
# ThÃªm Ä‘Æ°á»ng dáº«n Ä‘á»ƒ import
sys.path.insert(0, os.path.dirname(__file__))
try:
    from daily_gold_job_normalization_spark import (
        normalize_locations,
        enrich_gold_types,
        normalize_purity_format,
        normalize_category_smart,
        normalize_gold_type_and_unit,
        merge_duplicate_types_and_update_fact,
        build_similarity_groups,
        norm_txt,
        snapshot_table
    )
    BATCH_FUNCTIONS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ KhÃ´ng thá»ƒ import batch functions: {e}")
    print("   Sáº½ chá»‰ xá»­ lÃ½ FACT, khÃ´ng clean LOCATION vÃ  TYPE")
    BATCH_FUNCTIONS_AVAILABLE = False

# ====================== CONFIG ======================
# Äá»c tá»« environment variables (Docker) hoáº·c dÃ¹ng giÃ¡ trá»‹ máº·c Ä‘á»‹nh
DB_USER = os.environ.get("DB_USER", "SYSTEM")
DB_PASS = os.environ.get("DB_PASS", "Welcome_1234")
DB_HOST = os.environ.get("DB_HOST", "136.110.60.196")
DB_PORT = os.environ.get("DB_PORT", "1521")
DB_SERVICE = os.environ.get("DB_SERVICE", "XEPDB1")

DB_DSN = f"{DB_HOST}:{DB_PORT}/{DB_SERVICE}"
DB_URL = f"jdbc:oracle:thin:@{DB_DSN}"

SNAPSHOT_DIR = "./snapshots"
JOB_NAME = "DAILY_GOLD_JOB_STREAMING_ORACLE"
SIM_THRESHOLD_LOC = 0.80
SIM_THRESHOLD_TYPE = 0.75
FUZZY_FALLBACK = 90

# CÃ¡c constants cáº§n thiáº¿t cho batch functions (náº¿u import Ä‘Æ°á»£c)
try:
    from daily_gold_job_normalization_spark import (
        SIM_THRESHOLD_LOC as BATCH_SIM_THRESHOLD_LOC,
        SIM_THRESHOLD_TYPE as BATCH_SIM_THRESHOLD_TYPE,
        FUZZY_FALLBACK as BATCH_FUZZY_FALLBACK
    )
except:
    pass

# Streaming config
STREAMING_CHECKPOINT_DIR = "./checkpoints/streaming_oracle"
STREAMING_TRIGGER_INTERVAL = "60 seconds"  # Polling má»—i 60 giÃ¢y
TIMESTAMP_COLUMN = "RECORDED_AT"  # Cá»™t timestamp Ä‘á»ƒ phÃ¡t hiá»‡n thay Ä‘á»•i

# Spark config
SPARK_APP_NAME = "DailyGoldETLJobStreamingOracle"
SPARK_MASTER = "local[*]"

# ====================================================

def create_spark_session(ojdbc_path: str = None):
    """Táº¡o SparkSession vá»›i cáº¥u hÃ¬nh streaming."""
    builder = SparkSession.builder \
        .appName(SPARK_APP_NAME) \
        .master(SPARK_MASTER) \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.driver.memory", "2g") \
        .config("spark.executor.memory", "2g") \
        .config("spark.sql.streaming.checkpointLocation", STREAMING_CHECKPOINT_DIR)
    
    # ThÃªm JDBC driver náº¿u cÃ³
    if ojdbc_path:
        if os.path.exists(ojdbc_path):
            builder = builder.config("spark.jars", ojdbc_path)
            print(f"âœ… ÄÃ£ load JDBC driver tá»«: {ojdbc_path}")
        else:
            print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y JDBC driver táº¡i: {ojdbc_path}")
    else:
        possible_paths = [
            "ojdbc8.jar",
            "./ojdbc8.jar",
            "../ojdbc8.jar",
            os.path.join(os.path.dirname(__file__), "ojdbc8.jar")
        ]
        for path in possible_paths:
            if os.path.exists(path):
                builder = builder.config("spark.jars", os.path.abspath(path))
                print(f"âœ… ÄÃ£ tá»± Ä‘á»™ng tÃ¬m tháº¥y JDBC driver: {os.path.abspath(path)}")
                break
    
    spark = builder.getOrCreate()
    return spark

def read_table_from_oracle(spark: SparkSession, table_name: str, schema: str = None) -> 'DataFrame':
    """Äá»c báº£ng tá»« Oracle DB (batch)."""
    schema_prefix = f'"{schema}"."' if schema else '"'
    full_table = f'{schema_prefix}{table_name}"'
    
    df = spark.read \
        .format("jdbc") \
        .option("url", f"jdbc:oracle:thin:{DB_USER}/{DB_PASS}@{DB_DSN}") \
        .option("dbtable", full_table) \
        .option("driver", "oracle.jdbc.driver.OracleDriver") \
        .load()
    return df

def read_new_data_from_oracle(spark: SparkSession, table_name: str, 
                              last_timestamp: dt.datetime,
                              timestamp_column: str = TIMESTAMP_COLUMN) -> 'DataFrame':
    """
    Äá»c chá»‰ dá»¯ liá»‡u Má»šI tá»« Oracle dá»±a trÃªn timestamp.
    
    Logic:
    - Query: WHERE RECORDED_AT > last_timestamp
    - Chá»‰ láº¥y dá»¯ liá»‡u sau timestamp cuá»‘i cÃ¹ng Ä‘Ã£ xá»­ lÃ½
    - ORDER BY timestamp Ä‘á»ƒ Ä‘áº£m báº£o thá»© tá»±
    
    Args:
        spark: SparkSession
        table_name: TÃªn báº£ng Oracle
        last_timestamp: Timestamp cuá»‘i cÃ¹ng Ä‘Ã£ xá»­ lÃ½
        timestamp_column: TÃªn cá»™t timestamp (máº·c Ä‘á»‹nh: RECORDED_AT)
    
    Returns:
        DataFrame: Dá»¯ liá»‡u má»›i sau last_timestamp
    """
    schema_prefix = f'"{DB_USER}"."'
    full_table = f'{schema_prefix}{table_name}"'
    
    # Táº¡o query Ä‘á»ƒ chá»‰ láº¥y dá»¯ liá»‡u má»›i
    # DÃ¹ng > (lá»›n hÆ¡n) Ä‘á»ƒ trÃ¡nh láº¥y láº¡i record Ä‘Ã£ xá»­ lÃ½
    ts_str = last_timestamp.strftime('%Y-%m-%d %H:%M:%S')
    query = f"""
        (SELECT * FROM {full_table}
         WHERE {timestamp_column} > TO_TIMESTAMP('{ts_str}', 'YYYY-MM-DD HH24:MI:SS')
         ORDER BY {timestamp_column})
    """
    
    print(f"   ğŸ” Query: WHERE {timestamp_column} > '{ts_str}'")
    
    try:
        df = spark.read \
            .format("jdbc") \
            .option("url", f"jdbc:oracle:thin:{DB_USER}/{DB_PASS}@{DB_DSN}") \
            .option("dbtable", query) \
            .option("driver", "oracle.jdbc.driver.OracleDriver") \
            .load()
        
        count = df.count()
        if count > 0:
            # Láº¥y min vÃ  max timestamp Ä‘á»ƒ log
            min_ts = df.agg(spark_min(col(timestamp_column))).first()[0]
            max_ts = df.agg(spark_max(col(timestamp_column))).first()[0]
            print(f"   âœ… TÃ¬m tháº¥y {count} records má»›i (tá»« {min_ts} Ä‘áº¿n {max_ts})")
        else:
            print(f"   â„¹ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u má»›i sau {ts_str}")
        
        return df
    except Exception as e:
        print(f"   âš ï¸ Lá»—i khi Ä‘á»c dá»¯ liá»‡u má»›i: {e}")
        print(f"   ğŸ“ Tráº£ vá» DataFrame rá»—ng")
        return spark.createDataFrame([], get_fact_schema())

def get_last_timestamp_from_checkpoint(spark: SparkSession) -> dt.datetime:
    """
    Láº¥y timestamp cuá»‘i cÃ¹ng tá»« checkpoint.
    
    Logic:
    1. Äá»c tá»« báº£ng ETL_CHECKPOINT vá»›i JOB_NAME
    2. Náº¿u khÃ´ng cÃ³, láº¥y max timestamp tá»« GOLD_PRICE_FACT
    3. Náº¿u váº«n khÃ´ng cÃ³, dÃ¹ng 2000-01-01 lÃ m máº·c Ä‘á»‹nh
    
    Returns:
        dt.datetime: Timestamp cuá»‘i cÃ¹ng Ä‘Ã£ xá»­ lÃ½
    """
    try:
        df = read_table_from_oracle(spark, "ETL_CHECKPOINT", DB_USER)
        df_checkpoint = df.filter(col("JOB_NAME") == JOB_NAME)
        
        if df_checkpoint.count() > 0:
            last_run = df_checkpoint.select("LAST_RUN").first()
            if last_run and last_run[0]:
                last_ts = last_run[0]
                print(f"ğŸ“Œ Checkpoint tÃ¬m tháº¥y: {last_ts}")
                return last_ts
            else:
                print("âš ï¸ Checkpoint cÃ³ record nhÆ°ng LAST_RUN lÃ  NULL")
        else:
            print("â„¹ï¸ ChÆ°a cÃ³ checkpoint trong ETL_CHECKPOINT, Ä‘ang tÃ¬m trong FACT...")
    except Exception as e:
        print(f"âš ï¸ KhÃ´ng Ä‘á»c Ä‘Æ°á»£c checkpoint: {e}")
        print("   Äang fallback sang FACT table...")
    
    # Náº¿u chÆ°a cÃ³ checkpoint, láº¥y timestamp tá»« báº£ng FACT
    try:
        df_fact = read_table_from_oracle(spark, "GOLD_PRICE_FACT", DB_USER)
        if df_fact.count() > 0:
            max_ts = df_fact.agg(spark_max(col(TIMESTAMP_COLUMN))).first()[0]
            if max_ts:
                print(f"ğŸ“Œ Láº¥y max timestamp tá»« FACT: {max_ts}")
                return max_ts
            else:
                print("âš ï¸ FACT cÃ³ dá»¯ liá»‡u nhÆ°ng khÃ´ng cÃ³ timestamp há»£p lá»‡")
        else:
            print("â„¹ï¸ FACT table trá»‘ng")
    except Exception as e:
        print(f"âš ï¸ KhÃ´ng láº¥y Ä‘Æ°á»£c timestamp tá»« FACT: {e}")
    
    default_ts = dt.datetime(2000, 1, 1)
    print(f"ğŸ“Œ Sá»­ dá»¥ng timestamp máº·c Ä‘á»‹nh: {default_ts}")
    return default_ts

def update_checkpoint(spark: SparkSession, ts: dt.datetime):
    """
    Cáº­p nháº­t checkpoint vá»›i timestamp má»›i nháº¥t.
    
    Logic:
    1. Äá»c táº¥t cáº£ records tá»« ETL_CHECKPOINT
    2. Filter ra record cá»§a job khÃ¡c (giá»¯ láº¡i)
    3. Union vá»›i record má»›i cá»§a job nÃ y
    4. Overwrite toÃ n bá»™ báº£ng (cÃ³ thá»ƒ cáº£i thiá»‡n báº±ng MERGE/UPDATE)
    
    Args:
        spark: SparkSession
        ts: Timestamp má»›i nháº¥t Ä‘Ã£ xá»­ lÃ½
    """
    print(f"ğŸ’¾ Äang cáº­p nháº­t checkpoint vá»›i timestamp: {ts}")
    
    checkpoint_df = spark.createDataFrame(
        [(JOB_NAME, ts)],
        ["JOB_NAME", "LAST_RUN"]
    )
    
    try:
        # Äá»c táº¥t cáº£ records hiá»‡n cÃ³
        existing = read_table_from_oracle(spark, "ETL_CHECKPOINT", DB_USER)
        existing_count = existing.count()
        print(f"   ğŸ“Š Records hiá»‡n cÃ³ trong checkpoint: {existing_count}")
        
        # Giá»¯ láº¡i records cá»§a job khÃ¡c, thÃªm/update record cá»§a job nÃ y
        other_jobs = existing.filter(col("JOB_NAME") != JOB_NAME)
        other_count = other_jobs.count()
        print(f"   ğŸ“Š Records cá»§a job khÃ¡c: {other_count}")
        
        combined = other_jobs.union(checkpoint_df)
        combined_count = combined.count()
        print(f"   ğŸ“Š Tá»•ng records sau merge: {combined_count}")
        
    except Exception as e:
        print(f"   âš ï¸ KhÃ´ng Ä‘á»c Ä‘Æ°á»£c checkpoint hiá»‡n cÃ³: {e}")
        print(f"   ğŸ“ Sáº½ táº¡o checkpoint má»›i")
        combined = checkpoint_df
    
    # Ghi láº¡i toÃ n bá»™ báº£ng (cÃ³ thá»ƒ cáº£i thiá»‡n báº±ng MERGE/UPDATE trong tÆ°Æ¡ng lai)
    try:
        combined.write \
            .format("jdbc") \
            .option("url", f"jdbc:oracle:thin:{DB_USER}/{DB_PASS}@{DB_DSN}") \
            .option("dbtable", f"{DB_USER}.ETL_CHECKPOINT") \
            .option("driver", "oracle.jdbc.driver.OracleDriver") \
            .mode("overwrite") \
            .save()
        print(f"   âœ… ÄÃ£ cáº­p nháº­t checkpoint thÃ nh cÃ´ng")
    except Exception as e:
        print(f"   âŒ Lá»—i khi cáº­p nháº­t checkpoint: {e}")
        raise

def write_table_to_oracle(df: 'DataFrame', table_name: str, mode: str = "append"):
    """
    Ghi DataFrame vÃ o Oracle DB.
    
    âš ï¸ QUAN TRá»ŒNG: Khi dÃ¹ng mode="overwrite", Spark JDBC cÃ³ thá»ƒ drop vÃ  táº¡o láº¡i báº£ng.
    Äáº£m báº£o DataFrame cÃ³ Ä‘Ãºng schema vÃ  thá»© tá»± cá»™t trÆ°á»›c khi ghi.
    """
    if df.count() == 0:
        print(f"   âš ï¸ DataFrame rá»—ng, khÃ´ng ghi vÃ o {table_name}")
        return
    
    # Log schema trÆ°á»›c khi ghi
    print(f"   ğŸ“ Ghi vÃ o {table_name} vá»›i mode={mode}")
    print(f"   ğŸ“ Schema: {df.columns}")
    print(f"   ğŸ“ Sá»‘ records: {df.count()}")
    
    try:
        df.write \
            .format("jdbc") \
            .option("url", f"jdbc:oracle:thin:{DB_USER}/{DB_PASS}@{DB_DSN}") \
            .option("dbtable", table_name) \
            .option("driver", "oracle.jdbc.driver.OracleDriver") \
            .mode(mode) \
            .save()
        print(f"   âœ… ÄÃ£ ghi thÃ nh cÃ´ng vÃ o {table_name}")
    except Exception as e:
        print(f"   âŒ Lá»—i khi ghi vÃ o {table_name}: {e}")
        raise

def get_fact_schema():
    """Schema cho GOLD_PRICE_FACT."""
    return StructType([
        StructField("SOURCE_ID", IntegerType(), True),
        StructField("TYPE_ID", IntegerType(), True),
        StructField("LOCATION_ID", IntegerType(), True),
        StructField("TIME_ID", IntegerType(), True),
        StructField("BUY_PRICE", DoubleType(), True),
        StructField("SELL_PRICE", DoubleType(), True),
        StructField("RECORDED_AT", TimestampType(), True),
        StructField("UNIT", StringType(), True),
    ])

# ==================== PROCESSING FUNCTIONS ====================

def process_new_fact_data(spark: SparkSession, df_new: 'DataFrame',
                          location_mapping: Dict, type_mapping: Dict) -> 'DataFrame':
    """
    Xá»­ lÃ½ dá»¯ liá»‡u FACT má»›i:
    1. Apply location/type mappings
    2. Deduplicate
    3. Handle missing values
    4. Flag outliers
    """
    if df_new.count() == 0:
        return df_new
    
    # Apply location mapping
    if location_mapping:
        mapping_df = spark.createDataFrame(
            [(k, v) for k, v in location_mapping.items()],
            ["OLD_LOC_ID", "NEW_LOC_ID"]
        )
        df_new = df_new.join(
            mapping_df,
            df_new["LOCATION_ID"] == mapping_df["OLD_LOC_ID"],
            "left"
        ).withColumn(
            "LOCATION_ID",
            when(col("NEW_LOC_ID").isNotNull(), col("NEW_LOC_ID"))
            .otherwise(col("LOCATION_ID"))
        ).drop("OLD_LOC_ID", "NEW_LOC_ID")
    
    # Apply type mapping
    if type_mapping:
        mapping_df = spark.createDataFrame(
            [(k, v) for k, v in type_mapping.items()],
            ["OLD_TYPE_ID", "NEW_TYPE_ID"]
        )
        df_new = df_new.join(
            mapping_df,
            df_new["TYPE_ID"] == mapping_df["OLD_TYPE_ID"],
            "left"
        ).withColumn(
            "TYPE_ID",
            when(col("NEW_TYPE_ID").isNotNull(), col("NEW_TYPE_ID"))
            .otherwise(col("TYPE_ID"))
        ).drop("OLD_TYPE_ID", "NEW_TYPE_ID")
    
    # Deduplicate (giá»¯ record má»›i nháº¥t)
    df_new = df_new.withColumn(
        "COMBO",
        concat_ws("|",
            col("SOURCE_ID").cast("string"),
            col("TYPE_ID").cast("string"),
            col("LOCATION_ID").cast("string"),
            col("TIME_ID").cast("string")
        )
    )
    
    window_spec = Window.partitionBy("COMBO").orderBy(col(TIMESTAMP_COLUMN).desc())
    df_new = df_new.withColumn("rn", row_number().over(window_spec)) \
        .filter(col("rn") == 1) \
        .drop("rn", "COMBO")
    
    # Handle missing values
    df_new = df_new.filter(
        col("BUY_PRICE").isNotNull() &
        col("SELL_PRICE").isNotNull() &
        col(TIMESTAMP_COLUMN).isNotNull()
    )
    
    # Flag outliers
    df_new = df_new.withColumn("IS_DELETED", lit(0))
    df_new = df_new.withColumn("IS_DELETE", lit(0))
    
    return df_new

def load_dimension_mappings(spark: SparkSession) -> Tuple[Dict, Dict]:
    """Load location vÃ  type mappings tá»« CLEAN tables."""
    location_mapping = {}
    type_mapping = {}
    
    # TODO: Implement logic Ä‘á»ƒ load mappings
    # CÃ³ thá»ƒ load tá»« LOCATION_DIMENSION_CLEAN vÃ  GOLD_TYPE_DIMENSION_CLEAN
    
    return location_mapping, type_mapping

# ==================== STREAMING WITH FOREACHBATCH ====================

def merge_duplicate_types_and_update_fact_streaming(spark: SparkSession) -> Dict:
    """
    Gá»™p cÃ¡c báº£n ghi trÃ¹ng trong GOLD_TYPE_DIMENSION_CLEAN vÃ  cáº­p nháº­t láº¡i báº£ng CLEAN.
    
    âš ï¸ QUAN TRá»ŒNG - TUYá»†T Äá»I KHÃ”NG Äá»˜NG Äáº¾N Báº¢NG Gá»C: 
    - âœ… CHá»ˆ Ä‘á»c tá»«: GOLD_TYPE_DIMENSION_CLEAN
    - âœ… Merge cÃ¡c records trÃ¹ng trong CLEAN (giá»¯ 1 record cho má»—i group)
    - âœ… Cáº­p nháº­t láº¡i báº£ng CLEAN vá»›i dá»¯ liá»‡u Ä‘Ã£ merge
    - âœ… Táº¡o mapping Ä‘á»ƒ dÃ¹ng cho FACT
    - âŒ KHÃ”NG Ä‘á»c tá»«: GOLD_TYPE_DIMENSION (báº£ng gá»‘c)
    - âŒ KHÃ”NG ghi vÃ o: GOLD_TYPE_DIMENSION (báº£ng gá»‘c)
    """
    from decimal import Decimal
    
    # âš ï¸ QUAN TRá»ŒNG: CHá»ˆ Ä‘á»c tá»« báº£ng CLEAN, KHÃ”NG Ä‘á»c tá»« báº£ng gá»‘c GOLD_TYPE_DIMENSION
    df = read_table_from_oracle(spark, "GOLD_TYPE_DIMENSION_CLEAN", DB_USER)
    
    if df.count() == 0:
        print("âš ï¸ GOLD_TYPE_DIMENSION_CLEAN trá»‘ng.")
        return {}

    original_count = df.count()
    print(f"ğŸ“Š Báº£ng CLEAN hiá»‡n cÃ³: {original_count} records")
    
    # âš ï¸ QUAN TRá»ŒNG: LÆ°u backup dá»¯ liá»‡u CLEAN cÅ© Ä‘á»ƒ restore náº¿u merge tháº¥t báº¡i
    df_backup = df
    print(f"   ğŸ’¾ ÄÃ£ backup {original_count} records Ä‘á»ƒ phá»¥c há»“i náº¿u cáº§n")

    # Kiá»ƒm tra cÃ¡c cá»™t cÃ³ tá»“n táº¡i khÃ´ng
    columns = df.columns
    df_normalized = df.withColumn(
        "TYPE_NAME_NORM", lower(trim(col("TYPE_NAME")))
    ).withColumn(
        "PURITY_NORM", lower(trim(col("PURITY")))
    ).withColumn(
        "CATEGORY_NORM", lower(trim(col("CATEGORY")))
    )
    
    # Chá»‰ thÃªm BRAND_NORM náº¿u cá»™t BRAND tá»“n táº¡i
    if "BRAND" in columns:
        df_normalized = df_normalized.withColumn(
            "BRAND_NORM", lower(trim(col("BRAND")))
        )
        partition_cols = ["TYPE_NAME_NORM", "PURITY_NORM", "CATEGORY_NORM", "BRAND_NORM"]
    else:
        # Táº¡o cá»™t BRAND_NORM rá»—ng náº¿u khÃ´ng cÃ³ BRAND
        df_normalized = df_normalized.withColumn("BRAND_NORM", lit(""))
        partition_cols = ["TYPE_NAME_NORM", "PURITY_NORM", "CATEGORY_NORM", "BRAND_NORM"]
        print("âš ï¸ Cá»™t BRAND khÃ´ng tá»“n táº¡i, sá»­ dá»¥ng giÃ¡ trá»‹ rá»—ng cho BRAND_NORM")

    # Group by normalized values and find canonical ID (ID nhá» nháº¥t trong má»—i group)
    window_spec = Window.partitionBy(*partition_cols).orderBy("ID")
    
    df_with_canon = df_normalized.withColumn(
        "CANON_ID",
        first("ID").over(window_spec)
    )

    # Create mapping: old_id -> new_id (canonical_id)
    mapping_df = df_with_canon.filter(col("ID") != col("CANON_ID")) \
        .select(col("ID").alias("OLD_ID"), col("CANON_ID").alias("NEW_ID")) \
        .distinct()
    
    mapping = {}
    if mapping_df.count() > 0:
        for row in mapping_df.collect():
            # Xá»­ lÃ½ OLD_ID vÃ  NEW_ID - cÃ³ thá»ƒ lÃ  Decimal, float, int, hoáº·c NaN
            old_id_val = row["OLD_ID"]
            new_id_val = row["NEW_ID"]
            
            # Skip náº¿u cÃ³ giÃ¡ trá»‹ None hoáº·c NaN
            if old_id_val is None or new_id_val is None:
                continue
            try:
                # Convert OLD_ID
                if isinstance(old_id_val, int):
                    old_id = old_id_val
                elif isinstance(old_id_val, (float, Decimal)):
                    if pd.isna(old_id_val):
                        continue
                    old_id = int(old_id_val)
                else:
                    old_id = int(float(str(old_id_val)))
                
                # Convert NEW_ID
                if isinstance(new_id_val, int):
                    new_id = new_id_val
                elif isinstance(new_id_val, (float, Decimal)):
                    if pd.isna(new_id_val):
                        continue
                    new_id = int(new_id_val)
                else:
                    new_id = int(float(str(new_id_val)))
                
                mapping[old_id] = new_id
            except (ValueError, TypeError, OverflowError):
                continue  # Skip náº¿u khÃ´ng convert Ä‘Æ°á»£c
    
    # Táº¡o báº£ng CLEAN má»›i: merge cÃ¡c records trÃ¹ng (giá»¯ 1 record cho má»—i CANON_ID)
    # âš ï¸ QUAN TRá»ŒNG: CHá»ˆ xá»­ lÃ½ báº£ng CLEAN, KHÃ”NG Ä‘á»™ng Ä‘áº¿n báº£ng gá»‘c
    # âš ï¸ QUAN TRá»ŒNG: Äáº£m báº£o thá»© tá»± cá»™t Ä‘Ãºng vá»›i schema Oracle: ID, TYPE_NAME, PURITY, CATEGORY, BRAND
    select_cols = ["TYPE_NAME", "PURITY", "CATEGORY"]
    if "BRAND" in columns:
        select_cols.append("BRAND")
    
    print(f"   ğŸ“ CÃ¡c cá»™t sáº½ giá»¯ láº¡i: {select_cols}")
    print(f"   ğŸ“ CÃ¡c cá»™t cÃ³ sáºµn trong df_with_canon: {df_with_canon.columns}")
    
    # Láº¥y giÃ¡ trá»‹ tá»« canonical record (ID nhá» nháº¥t) trong má»—i group
    window_spec_clean = Window.partitionBy("CANON_ID").orderBy("ID")
    
    # Kiá»ƒm tra df_with_canon trÆ°á»›c khi merge
    canon_count = df_with_canon.count()
    print(f"   ğŸ“Š Sá»‘ records sau khi tÃ¬m CANON_ID: {canon_count}")
    
    df_clean_merged = df_with_canon.withColumn(
        "ROW_NUM", row_number().over(window_spec_clean)
    ).filter(col("ROW_NUM") == 1)
    
    # Kiá»ƒm tra sau filter
    after_filter_count = df_clean_merged.count()
    print(f"   ğŸ“Š Sá»‘ records sau filter ROW_NUM=1: {after_filter_count}")
    
    # Select cÃ¡c cá»™t cáº§n thiáº¿t - Ä‘áº£m báº£o Ä‘Ãºng thá»© tá»±: ID, TYPE_NAME, PURITY, CATEGORY, BRAND
    try:
        # Kiá»ƒm tra tá»«ng cá»™t cÃ³ tá»“n táº¡i khÃ´ng
        missing_cols = [c for c in select_cols if c not in df_clean_merged.columns]
        if missing_cols:
            print(f"   âŒ Lá»—i: Thiáº¿u cÃ¡c cá»™t: {missing_cols}")
            print(f"   ğŸ“ CÃ¡c cá»™t cÃ³ sáºµn: {df_clean_merged.columns}")
            print(f"   ğŸ“Š Giá»¯ nguyÃªn dá»¯ liá»‡u CLEAN cÅ©: {original_count} records")
            return mapping
        
        # Select vá»›i thá»© tá»± Ä‘Ãºng: ID trÆ°á»›c, sau Ä‘Ã³ cÃ¡c cá»™t khÃ¡c
        df_clean_merged = df_clean_merged.select(
            col("CANON_ID").alias("ID"),
            *[col(c) for c in select_cols]
        )
        
        # Kiá»ƒm tra schema sau khi select
        print(f"   ğŸ“ Schema sau select: {df_clean_merged.columns}")
        print(f"   ğŸ“ Sá»‘ cá»™t: {len(df_clean_merged.columns)}")
        
    except Exception as e:
        print(f"   âŒ Lá»—i khi select columns: {e}")
        print(f"   ğŸ“ CÃ¡c cá»™t cÃ³ sáºµn: {df_clean_merged.columns}")
        print(f"   ğŸ“ CÃ¡c cá»™t cáº§n select: ID, {select_cols}")
        print(f"   ğŸ“Š Giá»¯ nguyÃªn dá»¯ liá»‡u CLEAN cÅ©: {original_count} records")
        return mapping
    
    clean_count = df_clean_merged.count()
    print(f"   ğŸ“Š Sá»‘ records sau select: {clean_count}")
    
    # âš ï¸ QUAN TRá»ŒNG: Kiá»ƒm tra an toÃ n trÆ°á»›c khi ghi
    # Náº¿u df_clean_merged rá»—ng hoáº·c máº¥t quÃ¡ nhiá»u dá»¯ liá»‡u, restore dá»¯ liá»‡u cÅ©
    if clean_count == 0:
        print(f"âŒ Lá»–I: Sau merge báº£ng CLEAN rá»—ng! KhÃ´i phá»¥c dá»¯ liá»‡u cÅ©...")
        try:
            write_table_to_oracle(df_backup, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
            print(f"   âœ… ÄÃ£ khÃ´i phá»¥c {original_count} records")
        except Exception as restore_error:
            print(f"   âŒ Lá»—i khi khÃ´i phá»¥c: {restore_error}")
        return mapping
    
    if clean_count < original_count * 0.5:  # Náº¿u máº¥t > 50% thÃ¬ cÃ³ váº¥n Ä‘á»
        print(f"âš ï¸ Cáº¢NH BÃO: Sau merge máº¥t quÃ¡ nhiá»u dá»¯ liá»‡u ({original_count} â†’ {clean_count})!")
        print(f"   ğŸ“Š KhÃ´i phá»¥c dá»¯ liá»‡u CLEAN cÅ© Ä‘á»ƒ trÃ¡nh máº¥t dá»¯ liá»‡u...")
        try:
            write_table_to_oracle(df_backup, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
            print(f"   âœ… ÄÃ£ khÃ´i phá»¥c {original_count} records")
        except Exception as restore_error:
            print(f"   âŒ Lá»—i khi khÃ´i phá»¥c: {restore_error}")
        return mapping
    
    if mapping:
        print(f"âœ… ÄÃ£ táº¡o mapping cho {len(mapping)} TYPE trÃ¹ng:")
        for old_id, new_id in list(mapping.items())[:5]:  # In 5 mapping Ä‘áº§u
            print(f"   ID {old_id} â†’ ID {new_id}")
        if len(mapping) > 5:
            print(f"   ... vÃ  {len(mapping) - 5} mapping khÃ¡c")
        
        # Cáº­p nháº­t báº£ng CLEAN vá»›i dá»¯ liá»‡u Ä‘Ã£ merge
        # âš ï¸ QUAN TRá»ŒNG: Chá»‰ cáº­p nháº­t báº£ng CLEAN, KHÃ”NG Ä‘á»™ng vÃ o báº£ng gá»‘c
        try:
            # Kiá»ƒm tra schema trÆ°á»›c khi ghi
            print(f"   ğŸ“ Schema trÆ°á»›c khi ghi: {df_clean_merged.columns}")
            print(f"   ğŸ“ Sá»‘ records: {clean_count}")
            
            # Kiá»ƒm tra dá»¯ liá»‡u cÃ³ NULL khÃ´ng
            null_counts = {}
            for col_name in df_clean_merged.columns:
                null_count = df_clean_merged.filter(col(col_name).isNull()).count()
                if null_count > 0:
                    null_counts[col_name] = null_count
            if null_counts:
                print(f"   âš ï¸ Cáº£nh bÃ¡o: CÃ³ NULL trong cÃ¡c cá»™t: {null_counts}")
            
            write_table_to_oracle(df_clean_merged, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
            
            # âš ï¸ QUAN TRá»ŒNG: Verify sau khi ghi - Ä‘á»c láº¡i Ä‘á»ƒ kiá»ƒm tra
            spark.catalog.clearCache()
            df_verify = read_table_from_oracle(spark, "GOLD_TYPE_DIMENSION_CLEAN", DB_USER)
            verify_count = df_verify.count()
            
            if verify_count == 0:
                print(f"âŒ Lá»–I: Sau khi ghi, báº£ng CLEAN bá»‹ rá»—ng! KhÃ´i phá»¥c dá»¯ liá»‡u cÅ©...")
                print(f"   ğŸ“ Schema Ä‘Ã£ ghi: {df_clean_merged.columns}")
                print(f"   ğŸ“ Schema Ä‘Ã£ Ä‘á»c láº¡i: {df_verify.columns if df_verify.count() > 0 else 'Báº£ng rá»—ng'}")
                write_table_to_oracle(df_backup, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
                print(f"   âœ… ÄÃ£ khÃ´i phá»¥c {original_count} records")
                return mapping
            
            if verify_count != clean_count:
                print(f"âš ï¸ Cáº£nh bÃ¡o: Sá»‘ records sau khi ghi ({verify_count}) khÃ¡c vá»›i expected ({clean_count})")
            
            # Kiá»ƒm tra schema sau khi Ä‘á»c láº¡i
            print(f"   ğŸ“ Schema sau khi Ä‘á»c láº¡i: {df_verify.columns}")
            
            print(f"âœ… ÄÃ£ cáº­p nháº­t GOLD_TYPE_DIMENSION_CLEAN: {verify_count} records (tá»« {original_count} records)")
            print(f"   ğŸ“ ÄÃ£ merge {original_count - verify_count} records trÃ¹ng")
        except Exception as e:
            print(f"âŒ Lá»–I khi ghi vÃ o báº£ng CLEAN: {e}")
            print(f"   ğŸ“Š KhÃ´i phá»¥c dá»¯ liá»‡u CLEAN cÅ©: {original_count} records...")
            try:
                write_table_to_oracle(df_backup, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
                print(f"   âœ… ÄÃ£ khÃ´i phá»¥c {original_count} records")
            except Exception as restore_error:
                print(f"   âŒ Lá»—i khi khÃ´i phá»¥c: {restore_error}")
            return mapping
        
        # âš ï¸ QUAN TRá»ŒNG: Cáº­p nháº­t GOLD_PRICE_FACT_CLEAN vá»›i mapping
        # Táº¥t cáº£ records cÃ³ TYPE_ID = old_id pháº£i Ä‘á»•i thÃ nh TYPE_ID = new_id
        print(f"\nğŸ”„ Äang cáº­p nháº­t GOLD_PRICE_FACT_CLEAN vá»›i type mapping...")
        try:
            df_fact_clean = read_table_from_oracle(spark, "GOLD_PRICE_FACT_CLEAN", DB_USER)
            fact_before_count = df_fact_clean.count()
            
            # âš ï¸ QUAN TRá»ŒNG: Backup dá»¯ liá»‡u FACT_CLEAN trÆ°á»›c khi cáº­p nháº­t
            df_fact_backup = df_fact_clean
            print(f"   ğŸ’¾ ÄÃ£ backup {fact_before_count} records FACT_CLEAN")
            
            if fact_before_count > 0:
                # Táº¡o mapping DataFrame
                mapping_df = spark.createDataFrame(
                    [(k, v) for k, v in mapping.items()],
                    ["OLD_TYPE_ID", "NEW_TYPE_ID"]
                )
                
                # Join vÃ  cáº­p nháº­t TYPE_ID (LEFT JOIN Ä‘á»ƒ giá»¯ Táº¤T Cáº¢ records)
                df_fact_updated = df_fact_clean.join(
                    mapping_df,
                    df_fact_clean["TYPE_ID"] == mapping_df["OLD_TYPE_ID"],
                    "left"  # LEFT JOIN Ä‘á»ƒ giá»¯ táº¥t cáº£ records, ká»ƒ cáº£ khÃ´ng cÃ³ mapping
                ).withColumn(
                    "TYPE_ID",
                    when(col("NEW_TYPE_ID").isNotNull(), col("NEW_TYPE_ID"))
                    .otherwise(col("TYPE_ID"))  # Giá»¯ nguyÃªn náº¿u khÃ´ng cÃ³ mapping
                ).drop("OLD_TYPE_ID", "NEW_TYPE_ID")
                
                fact_after_count = df_fact_updated.count()
                
                # âš ï¸ QUAN TRá»ŒNG: Kiá»ƒm tra an toÃ n - sá»‘ records pháº£i giá»¯ nguyÃªn
                if fact_after_count == 0:
                    print(f"   âŒ Lá»–I: Sau cáº­p nháº­t FACT_CLEAN bá»‹ rá»—ng! KhÃ´i phá»¥c...")
                    write_table_to_oracle(df_fact_backup, f"{DB_USER}.GOLD_PRICE_FACT_CLEAN", "overwrite")
                    print(f"   âœ… ÄÃ£ khÃ´i phá»¥c {fact_before_count} records FACT_CLEAN")
                elif fact_after_count != fact_before_count:
                    print(f"   âš ï¸ Cáº¢NH BÃO: Sá»‘ records thay Ä‘á»•i ({fact_before_count} â†’ {fact_after_count})!")
                    print(f"   ğŸ“Š KhÃ´i phá»¥c dá»¯ liá»‡u FACT_CLEAN cÅ©...")
                    write_table_to_oracle(df_fact_backup, f"{DB_USER}.GOLD_PRICE_FACT_CLEAN", "overwrite")
                    print(f"   âœ… ÄÃ£ khÃ´i phá»¥c {fact_before_count} records FACT_CLEAN")
                else:
                    # Äáº¿m sá»‘ records Ä‘Æ°á»£c cáº­p nháº­t
                    updated_count = df_fact_clean.join(
                        mapping_df,
                        df_fact_clean["TYPE_ID"] == mapping_df["OLD_TYPE_ID"],
                        "inner"
                    ).count()
                    
                    # Ghi láº¡i báº£ng FACT_CLEAN Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t
                    write_table_to_oracle(df_fact_updated, f"{DB_USER}.GOLD_PRICE_FACT_CLEAN", "overwrite")
                    
                    # Verify sau khi ghi
                    spark.catalog.clearCache()
                    df_fact_verify = read_table_from_oracle(spark, "GOLD_PRICE_FACT_CLEAN", DB_USER)
                    verify_count = df_fact_verify.count()
                    
                    if verify_count == 0:
                        print(f"   âŒ Lá»–I: Sau khi ghi, FACT_CLEAN bá»‹ rá»—ng! KhÃ´i phá»¥c...")
                        write_table_to_oracle(df_fact_backup, f"{DB_USER}.GOLD_PRICE_FACT_CLEAN", "overwrite")
                        print(f"   âœ… ÄÃ£ khÃ´i phá»¥c {fact_before_count} records FACT_CLEAN")
                    else:
                        print(f"   âœ… ÄÃ£ cáº­p nháº­t {updated_count} records trong GOLD_PRICE_FACT_CLEAN")
                        print(f"   ğŸ“Š GOLD_PRICE_FACT_CLEAN: {fact_before_count} â†’ {verify_count} records")
                        
                        # In chi tiáº¿t cÃ¡c mapping Ä‘Ã£ Ã¡p dá»¥ng
                        for old_id, new_id in mapping.items():
                            count = df_fact_clean.filter(col("TYPE_ID") == old_id).count()
                            if count > 0:
                                print(f"      TYPE_ID {old_id} â†’ {new_id}: {count} records")
            else:
                print(f"   â„¹ï¸ GOLD_PRICE_FACT_CLEAN trá»‘ng, khÃ´ng cáº§n cáº­p nháº­t")
        except Exception as e:
            print(f"   âŒ Lá»–I khi cáº­p nháº­t GOLD_PRICE_FACT_CLEAN: {e}")
            print(f"   ğŸ“Š KhÃ´i phá»¥c dá»¯ liá»‡u FACT_CLEAN cÅ©...")
            try:
                if 'df_fact_backup' in locals():
                    write_table_to_oracle(df_fact_backup, f"{DB_USER}.GOLD_PRICE_FACT_CLEAN", "overwrite")
                    print(f"   âœ… ÄÃ£ khÃ´i phá»¥c {fact_before_count} records FACT_CLEAN")
            except Exception as restore_error:
                print(f"   âŒ Lá»—i khi khÃ´i phá»¥c FACT_CLEAN: {restore_error}")
            print(f"   ğŸ“ Mapping váº«n Ä‘Æ°á»£c tráº£ vá» Ä‘á»ƒ dÃ¹ng cho FACT má»›i")
        
        print(f"   ğŸ“ Mapping sáº½ Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ cáº­p nháº­t FACT.TYPE_ID cho dá»¯ liá»‡u má»›i")
    else:
        print("â„¹ï¸ KhÃ´ng cÃ³ TYPE trÃ¹ng cáº§n gá»™p.")
        print(f"   ğŸ“Š Báº£ng CLEAN giá»¯ nguyÃªn: {original_count} records")
    
    return mapping

def clean_all_dimensions_incremental(spark: SparkSession, merge_types: bool = False) -> Tuple[Dict, Dict]:
    """
    Clean táº¥t cáº£ dimension tables (LOCATION vÃ  TYPE) - INCREMENTAL.
    Giá»¯ nguyÃªn dá»¯ liá»‡u CLEAN cÅ©, chá»‰ cáº­p nháº­t/thÃªm má»›i.
    Tráº£ vá» mappings Ä‘á»ƒ dÃ¹ng cho FACT.
    """
    if not BATCH_FUNCTIONS_AVAILABLE:
        print("âš ï¸ KhÃ´ng thá»ƒ clean dimensions, chá»‰ dÃ¹ng mappings hiá»‡n cÃ³")
        return {}, {}
    
    print("\n" + "="*60)
    print("ğŸ§¹ Äang clean táº¥t cáº£ dimension tables (INCREMENTAL)...")
    print("="*60)
    
    # B1: LOCATION normalize -> LOCATION_DIMENSION_CLEAN
    print("\nğŸ“ BÆ°á»›c 1: Normalize LOCATION_DIMENSION...")
    
    # Äá»c dá»¯ liá»‡u CLEAN hiá»‡n cÃ³ TRÆ¯á»šC (Ä‘á»ƒ giá»¯ láº¡i)
    try:
        df_loc_clean_existing = read_table_from_oracle(spark, "LOCATION_DIMENSION_CLEAN", DB_USER)
        existing_loc_count = df_loc_clean_existing.count()
        existing_loc_ids = set([row["ID"] for row in df_loc_clean_existing.select("ID").collect()])
        print(f"ğŸ“Š LOCATION_CLEAN hiá»‡n cÃ³: {existing_loc_count} records")
    except:
        df_loc_clean_existing = None
        existing_loc_ids = set()
        existing_loc_count = 0
        print("ğŸ“Š LOCATION_CLEAN chÆ°a cÃ³, sáº½ táº¡o má»›i")
    
    # Clear cache Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»c dá»¯ liá»‡u má»›i nháº¥t
    spark.catalog.clearCache()
    
    # Gá»i normalize_locations (sáº½ overwrite, nhÆ°ng ta sáº½ merge láº¡i sau)
    try:
        location_mapping = normalize_locations(spark)
    except Exception as e:
        print(f"âŒ Lá»—i khi normalize_locations: {e}")
        print(f"   Traceback: {type(e).__name__}: {str(e)}")
        # Fallback: KhÃ´ng cÃ³ mapping, chá»‰ dÃ¹ng dá»¯ liá»‡u hiá»‡n cÃ³
        location_mapping = {}
        print("âš ï¸ Sá»­ dá»¥ng location_mapping rá»—ng, giá»¯ nguyÃªn dá»¯ liá»‡u CLEAN hiá»‡n cÃ³")
    
    # Clear cache láº¡i sau khi normalize
    spark.catalog.clearCache()
    
    # Äá»c CLEAN má»›i sau khi normalize
    try:
        df_loc_clean_new = read_table_from_oracle(spark, "LOCATION_DIMENSION_CLEAN", DB_USER)
        new_loc_count = df_loc_clean_new.count()
        
        # Kiá»ƒm tra náº¿u báº£ng CLEAN má»›i rá»—ng nhÆ°ng cÃ³ dá»¯ liá»‡u cÅ©
        if new_loc_count == 0 and existing_loc_count > 0:
            print("âš ï¸ Báº£ng CLEAN má»›i rá»—ng nhÆ°ng cÃ³ dá»¯ liá»‡u cÅ©. Giá»¯ nguyÃªn dá»¯ liá»‡u cÅ©...")
            write_table_to_oracle(df_loc_clean_existing, f"{DB_USER}.LOCATION_DIMENSION_CLEAN", "overwrite")
            print(f"âœ… ÄÃ£ giá»¯ nguyÃªn LOCATION_DIMENSION_CLEAN: {existing_loc_count} records")
        
        # Merge: Giá»¯ nguyÃªn CLEAN cÅ© + CLEAN má»›i (union vÃ  distinct)
        elif df_loc_clean_existing is not None and existing_loc_count > 0:
            df_loc_clean_combined = df_loc_clean_existing.unionByName(df_loc_clean_new, allowMissingColumns=True)
            df_loc_clean_final = df_loc_clean_combined.distinct()
            final_count = df_loc_clean_final.count()
            
            # Äáº£m báº£o cÃ³ dá»¯ liá»‡u trÆ°á»›c khi ghi
            if final_count > 0:
                write_table_to_oracle(df_loc_clean_final, f"{DB_USER}.LOCATION_DIMENSION_CLEAN", "overwrite")
                print(f"âœ… ÄÃ£ cáº­p nháº­t LOCATION_DIMENSION_CLEAN: {final_count} records (giá»¯ {existing_loc_count} cÅ©)")
            else:
                print("âš ï¸ Sau merge khÃ´ng cÃ²n dá»¯ liá»‡u! Giá»¯ nguyÃªn dá»¯ liá»‡u cÅ©...")
                write_table_to_oracle(df_loc_clean_existing, f"{DB_USER}.LOCATION_DIMENSION_CLEAN", "overwrite")
                print(f"âœ… ÄÃ£ giá»¯ nguyÃªn LOCATION_DIMENSION_CLEAN: {existing_loc_count} records")
        else:
            # Kiá»ƒm tra náº¿u báº£ng CLEAN má»›i cÃ³ dá»¯ liá»‡u
            if new_loc_count > 0:
                print(f"âœ… ÄÃ£ táº¡o LOCATION_DIMENSION_CLEAN: {new_loc_count} records")
            else:
                print("âš ï¸ Báº£ng CLEAN má»›i rá»—ng! Kiá»ƒm tra láº¡i báº£ng gá»‘c...")
                # Fallback: Ä‘á»c tá»« báº£ng gá»‘c
                try:
                    df_original = read_table_from_oracle(spark, "LOCATION_DIMENSION", DB_USER)
                    original_count = df_original.count()
                    if original_count > 0:
                        print(f"âš ï¸ Copy {original_count} records tá»« báº£ng gá»‘c...")
                        write_table_to_oracle(df_original, f"{DB_USER}.LOCATION_DIMENSION_CLEAN", "overwrite")
                        print(f"âœ… ÄÃ£ copy tá»« báº£ng gá»‘c: {original_count} records")
                    else:
                        print("âŒ Báº£ng gá»‘c cÅ©ng trá»‘ng!")
                except Exception as e2:
                    print(f"âŒ KhÃ´ng thá»ƒ copy tá»« báº£ng gá»‘c: {e2}")
    except Exception as e:
        print(f"âš ï¸ Lá»—i khi merge LOCATION_CLEAN: {e}")
        # Fallback: giá»¯ nguyÃªn dá»¯ liá»‡u cÅ© náº¿u cÃ³
        if df_loc_clean_existing is not None and existing_loc_count > 0:
            try:
                write_table_to_oracle(df_loc_clean_existing, f"{DB_USER}.LOCATION_DIMENSION_CLEAN", "overwrite")
                print(f"âœ… ÄÃ£ giá»¯ nguyÃªn dá»¯ liá»‡u cÅ©: {existing_loc_count} records")
            except:
                pass
    
    print(f"âœ… Location mapping: {len(location_mapping)} mappings")
    
    # B2: GOLD TYPE enrich -> GOLD_TYPE_DIMENSION_CLEAN
    print("\nğŸ’ BÆ°á»›c 2: Enrich GOLD_TYPE_DIMENSION...")
    
    # Äá»c dá»¯ liá»‡u CLEAN hiá»‡n cÃ³ TRÆ¯á»šC (Ä‘á»ƒ giá»¯ láº¡i)
    try:
        df_type_clean_existing = read_table_from_oracle(spark, "GOLD_TYPE_DIMENSION_CLEAN", DB_USER)
        existing_type_count = df_type_clean_existing.count()
        print(f"ğŸ“Š TYPE_CLEAN hiá»‡n cÃ³: {existing_type_count} records")
    except:
        df_type_clean_existing = None
        existing_type_count = 0
        print("ğŸ“Š TYPE_CLEAN chÆ°a cÃ³, sáº½ táº¡o má»›i")
    
    # Clear cache Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»c dá»¯ liá»‡u má»›i nháº¥t
    spark.catalog.clearCache()
    
    # Gá»i cÃ¡c hÃ m enrich (sáº½ overwrite, nhÆ°ng ta sáº½ merge láº¡i sau)
    try:
        enrich_gold_types(spark)
        normalize_purity_format(spark)
        normalize_category_smart(spark)
    except Exception as e:
        print(f"âŒ Lá»—i khi enrich/normalize TYPE: {e}")
        print(f"   Traceback: {type(e).__name__}: {str(e)}")
        print("âš ï¸ Giá»¯ nguyÃªn dá»¯ liá»‡u TYPE_CLEAN hiá»‡n cÃ³")
    
    # Clear cache láº¡i sau khi gá»i cÃ¡c hÃ m
    spark.catalog.clearCache()
    
    # Äá»c CLEAN má»›i sau khi enrich
    try:
        df_type_clean_new = read_table_from_oracle(spark, "GOLD_TYPE_DIMENSION_CLEAN", DB_USER)
        new_type_count = df_type_clean_new.count()
        
        # Kiá»ƒm tra náº¿u báº£ng CLEAN má»›i rá»—ng nhÆ°ng cÃ³ dá»¯ liá»‡u cÅ©
        if new_type_count == 0 and existing_type_count > 0:
            print("âš ï¸ Báº£ng CLEAN má»›i rá»—ng nhÆ°ng cÃ³ dá»¯ liá»‡u cÅ©. Giá»¯ nguyÃªn dá»¯ liá»‡u cÅ©...")
            write_table_to_oracle(df_type_clean_existing, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
            print(f"âœ… ÄÃ£ giá»¯ nguyÃªn GOLD_TYPE_DIMENSION_CLEAN: {existing_type_count} records")
            return location_mapping, {}
        
        # Merge: Giá»¯ nguyÃªn CLEAN cÅ© + CLEAN má»›i (union vÃ  distinct)
        if df_type_clean_existing is not None and existing_type_count > 0:
            df_type_clean_combined = df_type_clean_existing.unionByName(df_type_clean_new, allowMissingColumns=True)
            # Deduplicate theo ID (giá»¯ record má»›i nháº¥t náº¿u cÃ³ trÃ¹ng)
            window_spec = Window.partitionBy("ID").orderBy(col("ID"))
            df_type_clean_final = df_type_clean_combined.withColumn("rn", row_number().over(window_spec)) \
                .filter(col("rn") == 1) \
                .drop("rn") \
                .distinct()
            final_count = df_type_clean_final.count()
            
            # Äáº£m báº£o cÃ³ dá»¯ liá»‡u trÆ°á»›c khi ghi
            if final_count > 0:
                write_table_to_oracle(df_type_clean_final, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
                print(f"âœ… ÄÃ£ cáº­p nháº­t GOLD_TYPE_DIMENSION_CLEAN: {final_count} records (giá»¯ {existing_type_count} cÅ©)")
            else:
                print("âš ï¸ Sau merge khÃ´ng cÃ²n dá»¯ liá»‡u! Giá»¯ nguyÃªn dá»¯ liá»‡u cÅ©...")
                write_table_to_oracle(df_type_clean_existing, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
                print(f"âœ… ÄÃ£ giá»¯ nguyÃªn GOLD_TYPE_DIMENSION_CLEAN: {existing_type_count} records")
        else:
            # Kiá»ƒm tra náº¿u báº£ng CLEAN má»›i cÃ³ dá»¯ liá»‡u
            if new_type_count > 0:
                print(f"âœ… ÄÃ£ táº¡o GOLD_TYPE_DIMENSION_CLEAN: {new_type_count} records")
            else:
                print("âš ï¸ Báº£ng CLEAN má»›i rá»—ng! Kiá»ƒm tra láº¡i báº£ng gá»‘c...")
                # Fallback: CHá»ˆ Ä‘á»c tá»« báº£ng gá»‘c Ä‘á»ƒ copy vÃ o CLEAN (KHÃ”NG sá»­a báº£ng gá»‘c)
                # ÄÃ¢y lÃ  trÆ°á»ng há»£p Ä‘áº·c biá»‡t khi CLEAN bá»‹ rá»—ng, cáº§n copy tá»« gá»‘c Ä‘á»ƒ khÃ´i phá»¥c
                try:
                    df_original = read_table_from_oracle(spark, "GOLD_TYPE_DIMENSION", DB_USER)
                    original_count = df_original.count()
                    if original_count > 0:
                        print(f"âš ï¸ Copy {original_count} records tá»« báº£ng gá»‘c vÃ o CLEAN...")
                        # QUAN TRá»ŒNG: Chá»‰ ghi vÃ o CLEAN, KHÃ”NG Ä‘á»™ng vÃ o báº£ng gá»‘c
                        write_table_to_oracle(df_original, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
                        print(f"âœ… ÄÃ£ copy tá»« báº£ng gá»‘c vÃ o CLEAN: {original_count} records")
                    else:
                        print("âŒ Báº£ng gá»‘c cÅ©ng trá»‘ng!")
                except Exception as e2:
                    print(f"âŒ KhÃ´ng thá»ƒ copy tá»« báº£ng gá»‘c: {e2}")
    except Exception as e:
        print(f"âš ï¸ Lá»—i khi merge TYPE_CLEAN: {e}")
        # Fallback: giá»¯ nguyÃªn dá»¯ liá»‡u cÅ© náº¿u cÃ³
        if df_type_clean_existing is not None and existing_type_count > 0:
            try:
                write_table_to_oracle(df_type_clean_existing, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
                print(f"âœ… ÄÃ£ giá»¯ nguyÃªn dá»¯ liá»‡u cÅ©: {existing_type_count} records")
            except:
                pass
    
    # (Tuá»³ chá»n) gá»™p TYPE tÆ°Æ¡ng Ä‘á»“ng
    # QUAN TRá»ŒNG: DÃ¹ng hÃ m riÃªng trong file streaming (KHÃ”NG import tá»« batch)
    # - Äá»c tá»«: GOLD_TYPE_DIMENSION_CLEAN
    # - CHá»ˆ táº¡o mapping, KHÃ”NG ghi Ä‘Ã¨ báº£ng CLEAN
    # - KHÃ”NG Ä‘á»™ng vÃ o báº£ng gá»‘c GOLD_TYPE_DIMENSION
    type_mapping = {}
    if merge_types:
        print("\nğŸ”— BÆ°á»›c 3: Merge duplicate types...")
        print("   ğŸ“ Chá»‰ xá»­ lÃ½ báº£ng CLEAN, khÃ´ng Ä‘á»™ng vÃ o báº£ng gá»‘c")
        print("   ğŸ“ CHá»ˆ táº¡o mapping, KHÃ”NG ghi Ä‘Ã¨ báº£ng CLEAN (giá»‘ng logic cÅ©)")
        try:
            # DÃ¹ng hÃ m riÃªng trong file streaming (khÃ´ng import tá»« batch)
            type_mapping = merge_duplicate_types_and_update_fact_streaming(spark)
            print(f"âœ… Type mapping: {len(type_mapping)} mappings")
        except Exception as e:
            print(f"âŒ Lá»—i khi merge duplicate types: {e}")
            print(f"   Traceback: {type(e).__name__}: {str(e)}")
            type_mapping = {}
            print("âš ï¸ Sá»­ dá»¥ng type_mapping rá»—ng")
    else:
        print("\nâ­ï¸  BÆ°á»›c 3: Bá» qua merge types (dÃ¹ng --merge-types Ä‘á»ƒ báº­t)")
    
    try:
        normalize_gold_type_and_unit(spark)
    except Exception as e:
        print(f"âŒ Lá»—i khi normalize_gold_type_and_unit: {e}")
        print(f"   Traceback: {type(e).__name__}: {str(e)}")
        print("âš ï¸ Bá» qua bÆ°á»›c normalize_gold_type_and_unit")
    
    print("\nâœ… ÄÃ£ clean táº¥t cáº£ dimension tables (giá»¯ nguyÃªn dá»¯ liá»‡u cÅ©)!")
    print("="*60 + "\n")
    
    return location_mapping, type_mapping

def process_batch(batch_id: int, batch_df: 'DataFrame', 
                 spark: SparkSession, table_name: str,
                 clean_all: bool = False, merge_types: bool = False):
    """
    Xá»­ lÃ½ má»—i batch trong streaming.
    ÄÆ°á»£c gá»i tá»± Ä‘á»™ng bá»Ÿi foreachBatch.
    
    Náº¿u clean_all=True, sáº½ clean táº¥t cáº£ báº£ng (LOCATION, TYPE, FACT) má»—i khi FACT thay Ä‘á»•i.
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“¦ Batch {batch_id} - {dt.datetime.now()}")
    print(f"{'='*60}")
    
    # Bá» qua batch_df (khÃ´ng dÃ¹ng, chá»‰ lÃ  trigger)
    # Äá»c dá»¯ liá»‡u má»›i tá»« Oracle dá»±a trÃªn checkpoint
    print(f"\nğŸ” BÆ°á»›c 1: Láº¥y checkpoint Ä‘á»ƒ phÃ¡t hiá»‡n dá»¯ liá»‡u má»›i...")
    last_ts = get_last_timestamp_from_checkpoint(spark)
    print(f"   ğŸ“Œ Timestamp checkpoint: {last_ts}")
    
    print(f"\nğŸ” BÆ°á»›c 2: Äá»c dá»¯ liá»‡u má»›i sau checkpoint...")
    df_new = read_new_data_from_oracle(spark, table_name, last_ts)
    
    new_count = df_new.count()
    if new_count == 0:
        print(f"\nâ„¹ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u má»›i trong batch nÃ y (sau {last_ts})")
        print(f"   â­ï¸  Bá» qua batch {batch_id}")
        return
    
    print(f"\nâœ… PhÃ¡t hiá»‡n {new_count} records má»›i cáº§n xá»­ lÃ½")
    
    print(f"ğŸ“Š Sá»‘ lÆ°á»£ng records FACT má»›i: {df_new.count()}")
    
    # Náº¿u clean_all=True, clean táº¥t cáº£ dimension tables trÆ°á»›c
    location_mapping = {}
    type_mapping = {}
    
    if clean_all:
        print("\nğŸ”„ PhÃ¡t hiá»‡n FACT thay Ä‘á»•i, Ä‘ang clean Táº¤T Cáº¢ cÃ¡c báº£ng...")
        print("   (Giá»¯ nguyÃªn dá»¯ liá»‡u CLEAN cÅ©, chá»‰ cáº­p nháº­t/thÃªm má»›i)")
        location_mapping, type_mapping = clean_all_dimensions_incremental(spark, merge_types)
    else:
        # Chá»‰ load mappings hiá»‡n cÃ³
        location_mapping, type_mapping = load_dimension_mappings(spark)
        print(f"ğŸ“Š Sá»­ dá»¥ng mappings hiá»‡n cÃ³: Location={len(location_mapping)}, Type={len(type_mapping)}")
    
    # Xá»­ lÃ½ dá»¯ liá»‡u FACT má»›i vá»›i mappings
    df_processed = process_new_fact_data(spark, df_new, location_mapping, type_mapping)
    
    if df_processed.count() == 0:
        print("âš ï¸ Sau xá»­ lÃ½ khÃ´ng cÃ²n dá»¯ liá»‡u")
        return
    
    # Merge vá»›i dá»¯ liá»‡u CLEAN hiá»‡n cÃ³ (CHá»ˆ THÃŠM, KHÃ”NG XÃ“A Dá»® LIá»†U CÅ¨) - Logic giá»‘ng batch file
    try:
        # Äá»c báº£ng CLEAN hiá»‡n cÃ³
        df_existing = read_table_from_oracle(spark, "GOLD_PRICE_FACT_CLEAN", DB_USER)
        existing_count = df_existing.count()
        print(f"ğŸ“Š GOLD_PRICE_FACT_CLEAN hiá»‡n cÃ³: {existing_count} records")
        
        # Union dá»¯ liá»‡u má»›i vá»›i dá»¯ liá»‡u cÅ©
        df_combined = df_existing.unionByName(df_processed, allowMissingColumns=True)
        combined_count = df_combined.count()
        processed_count = df_processed.count()
        print(f"ğŸ“Š Sau merge: {combined_count} records (cÅ©: {existing_count}, má»›i: {processed_count})")
        
        # Apply cleaning trÃªn dá»¯ liá»‡u Ä‘Ã£ merge (dedup, handle missing, flag outliers)
        # Logic giá»‘ng há»‡t batch file Ä‘á»ƒ Ä‘áº£m báº£o consistency
        print("ğŸ§¹ Äang xá»­ lÃ½ cleaning trÃªn dá»¯ liá»‡u Ä‘Ã£ merge...")
        
        # 1. Dedup trÃªn toÃ n bá»™ dá»¯ liá»‡u Ä‘Ã£ merge
        df_combined = df_combined.cache()
        before_dedup = df_combined.count()
        
        # Táº¡o composite key Ä‘á»ƒ dedup (vá»›i RECORDED_AT_SAFE Ä‘á»ƒ handle null)
        df_combined = df_combined.withColumn(
            "COMBO",
            concat_ws("|", 
                col("SOURCE_ID").cast("string"),
                col("TYPE_ID").cast("string"),
                col("LOCATION_ID").cast("string"),
                col("TIME_ID").cast("string")
            )
        ).withColumn(
            "RECORDED_AT_SAFE",
            coalesce(col(TIMESTAMP_COLUMN), to_timestamp(lit("2000-01-01 00:00:00")))
        )
        
        window_spec = Window.partitionBy("COMBO").orderBy(col("RECORDED_AT_SAFE").desc())
        df_combined = df_combined.withColumn("rn", row_number().over(window_spec)) \
            .filter(col("rn") == 1) \
            .drop("rn", "COMBO", "RECORDED_AT_SAFE")
        
        after_dedup = df_combined.count()
        n_dup = before_dedup - after_dedup
        print(f"   âœ… ÄÃ£ loáº¡i bá» {n_dup} báº£n ghi trÃ¹ng")
        
        # 2. Handle missing values (chá»‰ loáº¡i bá» record thiáº¿u critical fields)
        before_missing = df_combined.count()
        df_combined = df_combined.filter(
            col("BUY_PRICE").isNotNull() & 
            col("SELL_PRICE").isNotNull() & 
            col(TIMESTAMP_COLUMN).isNotNull()
        )
        after_missing = df_combined.count()
        n_missing = before_missing - after_missing
        print(f"   âœ… ÄÃ£ loáº¡i bá» {n_missing} báº£n ghi thiáº¿u giÃ¡ hoáº·c thá»i gian")
        
        # 3. Flag outliers (khÃ´ng xÃ³a, chá»‰ flag) - Logic giá»‘ng batch file
        from pyspark.sql.functions import percentile_approx
        from decimal import Decimal
        
        def to_float(val):
            if val is None:
                return None
            if isinstance(val, Decimal):
                return float(val)
            return float(val)
        
        try:
            buy_q1_val = df_combined.select(percentile_approx("BUY_PRICE", 0.25).alias("q1")).first()[0]
            buy_q3_val = df_combined.select(percentile_approx("BUY_PRICE", 0.75).alias("q3")).first()[0]
            buy_q1 = to_float(buy_q1_val)
            buy_q3 = to_float(buy_q3_val)
            buy_iqr = buy_q3 - buy_q1
            buy_lower = buy_q1 - 1.5 * buy_iqr
            buy_upper = buy_q3 + 1.5 * buy_iqr
            
            sell_q1_val = df_combined.select(percentile_approx("SELL_PRICE", 0.25).alias("q1")).first()[0]
            sell_q3_val = df_combined.select(percentile_approx("SELL_PRICE", 0.75).alias("q3")).first()[0]
            sell_q1 = to_float(sell_q1_val)
            sell_q3 = to_float(sell_q3_val)
            sell_iqr = sell_q3 - sell_q1
            sell_lower = sell_q1 - 1.5 * sell_iqr
            sell_upper = sell_q3 + 1.5 * sell_iqr
            
            df_combined = df_combined.withColumn(
                "IS_DELETED",
                when(
                    (col("BUY_PRICE") < lit(buy_lower)) | (col("BUY_PRICE") > lit(buy_upper)) |
                    (col("SELL_PRICE") < lit(sell_lower)) | (col("SELL_PRICE") > lit(sell_upper)),
                    lit(1)
                ).otherwise(lit(0))
            )
            
            n_outliers = df_combined.filter(col("IS_DELETED") == 1).count()
            print(f"   âœ… ÄÃ£ flag {n_outliers} báº£n ghi outlier (IS_DELETED=1)")
        except Exception as e:
            print(f"   âš ï¸ KhÃ´ng thá»ƒ flag outliers: {e}. Giá»¯ nguyÃªn dá»¯ liá»‡u.")
            if "IS_DELETED" not in df_combined.columns:
                df_combined = df_combined.withColumn("IS_DELETED", lit(0))
        
        # Äáº£m báº£o cÃ³ cá»™t IS_DELETE (náº¿u cáº§n)
        if "IS_DELETE" not in df_combined.columns:
            df_combined = df_combined.withColumn("IS_DELETE", col("IS_DELETED"))
        
        # Ghi láº¡i báº£ng CLEAN vá»›i dá»¯ liá»‡u Ä‘Ã£ merge vÃ  Ä‘Ã£ clean
        final_count = df_combined.count()
        write_table_to_oracle(df_combined, f"{DB_USER}.GOLD_PRICE_FACT_CLEAN", "overwrite")
        print(f"âœ… ÄÃ£ merge vÃ  clean: {final_count} records (thÃªm {processed_count} má»›i, giá»¯ {existing_count} cÅ©)")
        
        # Cáº­p nháº­t checkpoint vá»›i timestamp má»›i nháº¥t tá»« dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
        print(f"\nğŸ’¾ BÆ°á»›c cuá»‘i: Cáº­p nháº­t checkpoint...")
        max_ts = df_processed.agg(spark_max(col(TIMESTAMP_COLUMN))).first()[0]
        if max_ts:
            update_checkpoint(spark, max_ts)
            print(f"âœ… Checkpoint Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t: {max_ts}")
            print(f"   ğŸ“Œ Batch tiáº¿p theo sáº½ xá»­ lÃ½ dá»¯ liá»‡u sau {max_ts}")
        else:
            print(f"âš ï¸ KhÃ´ng cÃ³ timestamp há»£p lá»‡ Ä‘á»ƒ cáº­p nháº­t checkpoint")
    
    except Exception as e:
        # Náº¿u báº£ng CLEAN chÆ°a cÃ³, ghi dá»¯ liá»‡u má»›i (chá»‰ láº§n Ä‘áº§u)
        print(f"âš ï¸ Báº£ng CLEAN chÆ°a cÃ³ hoáº·c lá»—i: {e}. Ghi dá»¯ liá»‡u má»›i...")
        # Apply basic cleaning trÆ°á»›c khi ghi
        df_processed = df_processed.filter(
            col("BUY_PRICE").isNotNull() & 
            col("SELL_PRICE").isNotNull() & 
            col(TIMESTAMP_COLUMN).isNotNull()
        )
        if "IS_DELETED" not in df_processed.columns:
            df_processed = df_processed.withColumn("IS_DELETED", lit(0))
        if "IS_DELETE" not in df_processed.columns:
            df_processed = df_processed.withColumn("IS_DELETE", lit(0))
        write_table_to_oracle(df_processed, f"{DB_USER}.GOLD_PRICE_FACT_CLEAN", "overwrite")
        print(f"âœ… ÄÃ£ ghi {df_processed.count()} records vÃ o GOLD_PRICE_FACT_CLEAN (láº§n Ä‘áº§u)")

def create_oracle_polling_stream(spark: SparkSession, table_name: str,
                                trigger_interval: str = STREAMING_TRIGGER_INTERVAL,
                                clean_all: bool = False,
                                merge_types: bool = False):
    """
    Táº¡o Spark Structured Streaming query Ä‘á»ƒ polling Oracle.
    
    CÃ¡ch hoáº¡t Ä‘á»™ng:
    1. DÃ¹ng rate source Ä‘á»ƒ táº¡o trigger (emit 1 row má»—i interval)
    2. DÃ¹ng foreachBatch Ä‘á»ƒ polling Oracle má»—i interval
    3. Náº¿u clean_all=True, sáº½ clean táº¥t cáº£ báº£ng má»—i khi FACT thay Ä‘á»•i
    4. Spark tá»± Ä‘á»™ng quáº£n lÃ½ checkpoint vÃ  recovery
    """
    
    # Táº¡o rate source - emit 1 row má»—i interval Ä‘á»ƒ trigger foreachBatch
    # Rate source lÃ  built-in streaming source cá»§a Spark
    trigger_df = spark.readStream \
        .format("rate") \
        .option("rowsPerSecond", 1) \
        .option("numPartitions", 1) \
        .load()
    
    # Chá»‰ láº¥y timestamp column Ä‘á»ƒ lÃ m trigger
    trigger_df = trigger_df.select(col("timestamp").alias("trigger_time"))
    
    # Táº¡o streaming query vá»›i foreachBatch
    def foreach_batch_wrapper(batch_id, batch_df):
        # Bá» qua batch_df (chá»‰ lÃ  trigger)
        # Gá»i process_batch Ä‘á»ƒ polling Oracle vÃ  clean náº¿u cáº§n
        process_batch(batch_id, batch_df, spark, table_name, clean_all, merge_types)
    
    # Táº¡o streaming query
    query = trigger_df.writeStream \
        .foreachBatch(foreach_batch_wrapper) \
        .outputMode("update") \
        .trigger(processingTime=trigger_interval) \
        .option("checkpointLocation", f"{STREAMING_CHECKPOINT_DIR}/oracle_polling") \
        .start()
    
    return query

# ==================== MAIN ====================

def main():
    parser = argparse.ArgumentParser(description="Spark Structured Streaming vá»›i Oracle polling")
    parser.add_argument("--interval", type=str, default=STREAMING_TRIGGER_INTERVAL,
                       help="Trigger interval (vÃ­ dá»¥: '30 seconds', '1 minute')")
    parser.add_argument("--table", type=str, default="GOLD_PRICE_FACT",
                       help="TÃªn báº£ng Oracle Ä‘á»ƒ monitor (GOLD_PRICE_FACT, LOCATION_DIMENSION, GOLD_TYPE_DIMENSION)")
    parser.add_argument("--clean-all", action="store_true",
                       help="Khi FACT thay Ä‘á»•i, tá»± Ä‘á»™ng clean Táº¤T Cáº¢ cÃ¡c báº£ng (LOCATION, TYPE, FACT)")
    parser.add_argument("--merge-types", action="store_true",
                       help="Gá»™p TYPE tÆ°Æ¡ng Ä‘á»“ng khi clean (chá»‰ dÃ¹ng vá»›i --clean-all)")
    
    args = parser.parse_args()
    
    # Táº¡o checkpoint directory
    os.makedirs(STREAMING_CHECKPOINT_DIR, exist_ok=True)
    
    spark = create_spark_session()
    
    print("\n" + "="*60)
    print("ğŸš€ SPARK STRUCTURED STREAMING - ORACLE POLLING")
    print("="*60)
    print(f"ğŸ“Š Table: {args.table}")
    print(f"â±ï¸  Trigger Interval: {args.interval}")
    print(f"ğŸ“ Checkpoint: {STREAMING_CHECKPOINT_DIR}")
    if args.clean_all:
        print(f"ğŸ”„ Mode: Clean ALL tables khi FACT thay Ä‘á»•i")
        print(f"   âœ… LOCATION_DIMENSION â†’ LOCATION_DIMENSION_CLEAN")
        print(f"   âœ… GOLD_TYPE_DIMENSION â†’ GOLD_TYPE_DIMENSION_CLEAN")
        print(f"   âœ… GOLD_PRICE_FACT â†’ GOLD_PRICE_FACT_CLEAN")
        if args.merge_types:
            print(f"   âœ… Merge duplicate types: ON")
    else:
        print(f"ğŸ”„ Mode: Streaming FACT only (chá»‰ xá»­ lÃ½ FACT)")
    print("="*60 + "\n")
    
    # Kiá»ƒm tra batch functions cÃ³ sáºµn khÃ´ng
    if args.clean_all and not BATCH_FUNCTIONS_AVAILABLE:
        print("âŒ Lá»—i: KhÃ´ng thá»ƒ import batch functions Ä‘á»ƒ clean dimensions!")
        print("   Vui lÃ²ng Ä‘áº£m báº£o cÃ¡c dependencies Ä‘Ã£ Ä‘Æ°á»£c cÃ i:")
        print("   pip install pandas numpy scikit-learn fuzzywuzzy python-Levenshtein")
        print("\n   Hoáº·c cháº¡y khÃ´ng cÃ³ --clean-all Ä‘á»ƒ chá»‰ xá»­ lÃ½ FACT")
        sys.exit(1)
    
    # Chá»‰ streaming FACT table
    if args.table != "GOLD_PRICE_FACT":
        print(f"âš ï¸ LÆ°u Ã½: Streaming chá»‰ há»— trá»£ GOLD_PRICE_FACT")
        print(f"   Äang chuyá»ƒn sang GOLD_PRICE_FACT...\n")
        args.table = "GOLD_PRICE_FACT"
    
    # Khá»Ÿi Ä‘á»™ng streaming query
    query = create_oracle_polling_stream(
        spark, 
        args.table, 
        args.interval,
        args.clean_all,
        args.merge_types
    )
    
    print(f"\nâœ… Streaming query Ä‘Ã£ khá»Ÿi Ä‘á»™ng!")
    print(f"ğŸ“Š Query ID: {query.id}")
    print(f"ğŸ“Š Status: {query.status}")
    print(f"ğŸ“Š Spark UI: http://localhost:4040")
    print(f"\nğŸ”„ Äang cháº¡y... Nháº¥n Ctrl+C Ä‘á»ƒ dá»«ng\n")
    
    try:
        # Chá» streaming query cháº¡y
        query.awaitTermination()
    except KeyboardInterrupt:
        print("\nâš ï¸ Äang dá»«ng streaming query...")
        query.stop()
        print("âœ… ÄÃ£ dá»«ng")
    
    spark.stop()

if __name__ == "__main__":
    main()

