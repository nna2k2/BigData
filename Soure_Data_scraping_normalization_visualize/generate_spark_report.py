#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Script táº¡o bÃ¡o cÃ¡o chá»©ng minh Ä‘ang dÃ¹ng Spark
Cháº¡y: python3 generate_spark_report.py
"""

from pyspark.sql import SparkSession
import datetime as dt
import os

def create_spark_session():
    """Táº¡o SparkSession."""
    spark = SparkSession.builder \
        .appName("SparkProofReport") \
        .master("local[*]") \
        .config("spark.sql.adaptive.enabled", "true") \
        .getOrCreate()
    return spark

def generate_spark_report():
    """Táº¡o bÃ¡o cÃ¡o chá»©ng minh Ä‘ang dÃ¹ng Spark."""
    spark = create_spark_session()
    
    report_lines = []
    report_lines.append("="*80)
    report_lines.append("ğŸ“Š BÃO CÃO CHá»¨NG MINH Sá»¬ Dá»¤NG SPARK")
    report_lines.append("="*80)
    report_lines.append(f"Thá»i gian táº¡o: {dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("")
    
    # ThÃ´ng tin Spark
    report_lines.append("ğŸ”¹ THÃ”NG TIN SPARK:")
    report_lines.append(f"   - Spark Version: {spark.version}")
    report_lines.append(f"   - Spark Master: {spark.sparkContext.master}")
    report_lines.append(f"   - Spark App Name: {spark.sparkContext.appName}")
    report_lines.append(f"   - Spark App ID: {spark.sparkContext.applicationId}")
    report_lines.append(f"   - Default Parallelism: {spark.sparkContext.defaultParallelism}")
    report_lines.append("")
    
    # ThÃ´ng tin cáº¥u hÃ¬nh
    report_lines.append("ğŸ”¹ Cáº¤U HÃŒNH SPARK:")
    conf = spark.sparkContext.getConf()
    important_configs = [
        "spark.master",
        "spark.app.name",
        "spark.sql.adaptive.enabled",
        "spark.sql.adaptive.coalescePartitions.enabled",
        "spark.driver.memory",
        "spark.executor.memory",
    ]
    for key in important_configs:
        value = conf.get(key, "N/A")
        report_lines.append(f"   - {key}: {value}")
    report_lines.append("")
    
    # Test Spark operations
    report_lines.append("ğŸ”¹ KIá»‚M TRA SPARK OPERATIONS:")
    
    # Test 1: Táº¡o DataFrame
    test_df = spark.createDataFrame([(1, "test"), (2, "spark")], ["id", "name"])
    report_lines.append(f"   âœ… Táº¡o DataFrame: {test_df.count()} records")
    
    # Test 2: Spark SQL
    test_df.createOrReplaceTempView("test_table")
    sql_result = spark.sql("SELECT COUNT(*) as cnt FROM test_table").collect()[0][0]
    report_lines.append(f"   âœ… Spark SQL: {sql_result} records")
    
    # Test 3: Transformations
    transformed = test_df.filter(test_df["id"] > 1).count()
    report_lines.append(f"   âœ… Transformations: {transformed} records sau filter")
    
    # Test 4: Aggregations
    agg_result = test_df.groupBy("name").count().count()
    report_lines.append(f"   âœ… Aggregations: {agg_result} groups")
    
    report_lines.append("")
    report_lines.append("="*80)
    report_lines.append("âœ… Káº¾T LUáº¬N: Äang sá»­ dá»¥ng Apache Spark Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u")
    report_lines.append("="*80)
    
    # Ghi ra file
    report_content = "\n".join(report_lines)
    report_file = f"spark_proof_report_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(report_content)
    
    # In ra console
    print(report_content)
    print(f"\nğŸ“„ BÃ¡o cÃ¡o Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o: {report_file}")
    
    spark.stop()
    return report_file

if __name__ == "__main__":
    generate_spark_report()

