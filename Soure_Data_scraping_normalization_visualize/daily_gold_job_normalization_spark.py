# -*- coding: utf-8 -*-
"""
Daily Gold ETL Job (Spark version - t·∫°o b·∫£ng _CLEAN)
- B1: LOCATION_DIMENSION: ph√°t hi·ªán th√†nh ph·ªë tr√πng ng·ªØ nghƒ©a -> g·ªôp v·ªÅ 1 ID v√† t·∫°o LOCATION_DIMENSION_CLEAN
- B2: GOLD_TYPE_DIMENSION: d√πng t∆∞∆°ng ƒë·ªìng ƒë·ªÉ ƒëi·ªÅn PURITY/CATEGORY c√≤n thi·∫øu -> t·∫°o GOLD_TYPE_DIMENSION_CLEAN
- B3: GOLD_PRICE_FACT: v·ªõi (SOURCE_ID, TYPE_ID, LOCATION_ID, TIME_ID) tr√πng nhau -> gi·ªØ RECORDED_AT m·ªõi nh·∫•t, t·∫°o GOLD_PRICE_FACT_CLEAN
- Incremental b·∫±ng checkpoint trong DB (b·∫£ng ETL_CHECKPOINT)
- Ch·ª•p snapshot tr∆∞·ªõc/sau ra CSV ƒë·ªÉ b√°o c√°o
"""

import argparse
import datetime as dt
import os
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, when, lit, trim, upper, lower, regexp_replace, 
    concat_ws, first, last, max as spark_max, min as spark_min,
    count, isnan, isnull, coalesce, to_timestamp, date_format,
    row_number, window, monotonically_increasing_id
)
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType
from pyspark.sql.window import Window

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from fuzzywuzzy import fuzz

import re
import unicodedata

# ====================== CONFIG ======================
DB_USER = "CLOUD"
DB_PASS = "cloud123"
DB_HOST = "136.110.60.196"
DB_PORT = "1521"
DB_SERVICE = "XEPDB1"

# T·∫°o DSN v√† JDBC URL t·ª´ c√°c th√¥ng s·ªë
DB_DSN = f"{DB_HOST}:{DB_PORT}/{DB_SERVICE}"
DB_URL = f"jdbc:oracle:thin:@{DB_DSN}"

SNAPSHOT_DIR = "./snapshots"
JOB_NAME = "DAILY_GOLD_JOB"
SIM_THRESHOLD_LOC = 0.80
SIM_THRESHOLD_TYPE = 0.75
FUZZY_FALLBACK = 90

# Spark config
SPARK_APP_NAME = "DailyGoldETLJob"
SPARK_MASTER = "local[*]"  # ho·∫∑c "yarn" n·∫øu ch·∫°y tr√™n cluster

# JDBC Driver path (c√≥ th·ªÉ ch·ªâ ƒë·ªãnh ƒë∆∞·ªùng d·∫´n t√πy ch·ªânh)
# ƒê·ªÉ None ƒë·ªÉ t·ª± ƒë·ªông t√¨m, ho·∫∑c ch·ªâ ƒë·ªãnh ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß
OJDBC_JAR_PATH = None  # V√≠ d·ª•: r"E:\THAC SI\BIGDATA\libs\ojdbc8.jar"

# ====================================================

def create_spark_session(ojdbc_path: str = None, java_home: str = None):
    """T·∫°o SparkSession v·ªõi Oracle JDBC driver."""
    # Set JAVA_HOME n·∫øu ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    if java_home and os.path.exists(java_home):
        os.environ['JAVA_HOME'] = java_home
        print(f"‚úÖ ƒê√£ set JAVA_HOME: {java_home}")
    elif 'JAVA_HOME' not in os.environ:
        # T·ª± ƒë·ªông t√¨m Java 17/11 trong c√°c v·ªã tr√≠ ph·ªï bi·∫øn
        possible_java_homes = [
            r"C:\Program Files\Java\jdk-17",
            r"C:\Program Files\Java\jdk-11",
            r"C:\Program Files\Eclipse Adoptium\jdk-17.0.0-hotspot",
            r"C:\Program Files\Eclipse Adoptium\jdk-11.0.0-hotspot",
            os.path.join(os.path.dirname(__file__), "jdk-17"),
            os.path.join(os.path.dirname(__file__), "jdk-11"),
        ]
        for java_path in possible_java_homes:
            java_exe = os.path.join(java_path, "bin", "java.exe")
            if os.path.exists(java_exe):
                os.environ['JAVA_HOME'] = java_path
                print(f"‚úÖ T·ª± ƒë·ªông t√¨m th·∫•y Java: {java_path}")
                break
    
    builder = SparkSession.builder \
        .appName(SPARK_APP_NAME) \
        .master(SPARK_MASTER) \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.driver.memory", "2g") \
        .config("spark.executor.memory", "2g")
    
    # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n JDBC driver
    # ∆Øu ti√™n: 1) tham s·ªë h√†m, 2) config OJDBC_JAR_PATH, 3) t·ª± ƒë·ªông t√¨m
    final_path = None
    
    if ojdbc_path:
        final_path = ojdbc_path
    elif OJDBC_JAR_PATH:
        final_path = OJDBC_JAR_PATH
    else:
        # T·ª± ƒë·ªông t√¨m ojdbc8.jar trong c√°c th∆∞ m·ª•c ph·ªï bi·∫øn
        possible_paths = [
            "ojdbc8.jar",
            "./ojdbc8.jar",
            "../ojdbc8.jar",
            os.path.join(os.path.dirname(__file__), "ojdbc8.jar"),
            os.path.join(os.path.dirname(__file__), "libs", "ojdbc8.jar"),
            os.path.join(os.path.dirname(__file__), "jars", "ojdbc8.jar"),
        ]
        for path in possible_paths:
            if os.path.exists(path):
                final_path = os.path.abspath(path)
                break
    
    # Th√™m JDBC driver v√†o Spark config
    if final_path:
        if os.path.exists(final_path):
            builder = builder.config("spark.jars", final_path)
            print(f"‚úÖ ƒê√£ load JDBC driver t·ª´: {final_path}")
        else:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y JDBC driver t·∫°i: {final_path}")
            print(f"   Vui l√≤ng t·∫£i ojdbc8.jar v√† ƒë·∫∑t v√†o th∆∞ m·ª•c d·ª± √°n")
            print(f"   Xem h∆∞·ªõng d·∫´n: HUONG_DAN_TAI_OJDBC.md")
    else:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y ojdbc8.jar")
        print("   Vui l√≤ng:")
        print("   1. T·∫£i ojdbc8.jar t·ª´ Oracle ho·∫∑c Maven")
        print("   2. ƒê·∫∑t file v√†o th∆∞ m·ª•c d·ª± √°n")
        print("   3. Ho·∫∑c ch·ªâ ƒë·ªãnh ƒë∆∞·ªùng d·∫´n trong OJDBC_JAR_PATH")
        print("   Xem h∆∞·ªõng d·∫´n: HUONG_DAN_TAI_OJDBC.md")
    
    spark = builder.getOrCreate()
    return spark

def read_table_from_oracle(spark: SparkSession, table_name: str, schema: str = None) -> 'DataFrame':
    """ƒê·ªçc b·∫£ng t·ª´ Oracle DB."""
    schema_prefix = f'"{schema}"."' if schema else '"'
    full_table = f'{schema_prefix}{table_name}"'
    
    df = spark.read \
        .format("jdbc") \
        .option("url", f"jdbc:oracle:thin:{DB_USER}/{DB_PASS}@{DB_DSN}") \
        .option("dbtable", full_table) \
        .option("driver", "oracle.jdbc.driver.OracleDriver") \
        .load()
    return df

def write_table_to_oracle(df: 'DataFrame', table_name: str, mode: str = "overwrite"):
    """Ghi DataFrame v√†o Oracle DB."""
    df.write \
        .format("jdbc") \
        .option("url", f"jdbc:oracle:thin:{DB_USER}/{DB_PASS}@{DB_DSN}") \
        .option("dbtable", table_name) \
        .option("driver", "oracle.jdbc.driver.OracleDriver") \
        .mode(mode) \
        .save()

def ensure_checkpoint_table(spark: SparkSession):
    """ƒê·∫£m b·∫£o b·∫£ng ETL_CHECKPOINT t·ªìn t·∫°i, n·∫øu ch∆∞a c√≥ th√¨ t·∫°o."""
    try:
        # Th·ª≠ ƒë·ªçc b·∫£ng ƒë·ªÉ ki·ªÉm tra xem c√≥ t·ªìn t·∫°i kh√¥ng
        read_table_from_oracle(spark, "ETL_CHECKPOINT", DB_USER)
        print("‚úÖ B·∫£ng ETL_CHECKPOINT ƒë√£ t·ªìn t·∫°i")
    except Exception as e:
        # B·∫£ng ch∆∞a t·ªìn t·∫°i, t·∫°o m·ªõi
        print("‚ö†Ô∏è B·∫£ng ETL_CHECKPOINT ch∆∞a t·ªìn t·∫°i, ƒëang t·∫°o m·ªõi...")
        try:
            # T·∫°o b·∫£ng b·∫±ng c√°ch t·∫°o DataFrame r·ªóng v·ªõi schema ƒë√∫ng v√† ghi v√†o
            from pyspark.sql.types import StructType, StructField, StringType, TimestampType
            
            schema = StructType([
                StructField("JOB_NAME", StringType(), False),
                StructField("LAST_RUN", TimestampType(), True)
            ])
            
            empty_df = spark.createDataFrame([], schema)
            write_table_to_oracle(empty_df, f"{DB_USER}.ETL_CHECKPOINT", "overwrite")
            print("‚úÖ ƒê√£ t·∫°o b·∫£ng ETL_CHECKPOINT")
        except Exception as create_error:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫°o b·∫£ng ETL_CHECKPOINT t·ª± ƒë·ªông: {create_error}")
            print("   Vui l√≤ng t·∫°o b·∫£ng th·ªß c√¥ng b·∫±ng SQL:")
            print(f"   CREATE TABLE {DB_USER}.ETL_CHECKPOINT (")
            print(f"       JOB_NAME VARCHAR2(100) PRIMARY KEY,")
            print(f"       LAST_RUN TIMESTAMP")
            print(f"   );")
            print("   Ho·∫∑c ch·∫°y file: create_etl_checkpoint.sql")

def get_last_checkpoint(spark: SparkSession) -> dt.datetime:
    """L·∫•y checkpoint cu·ªëi c√πng t·ª´ ETL_CHECKPOINT."""
    # ƒê·∫£m b·∫£o b·∫£ng t·ªìn t·∫°i
    ensure_checkpoint_table(spark)
    
    try:
        df = read_table_from_oracle(spark, "ETL_CHECKPOINT", DB_USER)
        df_checkpoint = df.filter(col("JOB_NAME") == JOB_NAME)
        
        if df_checkpoint.count() > 0:
            last_run = df_checkpoint.select("LAST_RUN").first()
            if last_run and last_run[0]:
                return last_run[0]
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c checkpoint: {e}")
    
    return dt.datetime(2000, 1, 1)

def set_checkpoint(spark: SparkSession, ts: dt.datetime):
    """C·∫≠p nh·∫≠t checkpoint."""
    # ƒê·∫£m b·∫£o b·∫£ng t·ªìn t·∫°i
    ensure_checkpoint_table(spark)
    
    checkpoint_df = spark.createDataFrame(
        [(JOB_NAME, ts)],
        ["JOB_NAME", "LAST_RUN"]
    )
    
    # Merge checkpoint (read existing, union, dedup, write)
    try:
        existing = read_table_from_oracle(spark, "ETL_CHECKPOINT", DB_USER)
        combined = existing.filter(col("JOB_NAME") != JOB_NAME).union(checkpoint_df)
    except:
        combined = checkpoint_df
    
    write_table_to_oracle(combined, f"{DB_USER}.ETL_CHECKPOINT", "overwrite")

def snapshot_table(df: 'DataFrame', table: str, tag: str):
    """Ch·ª•p snapshot DataFrame ra CSV."""
    os.makedirs(SNAPSHOT_DIR, exist_ok=True)
    path = os.path.join(SNAPSHOT_DIR, f"{table}_{tag}_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
    
    # Convert Spark DataFrame to Pandas for CSV export
    pandas_df = df.toPandas()
    pandas_df.to_csv(path, index=False, encoding="utf-8-sig")
    print(f"üì∏ Snapshot {table} -> {path}")

# -------------------- LOCATION normalize --------------------

def norm_txt(s: str) -> str:
    """Chu·∫©n ho√° ti·∫øng Vi·ªát (b·ªè d·∫•u, lowercase, trim) ƒë·ªÉ so kh·ªõp ·ªïn ƒë·ªãnh."""
    if not s:
        return ""
    s = str(s).strip().lower()
    s = unicodedata.normalize("NFD", s)
    s = "".join(ch for ch in s if unicodedata.category(ch) != "Mn")
    return s

POSITIVE_SYNONYMS = {
    ("ho chi minh", "tphcm"),
    ("ho chi minh", "tp hcm"),
    ("da nang", "danang"),
    ("ha noi", "thu do"),
    ("ha noi", "hn"),
}

NEGATIVE_SYNONYMS = {
    ("sai gon", "ho chi minh"),
    ("sai gon", "tphcm"),
    ("sai gon", "tp hcm"),
}

def pair_blocked(a: str, b: str) -> bool:
    A, B = norm_txt(a), norm_txt(b)
    return (A, B) in NEGATIVE_SYNONYMS or (B, A) in NEGATIVE_SYNONYMS

def pair_forced(a: str, b: str) -> bool:
    A, B = norm_txt(a), norm_txt(b)
    return (A, B) in POSITIVE_SYNONYMS or (B, A) in POSITIVE_SYNONYMS

def build_similarity_groups(values: List[str], threshold: float) -> List[List[int]]:
    """Nh√≥m c√°c index c√≥ ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine TF-IDF >= threshold + fallback fuzzy."""
    if not values:
        return []

    vec = TfidfVectorizer(ngram_range=(1, 2), analyzer='char_wb').fit(values)
    tf = vec.transform(values)
    sim = cosine_similarity(tf)

    n = len(values)
    visited = [False] * n
    groups = []

    for i in range(n):
        if visited[i]:
            continue
        group = [i]
        visited[i] = True

        for j in range(i + 1, n):
            if visited[j]:
                continue

            a, b = values[i], values[j]

            if pair_blocked(a, b):
                print(f"üö´ BLOCKED: '{a}' vs '{b}'")
                continue

            if pair_forced(a, b):
                print(f"‚úÖ FORCED GROUP: '{a}' ~ '{b}'")
                group.append(j)
                visited[j] = True
                continue

            if sim[i, j] >= threshold or fuzz.token_set_ratio(a, b) >= FUZZY_FALLBACK:
                print(f"‚âà SIMILAR: '{a}' ~ '{b}' (cos={sim[i,j]:.2f}, fuzzy={fuzz.token_set_ratio(a,b)})")
                group.append(j)
                visited[j] = True

        groups.append(group)
    return groups

def normalize_locations(spark: SparkSession) -> Dict[int, int]:
    """Ph√°t hi·ªán & g·ªôp c√°c LOCATION t∆∞∆°ng ƒë·ªìng; t·∫°o LOCATION_DIMENSION_CLEAN."""
    df_loc = read_table_from_oracle(spark, "LOCATION_DIMENSION", DB_USER)
    
    if df_loc.count() == 0:
        print("‚ö†Ô∏è LOCATION_DIMENSION tr·ªëng.")
        return {}

    snapshot_table(df_loc, "LOCATION_DIMENSION", "before_loc_norm")

    # Convert to Pandas for similarity computation
    pandas_loc = df_loc.toPandas()
    names = pandas_loc["CITY"].astype(str).fillna("").str.lower().tolist()
    groups = build_similarity_groups(names, SIM_THRESHOLD_LOC)

    mapping = {}
    for grp in groups:
        ids = pandas_loc.iloc[grp]["ID"].tolist()
        canon = min(ids)
        for idx in grp:
            lid = int(pandas_loc.iloc[idx]["ID"])
            if lid != canon:
                mapping[lid] = canon

    print(f"üîé Mapping location (old->new): {mapping}")

    # √Åp d·ª•ng mapping ƒë·ªÉ t·∫°o b·∫£ng CLEAN
    # ƒê·∫£m b·∫£o T·∫§T C·∫¢ d·ªØ li·ªáu g·ªëc ƒë·ªÅu c√≥ trong CLEAN (ch·ªâ merge ID, kh√¥ng m·∫•t record)
    if mapping:
        # T·∫°o mapping DataFrame
        mapping_df = spark.createDataFrame(
            [(k, v) for k, v in mapping.items()],
            ["OLD_ID", "NEW_ID"]
        )
        
        # Join LEFT ƒë·ªÉ ƒë·∫£m b·∫£o t·∫•t c·∫£ record g·ªëc ƒë·ªÅu ƒë∆∞·ª£c gi·ªØ l·∫°i
        df_clean = df_loc.join(
            mapping_df,
            df_loc["ID"] == mapping_df["OLD_ID"],
            "left"
        ).withColumn(
            "ID_CLEAN",
            when(col("NEW_ID").isNotNull(), col("NEW_ID"))
            .otherwise(col("ID"))  # Gi·ªØ nguy√™n ID n·∫øu kh√¥ng c√≥ mapping
        ).select(
            col("ID_CLEAN").alias("ID"),
            col("CITY"),
            col("REGION")
        )
    else:
        # Kh√¥ng c√≥ mapping, copy to√†n b·ªô d·ªØ li·ªáu g·ªëc
        df_clean = df_loc.select("ID", "CITY", "REGION")

    # L·∫•y distinct ƒë·ªÉ lo·∫°i b·ªè duplicate sau khi merge (ch·ªâ lo·∫°i b·ªè nh·ªØng record tr√πng ho√†n to√†n)
    df_final = df_clean.distinct()
    
    # Log s·ªë l∆∞·ª£ng record
    original_count = df_loc.count()
    final_count = df_final.count()
    print(f"üìä LOCATION_DIMENSION: {original_count} records -> LOCATION_DIMENSION_CLEAN: {final_count} records")
    
    # ƒê·∫£m b·∫£o lu√¥n c√≥ d·ªØ li·ªáu trong b·∫£ng CLEAN (copy to√†n b·ªô n·∫øu c·∫ßn)
    if final_count == 0 and original_count > 0:
        print("‚ö†Ô∏è C·∫£nh b√°o: B·∫£ng CLEAN r·ªóng nh∆∞ng b·∫£ng g·ªëc c√≥ d·ªØ li·ªáu! Copy to√†n b·ªô d·ªØ li·ªáu g·ªëc...")
        df_final = df_loc.select("ID", "CITY", "REGION")
        final_count = original_count

    # Ghi v√†o b·∫£ng _CLEAN (lu√¥n c√≥ d·ªØ li·ªáu, k·ªÉ c·∫£ kh√¥ng c√≥ g√¨ ƒë·ªÉ clean)
    write_table_to_oracle(df_final, f"{DB_USER}.LOCATION_DIMENSION_CLEAN", "overwrite")
    snapshot_table(df_final, "LOCATION_DIMENSION_CLEAN", "after_loc_norm")
    
    return mapping

# -------------------- GOLD TYPE enrichment --------------------

def enrich_gold_types(spark: SparkSession) -> Tuple[int, int]:
    """L√†m gi√†u GOLD_TYPE_DIMENSION v√† t·∫°o GOLD_TYPE_DIMENSION_CLEAN."""
    df = read_table_from_oracle(spark, "GOLD_TYPE_DIMENSION", DB_USER)
    
    if df.count() == 0:
        print("‚ö†Ô∏è GOLD_TYPE_DIMENSION tr·ªëng.")
        return (0, 0)

    snapshot_table(df, "GOLD_TYPE_DIMENSION", "before_type_enrich")

    pandas_df = df.toPandas()
    values = pandas_df["TYPE_NAME"].astype(str).str.lower().fillna("").tolist()
    groups = build_similarity_groups(values, SIM_THRESHOLD_TYPE)

    purity_fill = 0
    category_fill = 0
    
    # T·∫°o mapping dictionary ƒë·ªÉ update
    updates = {}
    
    for grp in groups:
        sub = pandas_df.iloc[grp]
        
        known_purity = sub["PURITY"].dropna()
        known_purity = known_purity[~known_purity.astype(str).str.lower().isin(["unknown", "nan", "none", ""])]
        known_cat = sub["CATEGORY"].dropna()
        known_cat = known_cat[~known_cat.astype(str).str.lower().isin(["unknown", "nan", "none", "other", ""])]
        
        purity_mode = known_purity.mode().iloc[0] if not known_purity.empty else "99.99%"
        cat_mode = known_cat.mode().iloc[0] if not known_cat.empty else "Gold bar"
        
        for _, row in sub.iterrows():
            tid = int(row["ID"])
            purity = str(row["PURITY"]).strip().lower() if pd.notna(row["PURITY"]) else ""
            cat = str(row["CATEGORY"]).strip().lower() if pd.notna(row["CATEGORY"]) else ""
            
            if purity in ["", "unknown", "nan", "none"]:
                updates[tid] = updates.get(tid, {})
                updates[tid]["PURITY"] = purity_mode
                purity_fill += 1
            
            if cat in ["", "unknown", "nan", "none", "other"]:
                updates[tid] = updates.get(tid, {})
                updates[tid]["CATEGORY"] = cat_mode
                category_fill += 1

    # Apply updates to Spark DataFrame
    # ƒê·∫£m b·∫£o lu√¥n copy to√†n b·ªô d·ªØ li·ªáu g·ªëc, k·ªÉ c·∫£ kh√¥ng c√≥ g√¨ ƒë·ªÉ enrich
    df_enriched = df
    if updates:  # Ch·ªâ update n·∫øu c√≥ thay ƒë·ªïi
        for tid, update_dict in updates.items():
            if "PURITY" in update_dict:
                df_enriched = df_enriched.withColumn(
                    "PURITY",
                    when(col("ID") == tid, lit(update_dict["PURITY"]))
                    .otherwise(col("PURITY"))
                )
            if "CATEGORY" in update_dict:
                df_enriched = df_enriched.withColumn(
                    "CATEGORY",
                    when(col("ID") == tid, lit(update_dict["CATEGORY"]))
                    .otherwise(col("CATEGORY"))
                )
    # N·∫øu kh√¥ng c√≥ updates, df_enriched = df (gi·ªØ nguy√™n to√†n b·ªô d·ªØ li·ªáu g·ªëc)

    # Log s·ªë l∆∞·ª£ng record
    original_count = df.count()
    enriched_count = df_enriched.count()
    print(f"üìä GOLD_TYPE_DIMENSION: {original_count} records -> GOLD_TYPE_DIMENSION_CLEAN: {enriched_count} records")
    
    # ƒê·∫£m b·∫£o lu√¥n c√≥ d·ªØ li·ªáu trong b·∫£ng CLEAN (copy to√†n b·ªô n·∫øu c·∫ßn)
    if enriched_count == 0 and original_count > 0:
        print("‚ö†Ô∏è C·∫£nh b√°o: B·∫£ng CLEAN r·ªóng nh∆∞ng b·∫£ng g·ªëc c√≥ d·ªØ li·ªáu! Copy to√†n b·ªô d·ªØ li·ªáu g·ªëc...")
        df_enriched = df
        enriched_count = original_count
    
    # Ghi v√†o b·∫£ng _CLEAN (lu√¥n c√≥ d·ªØ li·ªáu, k·ªÉ c·∫£ kh√¥ng c√≥ g√¨ ƒë·ªÉ clean)
    write_table_to_oracle(df_enriched, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
    snapshot_table(df_enriched, "GOLD_TYPE_DIMENSION_CLEAN", "after_type_enrich")

    print(f"‚ú® ƒê√£ fill PURITY: {purity_fill}, CATEGORY: {category_fill}")
    return (purity_fill, category_fill)

def normalize_purity_format(spark: SparkSession) -> int:
    """Chu·∫©n ho√° c·ªôt PURITY trong GOLD_TYPE_DIMENSION_CLEAN v·ªÅ d·∫°ng 'xx.xx%'."""
    df = read_table_from_oracle(spark, "GOLD_TYPE_DIMENSION_CLEAN", DB_USER)
    
    if df.count() == 0:
        print("‚ö†Ô∏è GOLD_TYPE_DIMENSION_CLEAN tr·ªëng.")
        return 0

    def normalize_purity_udf(purity):
        if not purity:
            return None
        s = str(purity).strip()
        s = s.replace("%", "").replace(" ", "").replace(",", ".").lower()
        if s in ["", "none", "nan", "unknown", "unk"]:
            return None
        nums = re.findall(r"[\d\.]+", s)
        if not nums:
            return None
        try:
            val = float(nums[0])
            if val <= 0 or val > 100:
                return None
            return f"{val:.2f}%"
        except:
            return None

    from pyspark.sql.functions import udf
    from pyspark.sql.types import StringType
    
    normalize_purity = udf(normalize_purity_udf, StringType())
    
    df_clean = df.withColumn(
        "PURITY",
        normalize_purity(col("PURITY"))
    )

    # Count changes by comparing before/after
    df_joined = df.alias("before").join(
        df_clean.alias("after"),
        df["ID"] == df_clean["ID"],
        "inner"
    )
    changed_count = df_joined.filter(
        (col("before.PURITY") != col("after.PURITY")) |
        (col("before.PURITY").isNull() & col("after.PURITY").isNotNull()) |
        (col("before.PURITY").isNotNull() & col("after.PURITY").isNull())
    ).count()
    
    write_table_to_oracle(df_clean, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
    
    print(f"üîß ƒê√£ chu·∫©n ho√° PURITY cho {changed_count} b·∫£n ghi.")
    return changed_count

def normalize_text(s: str) -> str:
    """Chu·∫©n ho√° text d·∫°ng lowercase, b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát."""
    if not s:
        return ""
    s = re.sub(r'[^A-Za-z0-9]+', ' ', str(s))
    s = re.sub(r'\s+', ' ', s).strip().lower()
    tokens = sorted(s.split())
    return " ".join(tokens)

def normalize_category_smart(spark: SparkSession) -> int:
    """Chu·∫©n ho√° CATEGORY trong GOLD_TYPE_DIMENSION_CLEAN."""
    df = read_table_from_oracle(spark, "GOLD_TYPE_DIMENSION_CLEAN", DB_USER)
    
    if df.count() == 0:
        print("‚ö†Ô∏è GOLD_TYPE_DIMENSION_CLEAN tr·ªëng.")
        return 0

    pandas_df = df.toPandas()
    pandas_df["CLEAN"] = pandas_df["CATEGORY"].astype(str).apply(normalize_text)
    unique_vals = pandas_df["CLEAN"].unique().tolist()

    groups = []
    visited = set()
    for i, base in enumerate(unique_vals):
        if base in visited:
            continue
        group = [base]
        visited.add(base)
        for j, other in enumerate(unique_vals):
            if other in visited:
                continue
            if fuzz.token_set_ratio(base, other) >= 90:
                group.append(other)
                visited.add(other)
        groups.append(group)

    mapping = {}
    for grp in groups:
        canon = sorted(grp, key=len)[0]
        for val in grp:
            mapping[val] = canon

    from pyspark.sql.functions import udf
    from pyspark.sql.types import StringType
    
    def title_case_udf(s):
        return s.title() if s else None
    
    title_case = udf(title_case_udf, StringType())
    
    def map_category_udf(cat):
        clean = normalize_text(cat)
        canon = mapping.get(clean, clean)
        return canon.title() if canon else None
    
    map_category = udf(map_category_udf, StringType())
    
    df_clean = df.withColumn("CATEGORY", map_category(col("CATEGORY")))
    
    changed_count = df.join(df_clean, df["ID"] == df_clean["ID"], "inner") \
        .filter(df["CATEGORY"] != df_clean["CATEGORY"]).count()
    
    write_table_to_oracle(df_clean, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
    
    print(f"‚úÖ ƒê√£ chu·∫©n ho√° CATEGORY cho {changed_count} b·∫£n ghi.")
    return changed_count

def merge_duplicate_types_and_update_fact(spark: SparkSession):
    """G·ªôp c√°c b·∫£n ghi GOLD_TYPE_DIMENSION tr√πng v√† t·∫°o mapping."""
    df = read_table_from_oracle(spark, "GOLD_TYPE_DIMENSION_CLEAN", DB_USER)
    
    if df.count() == 0:
        print("‚ö†Ô∏è GOLD_TYPE_DIMENSION_CLEAN tr·ªëng.")
        return {}

    # Ki·ªÉm tra c√°c c·ªôt c√≥ t·ªìn t·∫°i kh√¥ng
    columns = df.columns
    df_normalized = df.withColumn(
        "TYPE_NAME_NORM", lower(trim(col("TYPE_NAME")))
    ).withColumn(
        "PURITY_NORM", lower(trim(col("PURITY")))
    ).withColumn(
        "CATEGORY_NORM", lower(trim(col("CATEGORY")))
    )
    
    # Ch·ªâ th√™m BRAND_NORM n·∫øu c·ªôt BRAND t·ªìn t·∫°i
    if "BRAND" in columns:
        df_normalized = df_normalized.withColumn(
            "BRAND_NORM", lower(trim(col("BRAND")))
        )
        partition_cols = ["TYPE_NAME_NORM", "PURITY_NORM", "CATEGORY_NORM", "BRAND_NORM"]
    else:
        # T·∫°o c·ªôt BRAND_NORM r·ªóng n·∫øu kh√¥ng c√≥ BRAND
        df_normalized = df_normalized.withColumn("BRAND_NORM", lit(""))
        partition_cols = ["TYPE_NAME_NORM", "PURITY_NORM", "CATEGORY_NORM", "BRAND_NORM"]
        print("‚ö†Ô∏è C·ªôt BRAND kh√¥ng t·ªìn t·∫°i, s·ª≠ d·ª•ng gi√° tr·ªã r·ªóng cho BRAND_NORM")

    # Group by normalized values and find canonical ID
    window_spec = Window.partitionBy(*partition_cols).orderBy("ID")
    
    df_with_canon = df_normalized.withColumn(
        "CANON_ID",
        first("ID").over(window_spec)
    )

    # Create mapping
    mapping_df = df_with_canon.filter(col("ID") != col("CANON_ID")) \
        .select(col("ID").alias("OLD_ID"), col("CANON_ID").alias("NEW_ID")) \
        .distinct()
    
    mapping = {}
    if mapping_df.count() > 0:
        for row in mapping_df.collect():
            mapping[int(row["OLD_ID"])] = int(row["NEW_ID"])
    
    # Create clean table - ƒê·∫£m b·∫£o T·∫§T C·∫¢ d·ªØ li·ªáu g·ªëc ƒë·ªÅu c√≥ trong CLEAN
    # Ch·ªâ select c√°c c·ªôt c√≥ t·ªìn t·∫°i
    select_cols = ["TYPE_NAME", "PURITY", "CATEGORY"]
    if "BRAND" in columns:
        select_cols.append("BRAND")
    
    # L·∫•y t·∫•t c·∫£ record v·ªõi ID ƒë√£ ƒë∆∞·ª£c normalize (canonical ID)
    # ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o t·∫•t c·∫£ record g·ªëc ƒë·ªÅu c√≥ trong CLEAN (ch·ªâ ID ƒë∆∞·ª£c merge)
    df_clean = df_with_canon.select(
        col("CANON_ID").alias("ID"),
        *[col(c) for c in select_cols]
    ).distinct()
    
    # ƒê·∫£m b·∫£o s·ªë l∆∞·ª£ng kh√¥ng b·ªã m·∫•t qu√° nhi·ªÅu
    original_count = df.count()
    clean_count = df_clean.count()
    if clean_count < original_count * 0.8:  # N·∫øu m·∫•t > 20% th√¨ c√≥ v·∫•n ƒë·ªÅ
        print(f"‚ö†Ô∏è C·∫£nh b√°o: S·ªë l∆∞·ª£ng record gi·∫£m t·ª´ {original_count} xu·ªëng {clean_count}")
    
    write_table_to_oracle(df_clean, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")
    
    print(f"‚úÖ ƒê√£ g·ªôp {len(mapping)} TYPE tr√πng. Gi·ªØ l·∫°i {clean_count}/{original_count} records trong CLEAN.")
    return mapping

def normalize_gold_type_and_unit(spark: SparkSession):
    """Chu·∫©n h√≥a BRAND v√† UNIT."""
    df_type = read_table_from_oracle(spark, "GOLD_TYPE_DIMENSION_CLEAN", DB_USER)
    
    # Ki·ªÉm tra xem c·ªôt BRAND c√≥ t·ªìn t·∫°i kh√¥ng
    columns = df_type.columns
    df_type_clean = df_type
    
    if "BRAND" in columns:
        df_type_clean = df_type_clean.withColumn(
            "BRAND",
            when(col("BRAND").isNotNull(),
                 upper(trim(regexp_replace(regexp_replace(col("BRAND"), "\\.", ""), "V√ÄNG ", ""))))
            .otherwise(col("BRAND"))
        )
        print("üìè ƒê√£ chu·∫©n h√≥a BRAND.")
    else:
        print("‚ö†Ô∏è C·ªôt BRAND kh√¥ng t·ªìn t·∫°i trong GOLD_TYPE_DIMENSION_CLEAN, b·ªè qua chu·∫©n h√≥a BRAND.")
    
    write_table_to_oracle(df_type_clean, f"{DB_USER}.GOLD_TYPE_DIMENSION_CLEAN", "overwrite")

# -------------------- FACT dedup incremental --------------------

def dedup_fact_incremental(spark: SparkSession, last_run: dt.datetime, location_mapping: Dict, type_mapping: Dict):
    """Deduplicate FACT v√† c·∫≠p nh·∫≠t GOLD_PRICE_FACT_CLEAN."""
    # ƒê·ªçc t·ª´ b·∫£ng CLEAN ƒë√£ ƒë∆∞·ª£c t·∫°o (mappings ƒë√£ ƒë∆∞·ª£c apply)
    df_fact = read_table_from_oracle(spark, "GOLD_PRICE_FACT_CLEAN", DB_USER)
    
    if df_fact.count() == 0:
        print("‚ÑπÔ∏è Kh√¥ng c√≥ FACT ƒë·ªÉ dedup.")
        return 0

    before_count = df_fact.count()

    # Create combo key
    df_fact = df_fact.withColumn(
        "COMBO",
        concat_ws("_", 
                  col("SOURCE_ID").cast("string"),
                  col("TYPE_ID").cast("string"),
                  col("LOCATION_ID").cast("string"),
                  col("TIME_ID").cast("string"))
    )

    # Keep latest record per combo
    window_spec = Window.partitionBy("COMBO").orderBy(col("RECORDED_AT").desc())
    df_clean = df_fact.withColumn("rn", row_number().over(window_spec)) \
        .filter(col("rn") == 1) \
        .drop("rn", "COMBO") \
        .withColumn("IS_DELETED", lit(0)) \
        .withColumn("IS_DELETE", lit(0))

    n_dup = before_count - df_clean.count()

    write_table_to_oracle(df_clean, f"{DB_USER}.GOLD_PRICE_FACT_CLEAN", "overwrite")
    snapshot_table(df_clean, "GOLD_PRICE_FACT_CLEAN", "after_fact_dedup")
    
    print(f"üßπ ƒê√£ t·∫°o GOLD_PRICE_FACT_CLEAN v·ªõi {df_clean.count()} b·∫£n ghi (lo·∫°i b·ªè {n_dup} tr√πng).")
    return n_dup

def handle_missing_values_fact(spark: SparkSession, last_run: dt.datetime):
    """X·ª≠ l√Ω missing values trong FACT v√† c·∫≠p nh·∫≠t GOLD_PRICE_FACT_CLEAN.
    Ch·ªâ lo·∫°i b·ªè record thi·∫øu critical fields, c√≤n l·∫°i gi·ªØ nguy√™n.
    """
    df_fact = read_table_from_oracle(spark, "GOLD_PRICE_FACT_CLEAN", DB_USER)
    
    if df_fact.count() == 0:
        print("‚ÑπÔ∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω missing values.")
        return 0

    before_count = df_fact.count()
    
    # Ch·ªâ lo·∫°i b·ªè record thi·∫øu critical fields (BUY_PRICE, SELL_PRICE, TIME_ID)
    # C√°c record kh√°c gi·ªØ nguy√™n ƒë·ªÉ ƒë·∫£m b·∫£o c√≥ ƒë·∫ßy ƒë·ªß d·ªØ li·ªáu
    df_clean = df_fact.filter(
        col("BUY_PRICE").isNotNull() &
        col("SELL_PRICE").isNotNull() &
        col("TIME_ID").isNotNull()
    )

    # Fill UNIT with mode
    unit_mode_row = df_clean.groupBy("UNIT").count().orderBy(col("count").desc()).first()
    unit_default = unit_mode_row[0] if unit_mode_row else "VND/L∆∞·ª£ng"
    
    df_clean = df_clean.withColumn(
        "UNIT",
        when(col("UNIT").isNull(), lit(unit_default))
        .otherwise(col("UNIT"))
    )

    dropped = before_count - df_clean.count()

    write_table_to_oracle(df_clean, f"{DB_USER}.GOLD_PRICE_FACT_CLEAN", "overwrite")
    snapshot_table(df_clean, "GOLD_PRICE_FACT_CLEAN", "after_handle_missing")
    
    print(f"üß© ƒê√£ lo·∫°i b·ªè {dropped} b·∫£n ghi thi·∫øu gi√° ho·∫∑c th·ªùi gian.")
    return dropped

def flag_price_outliers(spark: SparkSession, last_run: dt.datetime):
    """Ph√°t hi·ªán outlier gi√° v√† c·∫≠p nh·∫≠t GOLD_PRICE_FACT_CLEAN."""
    df_fact = read_table_from_oracle(spark, "GOLD_PRICE_FACT_CLEAN", DB_USER)
    
    if df_fact.count() == 0:
        print("‚ÑπÔ∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ flag outlier.")
        return 0

    # Calculate IQR for BUY_PRICE and SELL_PRICE using Spark
    from pyspark.sql.functions import percentile_approx
    
    # Get quantiles using Spark
    buy_q1 = df_fact.select(percentile_approx("BUY_PRICE", 0.25).alias("q1")).first()[0]
    buy_q3 = df_fact.select(percentile_approx("BUY_PRICE", 0.75).alias("q3")).first()[0]
    buy_iqr = buy_q3 - buy_q1
    buy_lower = buy_q1 - 1.5 * buy_iqr
    buy_upper = buy_q3 + 1.5 * buy_iqr
    
    sell_q1 = df_fact.select(percentile_approx("SELL_PRICE", 0.25).alias("q1")).first()[0]
    sell_q3 = df_fact.select(percentile_approx("SELL_PRICE", 0.75).alias("q3")).first()[0]
    sell_iqr = sell_q3 - sell_q1
    sell_lower = sell_q1 - 1.5 * sell_iqr
    sell_upper = sell_q3 + 1.5 * sell_iqr
    
    df_clean = df_fact.withColumn(
        "BUY_PRICE_OUTLIER",
        (col("BUY_PRICE") < lit(buy_lower)) | (col("BUY_PRICE") > lit(buy_upper))
    ).withColumn(
        "SELL_PRICE_OUTLIER",
        (col("SELL_PRICE") < lit(sell_lower)) | (col("SELL_PRICE") > lit(sell_upper))
    ).withColumn(
        "IS_DELETED",
        when((col("BUY_PRICE_OUTLIER") == True) | (col("SELL_PRICE_OUTLIER") == True), lit(1))
        .otherwise(lit(0))
    ).drop("BUY_PRICE_OUTLIER", "SELL_PRICE_OUTLIER")

    flagged = df_clean.filter(col("IS_DELETED") == 1).count()

    write_table_to_oracle(df_clean, f"{DB_USER}.GOLD_PRICE_FACT_CLEAN", "overwrite")
    snapshot_table(df_clean, "GOLD_PRICE_FACT_CLEAN", "after_outlier_flag")
    
    print(f"‚ö†Ô∏è ƒê√£ flag {flagged} b·∫£n ghi outlier (IS_DELETED=1).")
    return flagged

# --------------------------- MAIN ---------------------------

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--merge-types", action="store_true", 
                       help="Th·ª≠ g·ªôp TYPE t∆∞∆°ng ƒë·ªìng v·ªÅ 1 ID v√† c·∫≠p nh·∫≠t FACT")
    args = parser.parse_args()

    spark = create_spark_session()

    # Snapshot tr∆∞·ªõc khi x·ª≠ l√Ω
    df_loc = read_table_from_oracle(spark, "LOCATION_DIMENSION", DB_USER)
    df_type = read_table_from_oracle(spark, "GOLD_TYPE_DIMENSION", DB_USER)
    df_fact = read_table_from_oracle(spark, "GOLD_PRICE_FACT", DB_USER)
    
    # Log s·ªë l∆∞·ª£ng record g·ªëc
    print(f"\nüìä S·ªë l∆∞·ª£ng d·ªØ li·ªáu g·ªëc:")
    print(f"   LOCATION_DIMENSION: {df_loc.count()} records")
    print(f"   GOLD_TYPE_DIMENSION: {df_type.count()} records")
    print(f"   GOLD_PRICE_FACT: {df_fact.count()} records\n")
    
    snapshot_table(df_loc, "LOCATION_DIMENSION", "before")
    snapshot_table(df_type, "GOLD_TYPE_DIMENSION", "before")
    snapshot_table(df_fact, "GOLD_PRICE_FACT", "before")

    last_run = get_last_checkpoint(spark)
    print(f"‚è±Ô∏è Last checkpoint: {last_run}")

    # B1: LOCATION normalize -> LOCATION_DIMENSION_CLEAN
    location_mapping = normalize_locations(spark)

    # B2: GOLD TYPE enrich -> GOLD_TYPE_DIMENSION_CLEAN
    enrich_gold_types(spark)
    normalize_purity_format(spark)
    normalize_category_smart(spark)

    # (Tu·ª≥ ch·ªçn) g·ªôp TYPE t∆∞∆°ng ƒë·ªìng
    type_mapping = {}
    if args.merge_types:
        type_mapping = merge_duplicate_types_and_update_fact(spark)

    normalize_gold_type_and_unit(spark)

    # B3: FACT dedup incremental -> GOLD_PRICE_FACT_CLEAN
    # ƒê·ªçc to√†n b·ªô FACT v√† apply mappings, sau ƒë√≥ dedup
    df_fact_all = read_table_from_oracle(spark, "GOLD_PRICE_FACT", DB_USER)
    fact_original_count = df_fact_all.count()
    print(f"üìä GOLD_PRICE_FACT g·ªëc: {fact_original_count} records")
    
    # Apply location mapping
    if location_mapping:
        mapping_df = spark.createDataFrame(
            [(k, v) for k, v in location_mapping.items()],
            ["OLD_LOC_ID", "NEW_LOC_ID"]
        )
        df_fact_all = df_fact_all.join(
            mapping_df,
            df_fact_all["LOCATION_ID"] == mapping_df["OLD_LOC_ID"],
            "left"
        ).withColumn(
            "LOCATION_ID",
            when(col("NEW_LOC_ID").isNotNull(), col("NEW_LOC_ID"))
            .otherwise(col("LOCATION_ID"))
        ).drop("OLD_LOC_ID", "NEW_LOC_ID")
    
    # Apply type mapping
    if type_mapping:
        mapping_df = spark.createDataFrame(
            [(k, v) for k, v in type_mapping.items()],
            ["OLD_TYPE_ID", "NEW_TYPE_ID"]
        )
        df_fact_all = df_fact_all.join(
            mapping_df,
            df_fact_all["TYPE_ID"] == mapping_df["OLD_TYPE_ID"],
            "left"
        ).withColumn(
            "TYPE_ID",
            when(col("NEW_TYPE_ID").isNotNull(), col("NEW_TYPE_ID"))
            .otherwise(col("TYPE_ID"))
        ).drop("OLD_TYPE_ID", "NEW_TYPE_ID")
    
    # Log s·ªë l∆∞·ª£ng sau khi apply mappings
    fact_after_mapping_count = df_fact_all.count()
    print(f"üìä GOLD_PRICE_FACT sau mapping: {fact_after_mapping_count} records")
    
    # ƒê·∫£m b·∫£o lu√¥n c√≥ d·ªØ li·ªáu trong b·∫£ng CLEAN (copy to√†n b·ªô n·∫øu c·∫ßn)
    if fact_after_mapping_count == 0 and fact_original_count > 0:
        print("‚ö†Ô∏è C·∫£nh b√°o: Sau mapping b·∫£ng CLEAN r·ªóng nh∆∞ng b·∫£ng g·ªëc c√≥ d·ªØ li·ªáu! Copy to√†n b·ªô d·ªØ li·ªáu g·ªëc...")
        df_fact_all = read_table_from_oracle(spark, "GOLD_PRICE_FACT", DB_USER)
        fact_after_mapping_count = df_fact_all.count()
    
    # Write initial clean fact table (lu√¥n c√≥ d·ªØ li·ªáu, k·ªÉ c·∫£ kh√¥ng c√≥ g√¨ ƒë·ªÉ clean)
    write_table_to_oracle(df_fact_all, f"{DB_USER}.GOLD_PRICE_FACT_CLEAN", "overwrite")
    print(f"‚úÖ ƒê√£ ghi {fact_after_mapping_count} records v√†o GOLD_PRICE_FACT_CLEAN")
    
    # Then apply dedup and other cleaning
    dedup_fact_incremental(spark, last_run, {}, {})  # Mappings already applied
    handle_missing_values_fact(spark, last_run)
    flag_price_outliers(spark, last_run)

    # C·∫≠p nh·∫≠t checkpoint
    now = dt.datetime.now()
    set_checkpoint(spark, now)
    print(f"‚úÖ Job ho√†n t·∫•t. Checkpoint m·ªõi: {now}")

    # Snapshot cu·ªëi
    df_loc_clean = read_table_from_oracle(spark, "LOCATION_DIMENSION_CLEAN", DB_USER)
    df_type_clean = read_table_from_oracle(spark, "GOLD_TYPE_DIMENSION_CLEAN", DB_USER)
    df_fact_clean = read_table_from_oracle(spark, "GOLD_PRICE_FACT_CLEAN", DB_USER)
    
    snapshot_table(df_loc_clean, "LOCATION_DIMENSION_CLEAN", "final")
    snapshot_table(df_type_clean, "GOLD_TYPE_DIMENSION_CLEAN", "final")
    snapshot_table(df_fact_clean, "GOLD_PRICE_FACT_CLEAN", "final")

    spark.stop()

if __name__ == "__main__":
    # C√≥ th·ªÉ ch·∫°y tr·ª±c ti·∫øp v·ªõi Python: python daily_gold_job_normalization_spark.py
    # Ho·∫∑c d√πng script helper: python run_spark_job_local.py
    main()

